<!doctype html><html lang=zh-cn dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="zh-CN"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><title>入门: 神经网络（Neural Network）及 Python 实现 &#183; N3xtChen 的博客</title><meta name=title content="入门: 神经网络（Neural Network）及 Python 实现 &#183; N3xtChen 的博客"><meta name=keywords content="NN,"><link rel=canonical href=https://n3xtchen.github.io/n3xtchen/2018/12/06/2018-12-06-neural-network/><link type=text/css rel=stylesheet href=/n3xtchen/css/main.bundle.min.5775579083e26679417a451dd3d7c3e540c7be85426fba5a47019603552ac89b95e12dddb64138e0aa488d68bc318ec4b0820255a5859b4da3f1b45f140abf11.css integrity="sha512-V3VXkIPiZnlBekUd09fD5UDHvoVCb7paRwGWA1UqyJuV4S3dtkE44KpIjWi8MY7EsIICVaWFm02j8bRfFAq/EQ=="><script type=text/javascript src=/n3xtchen/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
<script defer type=text/javascript id=script-bundle src=/n3xtchen/js/main.bundle.min.41e474f5d66052b313c33b8f56008afac498015b7472d0f2ab78e22c187405fdff125b6735973309e4743b6924ca8c575db297f464c42da8e30ef9daed3b050c.js integrity="sha512-QeR09dZgUrMTwzuPVgCK+sSYAVt0ctDyq3jiLBh0Bf3/EltnNZczCeR0O2kkyoxXXbKX9GTELajjDvna7TsFDA==" data-copy data-copied></script>
<script src=/n3xtchen/js/zoom.min.js></script>
<link rel=icon href=/n3xtchen/favicon.png pe=image/png><meta property="og:title" content="入门: 神经网络（Neural Network）及 Python 实现"><meta property="og:description" content="为了更好的理解深度学习，我决定从零开始构建一个神经网络（Neural Network）。"><meta property="og:type" content="article"><meta property="og:url" content="https://n3xtchen.github.io/n3xtchen/2018/12/06/2018-12-06-neural-network/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-12-06T09:55:19+00:00"><meta property="article:modified_time" content="2018-12-06T09:55:19+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="入门: 神经网络（Neural Network）及 Python 实现"><meta name=twitter:description content="为了更好的理解深度学习，我决定从零开始构建一个神经网络（Neural Network）。"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"时间线","name":"入门: 神经网络（Neural Network）及 Python 实现","headline":"入门: 神经网络（Neural Network）及 Python 实现","abstract":"为了更好的理解深度学习，我决定从零开始构建一个神经网络（Neural Network）。","inLanguage":"zh-CN","url":"https:\/\/n3xtchen.github.io\/n3xtchen\/2018\/12\/06\/2018-12-06-neural-network\/","author":{"@type":"Person","name":"n3xtchen"},"copyrightYear":"2018","dateCreated":"2018-12-06T00:00:00\u002b00:00","datePublished":"2018-12-06T09:55:19\u002b00:00","dateModified":"2018-12-06T09:55:19\u002b00:00","keywords":["NN"],"mainEntityOfPage":"true","wordCount":"305"}]</script><meta name=author content="n3xtchen"><link href=https://twitter.com/mN3XT rel=me><link href=https://githhub.com/n3xtchen rel=me><script src=/n3xtchen/lib/jquery/jquery.slim.min.js integrity></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EMLJCHXBFD"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EMLJCHXBFD")</script><meta name=theme-color><style>code.has-jax{-webkit-font-smoothing:antialiased;background:inherit!important;border:none!important;font-size:100%}</style><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span></a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/n3xtchen class="text-base font-medium text-gray-500 hover:text-gray-900">N3xtChen 的博客</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/n3xtchen/posts/ class="flex items-center"><p class="text-base font-medium text-gray-500 hover:text-gray-900" title=时间线>时间线</p></a><a href=/n3xtchen/categories/ class="flex items-center"><p class="text-base font-medium text-gray-500 hover:text-gray-900" title=分类页>分类</p></a><a href=/n3xtchen/tags/ class="flex items-center"><p class="text-base font-medium text-gray-500 hover:text-gray-900" title=Tags>标签</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400 h-12" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher aria-label="Dark mode switcher" type=button><div class="flex items-center justify-center h-12 dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden h-12 dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button style=margin-right:5px><div class="flex items-center justify-center h-12 dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden h-12 dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button for=menu-controller class=block><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/n3xtchen/posts/ class="flex items-center"><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title=时间线>时间线</p></a></li><li class=mt-1><a href=/n3xtchen/categories/ class="flex items-center"><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title=分类页>分类</p></a></li><li class=mt-1><a href=/n3xtchen/tags/ class="flex items-center"><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title=Tags>标签</p></a></li></ul></div></label></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">入门: 神经网络（Neural Network）及 Python 实现</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2018-12-06 00:00:00 +0000 UTC">2018-12-06</time><span class="px-2 text-primary-500">&#183;</span><span>305 字</span><span class="px-2 text-primary-500">&#183;</span><span title=预计阅读>2 分钟</span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/n3xtchen/categories/algorithm/","_self")'><span class=flex><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">algorithm</span></span></span>
<span style=margin-top:.5rem class=mr-2 onclick='window.open("/n3xtchen/tags/nn/","_self")'><span class=flex><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">NN</span></span></span></div></div><div class=flex><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt=n3xtchen src=/n3xtchen/images/author.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">作者</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">n3xtchen</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Sharing Funny Tech With You</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/mN3XT target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://githhub.com/n3xtchen target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first sm:max-w-prose lg:ml-auto px-0 lg:order-last lg:max-w-xs ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"></summary><div class="min-w-[220px] py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#什么是神经网络>什么是神经网络？</a><ul><li><a href=#起源m-p-模型单层神经元>起源：M-P 模型（单层神经元）</a></li><li><a href=#mlp多层感知机multilayer-perceptron-我们常说的神经网络->MLP：多层感知机（Multilayer Perceptron， 我们常说的神经网络 ）</a></li></ul></li><li><a href=#开始构建>开始构建</a></li><li><a href=#神经网络的训练>神经网络的训练</a><ul><li><a href=#前馈feedforward>前馈（feedforward）</a></li><li><a href=#损失函数loss-function>损失函数（Loss Function）</a></li><li><a href=#反向反馈backpropagation>反向反馈（Backpropagation）</a></li><li><a href=#整合在一起>整合在一起</a></li></ul></li><li><a href=#接下来呢>接下来呢？</a></li><li><a href=#最后的思考>最后的思考</a></li></ul></nav></div></details><details class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"></summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#什么是神经网络>什么是神经网络？</a><ul><li><a href=#起源m-p-模型单层神经元>起源：M-P 模型（单层神经元）</a></li><li><a href=#mlp多层感知机multilayer-perceptron-我们常说的神经网络->MLP：多层感知机（Multilayer Perceptron， 我们常说的神经网络 ）</a></li></ul></li><li><a href=#开始构建>开始构建</a></li><li><a href=#神经网络的训练>神经网络的训练</a><ul><li><a href=#前馈feedforward>前馈（feedforward）</a></li><li><a href=#损失函数loss-function>损失函数（Loss Function）</a></li><li><a href=#反向反馈backpropagation>反向反馈（Backpropagation）</a></li><li><a href=#整合在一起>整合在一起</a></li></ul></li><li><a href=#接下来呢>接下来呢？</a></li><li><a href=#最后的思考>最后的思考</a></li></ul></nav></div></details></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="max-w-prose mb-20"><p>为了更好的理解深度学习，我决定从零开始构建一个神经网络（Neural Network）。</p><div id=什么是神经网络 class=anchor></div><h2 class="relative group">什么是神经网络？
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e4%bb%80%e4%b9%88%e6%98%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=锚点>#</a></span></h2><div id=起源m-p-模型单层神经元 class=anchor></div><h3 class="relative group">起源：M-P 模型（单层神经元）
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e8%b5%b7%e6%ba%90m-p-%e6%a8%a1%e5%9e%8b%e5%8d%95%e5%b1%82%e7%a5%9e%e7%bb%8f%e5%85%83 aria-label=锚点>#</a></span></h3><p>所谓M-P模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型。简单点说，它是对一个生物神经元的建模。它实际上是两位科学家的名字的合称，1943年心理学家W.McCulloch和数学家W.Pitts合作提出了这个模型，所以取了他们两个人的名字（McCulloch-Pitts）。</p><p>先来看看神经元的简化示意图：</p><p><figure><img class="my-0 rounded-md" src=http://www.hahack.com/images/ann1/gb1bJ.png alt=生物神经元模型><figcaption>生物神经元模型</figcaption></figure>{:width=&ldquo;800px&rdquo;}</p><p>神经元在结构上由细胞体、树突（输入）、轴突（输出）和突触4部分组成。</p><ol><li>每个神经元都是一个多输入单输出（轴突）的信息处理单元；</li><li>神经元输入（树突）分兴奋性输入和抑制性输入两种类型；</li><li>神经元具有空间整合特性和阈值特性（兴奋和抑制，超过阈值为兴奋，低于是抑制）；</li><li>神经元输入与输出间有固定的时滞，主要取决于突触延搁；</li></ol><p>我们可以把一个神经元想象成一个水桶，这个水桶侧边接着很多条水管（<strong>神经末梢</strong>），水管既可以将桶里的水输出去（<strong>抑制性</strong>），也可以将其他水桶的水输进来（<strong>兴奋性</strong>）。当桶里的水达到一个高度时，就会通过另一条管子（<strong>轴突</strong>）将水输送出去。由于水管的粗细不同，对桶里的水的影响程度（<strong>权重</strong>）也不同。水管对水桶里的水位的改变（<strong>膜电位</strong>）自然就是这些水管输水量的<strong>累加</strong>。当然，这样来理解并不是很完美，因为神经元中的信号是采用一个个脉冲串的离散形式，而这里的水则是连续的。</p><p>关于权值的理解，还有人做出一个非常形象的比喻。比如现在我们要选一个餐厅吃饭，于是对于某一个餐厅，我们有好几种选择因素 e.g.口味、位置、装潢、价格等等，这些选择因素就是输入，而每一个因素占的比重往往不同，比如我们往往会把口味和价格放在更重要的位置，装潢和位置则稍微不那么重要。很多个候选餐厅的选择结果最终汇总之后，就可以得到最后的决策。</p><p>按照生物神经元，我们建立M-P模型。下图，展示的就是 M-P 模型的示意图：</p><p><figure><img class="my-0 rounded-md" src=http://www.hahack.com/images/ann1/BGCSG.png alt=M-P></figure>{:width=&ldquo;800px&rdquo;}</p><p>那么接下来就好类比理解了。我们将这个模型和生物神经元的特性列表来比较：</p><p><figure><img class="my-0 rounded-md" src=http://www.hahack.com/images/ann1/vBksY.png alt=生物神经元与MP模型></figure>{:width=&ldquo;800px&rdquo;}</p><p>结合M-P模型示意图来看，对于某一个神经元j，它可能接受同时接受了许多个输入信号，用χi表示。</p><p>由于生物神经元具有不同的突触性质和突触强度，所以对神经元的影响不同，我们用权值 $w_{ij}$ 来表示，其大小则代表了突出的不同连接强度。</p><p>$T_j$ 表示为一个阈值，或称为偏置，超过阈值为兴奋，低于是抑制。</p><p>由于累加性，我们对全部输入信号进行累加整合，相当于生物神经元中的膜电位（水的变化总量），其值就为：</p><p>$$
net\_j&rsquo;(t) = \sum_{i=1}^{n}{w_{ij}X_i(t)} - T_j
$$</p><div id=mlp多层感知机multilayer-perceptron-我们常说的神经网络- class=anchor></div><h3 class="relative group">MLP：多层感知机（Multilayer Perceptron， 我们常说的神经网络 ）
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#mlp%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%bamultilayer-perceptron-%e6%88%91%e4%bb%ac%e5%b8%b8%e8%af%b4%e7%9a%84%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c- aria-label=锚点>#</a></span></h3><p>简单地将神经网络描述为将给定输入映射到所需输出的数学函数更容易理解一下。</p><p>神经网络包含下面几个组件：</p><ul><li>一个输入层（input layer）: x</li><li>一个任意数量的隐藏层（hidden layers）</li><li>一个输出层（output layer）: $ \hat{y} $</li><li>每一个层之间有一个权重（weight）集和偏置（bias）集: W 和 b</li><li>需要为每一个隐藏层选择一个激活函数（activation function）， $\sigma$。这里，我们都是用 Sigmod 作为激活函数。</li></ul><p>下面显示2层神经网络的架构（注意：当计算层数的时候，一般都会把输入层忽略掉）。</p><p><figure><img class="my-0 rounded-md" src=https://cdn-images-1.medium.com/max/1600/1*sX6T0Y4aa3ARh7IBS_sdqw.png alt=2层神经网络><figcaption>一个2层神经网络的架构</figcaption></figure>{:width=&ldquo;800px&rdquo;}</p><div id=开始构建 class=anchor></div><h2 class="relative group">开始构建
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%bc%80%e5%a7%8b%e6%9e%84%e5%bb%ba aria-label=锚点>#</a></span></h2><p>使用 Python 创建一个神经网络的类：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NeuralNetwork</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>input</span>      <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weights1</span>   <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>input</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span><span class=mi>4</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weights2</span>   <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span><span class=mi>1</span><span class=p>)</span>   
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>y</span>          <span class=o>=</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output</span>     <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>y</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><div id=神经网络的训练 class=anchor></div><h2 class="relative group">神经网络的训练
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e8%ae%ad%e7%bb%83 aria-label=锚点>#</a></span></h2><p>一个 2-layer 神经网络的输出 $\hat{y}$ 是：</p><p>$$
\hat{y} = \sigma(W_2\sigma(W_1x+b_1)+b_2)
$$</p><p>你可能注意到上一一个等式，权重 W 和偏置 b 是影响 $\hat{y}$。</p><p>很自然，右边值的权重和偏置决定预测的强弱。使用输入数据对权重和偏置的微调过程，就是我们所说的神经网络的训练。</p><p>训练过程的每一次迭代包含下面的步骤：</p><ul><li>计算预测输出 $\hat{y}$ ，称之为前馈（feedforward）</li><li>更新权重和偏置，称之为反向反馈（backpropagation）</li></ul><p>下面的序列图说明了该过程：</p><p><figure><img class="my-0 rounded-md" src=https://cdn-images-1.medium.com/max/1600/1*CEtt0h8Rss_qPu7CyqMTdQ.png alt=反馈过程></figure>{:width=&ldquo;800px&rdquo;}</p><div id=前馈feedforward class=anchor></div><h3 class="relative group">前馈（feedforward）
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%89%8d%e9%a6%88feedforward aria-label=锚点>#</a></span></h3><p>正如我们所看到的，前馈就是简单的计算，现在看看代码时间</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NeuralNetwork</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>feedforward</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;前馈&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>input</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>weights1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>weights2</span><span class=p>))</span>
</span></span></code></pre></div><p>然而，我们还需要一种评估预测好坏的方式。损失函数（Loss Function）就是这个用途。</p><div id=损失函数loss-function class=anchor></div><h3 class="relative group">损失函数（Loss Function）
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0loss-function aria-label=锚点>#</a></span></h3><p>损失函数有很多种，选择应该由我们问题的性质决定。在这里，我们将 SSE（sum of sqares error，和方差）作为损失函数：</p><p>$$
SSE = \sum_{i=1}^n{(y - \hat{y})^2}
$$
SSE 就是预测值和实际值的差异的和。差异是平方的，所以我们可以使用差的绝对值来衡量。</p><p><strong>我们训练的目标实际找出最佳权重和偏置组合使得损失函数最小化</strong>。</p><div id=反向反馈backpropagation class=anchor></div><h3 class="relative group">反向反馈（Backpropagation）
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e5%8f%8d%e5%90%91%e5%8f%8d%e9%a6%88backpropagation aria-label=锚点>#</a></span></h3><p>现在，我们已经测出了预测的误差（损失），我们需要寻找一种方式将误差反馈回去，来更新我们的权重和偏置。</p><p>为了知道调整权重和偏置的适当数量，我们需要知道损失函数相对于权重和偏置的导数（derivative）。</p><p>回想一下微积分函数的导数就是函数的斜率。</p><p><figure><img class="my-0 rounded-md" src=https://cdn-images-1.medium.com/max/1600/1*3FgDOt4kJxK2QZlb9T0cpg.png alt="gradient descent algorithm"><figcaption>梯度下降算法</figcaption></figure>{:width=&ldquo;800px&rdquo;}</p><p>如果我们有导数，我们就能通过加减它来更新权重和偏置。这就是所谓的梯度下降。</p><p>然而，我们不能直接计算损失函数的导数，因为损失函数不包含权重和偏置。因此，我们需要链规则（chain rule）来帮助我们计算它</p><p>$$
Loss(y, \hat{y}) = \sum_{i=1}^{n}{(y-\hat{y})^2}
$$</p><p>对 W 进行求导：</p><p>$$
\begin{split}
\frac{\partial Loss(y, \hat{y})}{\partial W} &= \frac{\partial Loss(y, \hat{y}}{\partial \hat{y}} * \frac{\partial \hat{y}}{\partial z} * \frac{\partial z}{ \partial W}
\\ &= 2(y-\hat{y}) * sigmoid 函数的导数 * x
\\ &= 2(y-\hat{y}) * z(1-z) * x
\end{split}
$$</p><blockquote><p>计算损失函数对权重的导数的推导。为了简化，我们只展示了一层神经网络的导数。</p></blockquote><p>Phew！虽然很丑，但是这就是我们需要的——损失函数对权重的导数，来帮助我们调整我们的权重。</p><p>现在，我们把反向反馈函数添加到代码中：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NeuralNetwork</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backprop</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;反向反馈&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 计算出各层的导数</span>
</span></span><span class=line><span class=cl>        <span class=n>d_weights2</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> <span class=p>(</span><span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>output</span><span class=p>)</span> <span class=o>*</span> <span class=n>sigmoid_derivative</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>output</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>d_weights1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>input</span><span class=o>.</span><span class=n>T</span><span class=p>,</span>  <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>y</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>output</span><span class=p>)</span> <span class=o>*</span> <span class=n>sigmoid_derivative</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>output</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>weights2</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>*</span> <span class=n>sigmoid_derivative</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 使用导数来更新参数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weights1</span> <span class=o>+=</span> <span class=n>d_weights1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weights2</span> <span class=o>+=</span> <span class=n>d_weights2</span>
</span></span></code></pre></div><div id=整合在一起 class=anchor></div><h3 class="relative group">整合在一起
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%95%b4%e5%90%88%e5%9c%a8%e4%b8%80%e8%b5%b7 aria-label=锚点>#</a></span></h3><p>现在，我们有了完整的神经网络的代码实现。让我们来应用到例子中，看看效果</p><table><thead><tr><th>X1</th><th>X2</th><th>X3</th><th>y</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>1</td><td>1</td><td>0</td></tr></tbody></table><p>我们的神经网络应该学习到立项的权重集来展示这个函数。请注意，仅仅通过检查来计算权重对我们来说并不是微不足道的。</p><p>让我们迭代 1500 次，看看会是什么结果。看看每一次迭代的损失，我们可以明显的看出损失单调递减到一个最小值。这与我们之前讨论过的梯度下降算法一致。</p><p><figure><img class="my-0 rounded-md" src=https://cdn-images-1.medium.com/max/1600/1*fWNNA2YbsLSoA104K3Z3RA.png alt=损失曲线></figure>{:width=&ldquo;800px&rdquo;}</p><p>让我们看一下 1500 次迭代后神经网络最后的预测（输出）：</p><table><thead><tr><th>预测</th><th>Y（实际）</th></tr></thead><tbody><tr><td>0.023</td><td>0</td></tr><tr><td>0.979</td><td>1</td></tr><tr><td>0.975</td><td>1</td></tr><tr><td>0.025</td><td>0</td></tr></tbody></table><p>我们做到了！我们前馈和反向反馈算法成功的训练了一个神经网络，他的预测收敛于真正的价值观。</p><p>注意到了预测值和实际值存在略微差别。这个是合理的，为了避免过拟合，让神经网络对未知数据有更好的泛化能力。</p><div id=接下来呢 class=anchor></div><h2 class="relative group">接下来呢？
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%8e%a5%e4%b8%8b%e6%9d%a5%e5%91%a2 aria-label=锚点>#</a></span></h2><p>后续，我们将会深入讲解激活函数，学习率等等，如果有时间的话^_^。</p><div id=最后的思考 class=anchor></div><h2 class="relative group">最后的思考
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%9c%80%e5%90%8e%e7%9a%84%e6%80%9d%e8%80%83 aria-label=锚点>#</a></span></h2><p>我是从0开始学习，甚至我还用不用的语言构造自己的神经网络，过程是很有趣的。</p><p>虽然像 TensorFlow 和 Keras 这样的深度学习库，让我们不需要对神经网络内部机制有足够的了解，也能构建深度网络，但是能够更深入的了解，对后续的研究更有益。</p></div></div><script>var oid="views_posts/2018-12-06-neural-network.md",oid_likes="likes_posts/2018-12-06-neural-network.md"</script><script type=text/javascript src=/n3xtchen/js/page.min.0e49973b4ad0a382c7c6012d8bff8226316642daabc4f8a20477bd08674f3da6e2fa993bc20ad4f51e7c5bb68e6f913a207a7c4fe37ea0e7b806894afce0a64e.js integrity="sha512-DkmXO0rQo4LHxgEti/+CJjFmQtqrxPiiBHe9CGdPPabi+pk7wgrU9R58W7aOb5E6IHp8T+N+oOe4BolK/OCmTg=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/n3xtchen/2018/11/12/2018-11-12-apt-e-repository-change-its-origin-vale/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">E: Repository 'http://url stable Release' changed its 'Origin' value from 'XX' to 'YY'</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2018-11-12 00:00:00 +0000 UTC">2018-11-12</time></span></span></a></span>
<span><a class="flex text-right group ml-3" href=/n3xtchen/2018/12/11/2018-12-11-os-x-pkgutil/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">OsX: 完全卸载攻略（pkg）</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2018-12-11 00:00:00 +0000 UTC">2018-12-11</time></span></span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label title>&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2023
n3xtchen</p><p class="text-xs text-neutral-500 dark:text-neutral-400">由 <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a> 强力驱动</p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/n3xtchen/js/process.min.35c1113bcc16c5a59bf031082f9e63822aa95280423881a7847a7ff33a16e6299ce6a840d9ef4e10d947e030a18f3f20359afb2ec0f35967484b9a9360ac3145.js integrity="sha512-NcERO8wWxaWb8DEIL55jgiqpUoBCOIGnhHp/8zoW5imc5qhA2e9OENlH4DChjz8gNZr7LsDzWWdIS5qTYKwxRQ=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://n3xtchen.github.io/n3xtchen style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title>
<span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>