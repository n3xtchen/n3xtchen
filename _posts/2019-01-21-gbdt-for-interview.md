---
layout: post
title:  gbdt-for-interview
date:   2019-01-21 19:28:15 +0800
---

# GBDT的原理（知识）

GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。

GBDT中的树是回归树（不是分类树），GBDT用来做回归预测，调整后也可以用于分类。

GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合。

算法的处理流程：

1. 从原始样本中采用有放回抽样的方
2. 目标变量 = 预测结果
2. 开始轮询 m 次:
   2.1. 训练回归树
   2.3. 目标变量_新=目标变量_旧-本轮模型预测结果*学习步长

# 决策树节点分裂时是如何选择特征的？

常见的选择方法：

- 信息熵增益
- 信息熵增益率
- 基尼指数

# 信息熵（Information Gain）和基尼指数（Gini Index）

## 信息熵（Infomation entropy）

度量样本集合纯度的一种常用指标，假如这里一堆男性，里面有你想嫁的有你不想嫁的，而当这堆男性中如果有且仅有你想嫁的或者不想嫁的，那么我们就说这个数据集纯度很高，因为他们都属于同一类。

$$
Entropy(D) = - \sum_{k=1}^{|y|} p_klog_{2}{p_k}; 类别 k = 1,2,...,|y|
$$

D 是样本集合

信息越小，样本集合纯度

### 信息熵增益（Infomation Gain）

信息熵和属性选择又有什么关系呢？已知我们有高，富，帅三个属性，他们的取值为{0，1}，都是离散属性，现在我们试着依次对这三个属性进行划分，最后计算它们划分后的信息增益，最后再根据三种划分的信息增益的大小来确定采用哪个属性进行划分。

$$
Gain(D, a) = Entropy(D) - \sum_{v=1}^{V} \frac{|D^{v}|}{|D|}Entropy(D^v); a: 属性, V: 属性的取值
$$

**ID3** 决策树使用的是信息熵增益

### 信息熵增益率（Gain ratio）

$$
Gain\_ratio = \frac{Gain(D,a)}{IV(a)}, where IV(a) = - \sum_{v=1}^{V} \frac{|D_v|}{D}log_2{\frac{|D_v|}{D}}

$$

**C4.5** 决策树算法使用增益率指标选择划分属性。

## 基尼指数（Gini Index）

样本集合数据集的纯度除了可以用熵来衡量外，也可以用基尼值来度量：

$$
Gini(D) = \sum^{|y|}_{k=1} \sum_{k' \ne k} p_kp_{k'} = 1 - \sum_{k=1}^{|y|}{p_k^2}
$$

基尼值反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，因此基尼值越小，则数据集纯度越高。

对属性 a 进行划分，则属性 a 的基尼指数：

$$
Gini_index(D, a) = \sum_{v=1}^{V} \frac{D^V}{D}Gini(D^v)
$$

因此，在选择划分属性时，应该选择那个使得划分之后基尼指数最小的属性作为划分属性。

**CART** 使用基尼指数作为属性划分指标

# 分类树和回归树的区别是什么？（知识）、

- **分类树** 的输出是样本的类标，例如点和不点，买还是不买，经典的算法有 **ID3** 和 **C4.5**
- **回归树** 的输出是一个实数，例如价格，点击率；**CART** 即是分类树也是决策树

## CART与ID3区别：

如上文介绍，ID3 用的是信息熵增益，CART中用的是 Gini指数；

- 如果目标变量是离散的，并且是具有两个以上的类别，则CART可能考虑将目标类别合并成两个超类别（双化）；
- 如果目标变量是连续的，则CART算法找出一组基于树的回归方程来预测目标变量。

# 与 随机森林（Random Forest） 作比较，并以此介绍什么是模型的

他们的最大区别是随机森林（后面使用 RF 简称）采用的 bagging 思想，而GBDT采用的 boosting 思想。

先来看看 RF 的生产过程：

1. 从原始样本中采用有放回抽样的方法选取n个样本；
2. 对n个样本选取a个特征中的随机k个，用建立决策树的方法获得最佳分割点；
3. 重复m次，获得m个决策树；
4. 对输入样例进行预测时，每个子树都产生一个结果，采用多数投票机制输出。


这两种方法都是 Bootstrap 思想的应用，Bootstrap 是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：

- Bagging采用有放回的均匀取样
- Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。

Bagging 的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而 Boosting 的训练集的选择与前一轮的学习结果有关，是串行的。

组成 RF 的树可以是分类树，也可以是回归树；而 GBDT 只能由回归树组成。

组成 RF 的树可以并行生成；而 GBDT 只能是串行生成。

对于最终的输出结果而言，RF 采用多数投票等；而 GBDT 则是将所有结果累加起来，或者加权累加起来。

RF 对异常值不敏感；GBDT 对异常值非常敏感。

RF 对训练集一视同仁；GBDT 是基于权值的弱分类器的集成。

RF 是通过减少模型方差（Variance）提高性能；GBDT 是通过减少模型偏差（Bias）提高性能。

## 方差（Variance）和偏差（Bias）

### 方差

描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如下图右列所示。

### 偏差

描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，如下图第二行所示。

![方差和偏差](https://liuchengxu.github.io/blog-cn/assets/images/posts/bulls-eye-label-diagram.png)

# GBDT 的参数调优

- max_feature:
- max_depth
- min_samples_split
- min_samples_split
- 

# XGBoost 的正则化是如何实现的


# XGBoost 的并行化部分是如何实现的


# 如果选用一种其他的模型替代XGBoost或者改进XGBoost你会怎么做，为什么？

