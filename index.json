[{"content":"这一阵子，把我的 Gihub Page 迁移到 Hugo，但是总有点舍不得，决定写点东西重温一下。\n看大家都在吐槽无非围绕 Jekyll 难部署，性能差 以及 主题和内容混淆 等问题；作为十年的 Jekyll 用户，没有亲眼看见，不能以讹传讹。针对对这些问题，通过一系列博客，来洗白（也可能是证明） Jekyll；\n这个系列的第一篇从部署维度来看看 Jekyll，先给个结论，==相比 Hugo，它确实没那么方便，但是说很难，就有点过了！== 所以，选择 Hugo 还是 Jekyll，都决定写博客，这个绝对难不倒你，更何况它不难！\n一、Ruby 环境安装（rbenv） # ==假设你已经安装了 Bash/Zsh 和 Git 命令行工具==\n1. 安装和配置 rbenv # rbenv： 安装特定 Ruby 版本的工具；\n安装 rbenv # 从源码中安装：git clone https://github.com/rbenv/rbenv.git ~/.rbenv\n如果你是 Mac 用户，你可以使用 brew install rbenv;\n如果你是 Debian/Ubuntu 用户，你可以使用 sudo apt-get install rbenv\n如果你是 Red Hat 系的 Linux 用户，你可以使用 sudo yum install rbenv\n配置 rbenv # 如果你是 Bash 用户，执行 `echo \u0026rsquo;eval \u0026ldquo;$(~/.rbenv/bin/rbenv init - bash)\u0026rdquo;\u0026rsquo; \u0026raquo; ~/.bashrc\n如果你使用时 Zsh，执行 `echo \u0026rsquo;eval \u0026ldquo;$(~/.rbenv/bin/rbenv init - zsh)\u0026rdquo;\u0026rsquo; \u0026raquo; ~/.zshrc\n==重启你的 Shell 或者 source ~/.bashrc （ZSH 下就是，source ~/.zshrc），就可以开始使用 rbenv 了！==\n2. 安装 Ruby # $ rbenv install 3.1.3 To follow progress, use \u0026#39;tail -f /var/folders/b7/ypb83z053wdb6jnt4rlllhq40000gn/T/ruby-build.20231027143205.94068.log\u0026#39; or pass --verbose Downloading openssl-3.1.4.tar.gz... -\u0026gt; https://dqw8nmjcqpjn7.cloudfront.net/840af5366ab9b522bde525826be3ef0fb0af81c6a9ebd84caa600fea1731eee3 Installing openssl-3.1.4... Installed openssl-3.1.4 to /Users/nextchen/.rbenv/versions/3.1.3 Downloading ruby-3.1.3.tar.gz... -\u0026gt; https://cache.ruby-lang.org/pub/ruby/3.1/ruby-3.1.3.tar.gz Installing ruby-3.1.3... ruby-build: using readline from homebrew ruby-build: using libyaml from homebrew ruby-build: using gmp from homebrew Installed ruby-3.1.3 to /Users/nextchen/.rbenv/versions/3.1.3 $ rbenv local 3.13 # 在项目根目录下，将会生成 .ruby-version，用来记录你使用 ruby 版本 $ rbenv rehash # 每次切换环境的时候都要，在当前环境切换到指定的 ruby 版本 检查你的 ruby 环境：\n$ gem env RubyGems Environment: - RUBYGEMS VERSION: 3.3.26 - RUBY VERSION: 3.1.3 (2022-11-24 patchlevel 185) [arm64-darwin23] - INSTALLATION DIRECTORY: /Users/nextchen/.rbenv/versions/3.1.3/lib/ruby/gems/3.1.0 - USER INSTALLATION DIRECTORY: /Users/nextchen/.gem/ruby/3.1.0 - RUBY EXECUTABLE: /Users/nextchen/.rbenv/versions/3.1.3/bin/ruby - GIT EXECUTABLE: /opt/homebrew/bin/git - EXECUTABLE DIRECTORY: /Users/nextchen/.rbenv/versions/3.1.3/bin - SPEC CACHE DIRECTORY: /Users/nextchen/.gem/specs - SYSTEM CONFIGURATION DIRECTORY: /Users/nextchen/.rbenv/versions/3.1.3/etc - RUBYGEMS PLATFORMS: - ruby - arm64-darwin-23 - GEM PATHS: - /Users/nextchen/.rbenv/versions/3.1.3/lib/ruby/gems/3.1.0 - /Users/nextchen/.gem/ruby/3.1.0 - GEM CONFIGURATION: - :update_sources =\u0026gt; true - :verbose =\u0026gt; true - :backtrace =\u0026gt; false - :bulk_threshold =\u0026gt; 1000 - :sources =\u0026gt; [\u0026#34;https://gems.ruby-china.com/\u0026#34;] - REMOTE SOURCES: - https://gems.ruby-china.com/ # gem 源，你可以替换成本地速度比较快的源 - SHELL PATH: - ...你的系统path 3. 安装 Jekyll 工具 # 安装 Jekyll 的同时会预装 minimal(主题)，CoffeeScript（Javascript 的转译语言） 和 Sass（Css 的转译语言）\n$ gem install jekyll Fetching terminal-table-3.0.2.gem Fetching safe_yaml-1.0.5.gem Fetching rouge-4.2.0.gem Fetching mercenary-0.4.0.gem Fetching webrick-1.8.1.gem Fetching unicode-display_width-2.5.0.gem Fetching forwardable-extended-2.6.0.gem Fetching pathutil-0.16.2.gem Fetching liquid-4.0.4.gem Fetching kramdown-2.4.0.gem Fetching kramdown-parser-gfm-1.1.0.gem Fetching ffi-1.16.3.gem Fetching rb-inotify-0.10.1.gem Fetching rb-fsevent-0.11.2.gem Fetching listen-3.8.0.gem Fetching jekyll-watch-2.2.1.gem Fetching google-protobuf-3.24.4-arm64-darwin.gem Fetching sass-embedded-1.69.5-arm64-darwin.gem Fetching jekyll-sass-converter-3.0.0.gem Fetching concurrent-ruby-1.2.2.gem Fetching i18n-1.14.1.gem Fetching http_parser.rb-0.8.0.gem Fetching eventmachine-1.2.7.gem Fetching em-websocket-0.5.3.gem Fetching jekyll-4.3.2.gem Fetching colorator-1.1.0.gem Fetching public_suffix-5.0.3.gem Fetching addressable-2.8.5.gem ... Successfully installed jekyll-4.3.2 ... Installing ri documentation for jekyll-4.3.2 Done installing documentation for webrick, unicode-display_width, terminal-table, safe_yaml, rouge, forwardable-extended, pathutil, mercenary, liquid, kramdown, kramdown-parser-gfm, ffi, rb-inotify, rb-fsevent, listen, jekyll-watch, google-protobuf, sass-embedded, jekyll-sass-converter, concurrent-ruby, i18n, http_parser.rb, eventmachine, em-websocket, colorator, public_suffix, addressable, jekyll after 13 seconds 28 gems installed 二、Jekyll 站点初始化和配置1 # 1. 初始化项目目录 # ==需要指定 \u0026ndash;skip-bundle，因为我们要调整 Gem 配置和包！==\n$ jekyll new --skip-bundle . New jekyll site installed in /private/tmp/x. Bundle install skipped. # 需要跳过，我们需要修改文件 目录结构如下：\n. ├── .gitignore ├── 404.html ├── Gemfile ├── _config.yml ├── _posts # 你博客编写的地方 │ └── 2023-11-01-welcome-to-jekyll.markdown ├── about.markdown └── index.markdown 2. 配置 Gem 源，加速下载 # 修改你的 Gemfile ,配置更快的 Gem 源，如果你在国内（==用默认会很不稳定==），你可以配置成 https://gems.ruby-china.com/:\n# source \u0026#34;https://rubygems.org\u0026#34; ## 注释掉这行 source \u0026#34;https://gems.ruby-china.com/\u0026#34; ## 修改源，加速依赖安装 ... 3. 添加 webbrick（Ruby 3.0 以下，请跳过） # 由于 Ruby 版本3及以上，不再默认安装 webbrick ，所以你需要手动安装它:\n$ bundle add webrick Fetching gem metadata from https://gems.ruby-china.com/........... Resolving dependencies... Resolving dependencies... Resolving dependencies... Resolving dependencies... Resolving dependencies.... Resolving dependencies... Fetching gem metadata from https://gems.ruby-china.com/......... Resolving dependencies... ... 4. 项目依赖安装 # $ bundle install ... Bundle complete! 8 Gemfile dependencies, 94 gems now installed. Use `bundle info [gemname]` to see where a bundled gem is installed. 如果出现如下警告, bundle add rubyzip --version \u0026quot;2.3.0\u0026quot;:\nPost-install message from rubyzip: RubyZip 3.0 is coming! ********************** The public API of some Rubyzip classes has been modernized to use named parameters for optional arguments. Please check your usage of the following classes: * `Zip::File` * `Zip::Entry` * `Zip::InputStream` * `Zip::OutputStream` Please ensure that your Gemfiles and .gemspecs are suitably restrictive to avoid an unexpected breakage when 3.0 is released (e.g. ~\u0026gt; 2.3.0). See https://github.com/rubyzip/rubyzip for details. The Changelog also lists other enhancements and bugfixes that have been implemented since version 2.3.0. 最终的目录结构和 Gemfile 配置 # 目录结构如下（==注意：Gemfile.lock 需要纳入版本控制==）：\n$ tree . ├── 404.html ├── Gemfile ├── Gemfile.lock ├── _config.yml ├── _posts │ └── 2023-11-01-welcome-to-jekyll.markdown ├── about.markdown └── index.markdown Gemfile(## 是我添加的注释，注释的行都是有修改)如下：\n# source \u0026#34;https://rubygems.org\u0026#34; ## 注释掉这行 source \u0026#34;https://gems.ruby-china.com/\u0026#34; ## 修改源，加速依赖安装 # Hello! This is where you manage which Jekyll version is used to run. # When you want to use a different version, change it below, save the # file and run `bundle install`. Run Jekyll with `bundle exec`, like so: # # bundle exec jekyll serve # # This will help ensure the proper Jekyll version is running. # Happy Jekylling! # gem \u0026#34;jekyll\u0026#34;, \u0026#34;~\u0026gt; 4.3.2\u0026#34; ## 注释掉，我们将使用 GitHub Pages # This is the default theme for new Jekyll sites. You may change this to anything you like. gem \u0026#34;minima\u0026#34;, \u0026#34;~\u0026gt; 2.5\u0026#34; # If you want to use GitHub Pages, remove the \u0026#34;gem \u0026#34;jekyll\u0026#34;\u0026#34; above and # uncomment the line below. To upgrade, run `bundle update github-pages`. gem \u0026#34;github-pages\u0026#34;, group: :jekyll_plugins # 关闭注释，我们需要部署 GitHub Pages # If you have any plugins, put them here! group :jekyll_plugins do gem \u0026#34;jekyll-feed\u0026#34;, \u0026#34;~\u0026gt; 0.12\u0026#34; end # Windows and JRuby does not include zoneinfo files, so bundle the tzinfo-data gem # and associated library. platforms :mingw, :x64_mingw, :mswin, :jruby do gem \u0026#34;tzinfo\u0026#34;, \u0026#34;\u0026gt;= 1\u0026#34;, \u0026#34;\u0026lt; 3\u0026#34; gem \u0026#34;tzinfo-data\u0026#34; end # Performance-booster for watching directories on Windows gem \u0026#34;wdm\u0026#34;, \u0026#34;~\u0026gt; 0.1.1\u0026#34;, :platforms =\u0026gt; [:mingw, :x64_mingw, :mswin] # Lock `http_parser.rb` gem to `v0.6.x` on JRuby builds since newer versions of the gem # do not have a Java counterpart. gem \u0026#34;http_parser.rb\u0026#34;, \u0026#34;~\u0026gt; 0.6.0\u0026#34;, :platforms =\u0026gt; [:jruby] gem \u0026#39;rubyzip\u0026#39;, \u0026#39;2.3.0\u0026#39; # 新增 gem \u0026#34;webrick\u0026#34;, \u0026#34;~\u0026gt; 1.8\u0026#34; # 新增，解决 jekyll 无法启动问题 5. 启动 Jekyll # --incremental 表示文件内容变动就会自动编译\n$ bundle exec jekyll serve --incremental Configuration file: /private/tmp/x/_config.yml To use retry middleware with Faraday v2.0+, install `faraday-retry` gem Source: /private/tmp/x Destination: /private/tmp/x/_site Incremental build: disabled. Enable with --incremental Generating... Jekyll Feed: Generating feed for posts done in 0.098 seconds. Auto-regeneration: enabled for \u0026#39;/private/tmp/x\u0026#39; Server address: http://127.0.0.1:4000/ Server running... press ctrl-c to stop. 问题：Deprecation Warning: Using / for division outside of calc() is deprecated and will be removed in Dart Sass 2.0.0. 解决方法：bundle add \u0026quot;jekyll-sass-converter\u0026quot; -v \u0026quot;~\u0026gt; 2.0\u0026quot;; bundle install; 重新启动服务即可。\n三、编写博客 # 1. 博客文件名 # 博客文件：yyyy-mm-dd-{文件名}.markdown 例如模版给我们的 2023-11-01-welcome-to-jekyll.markdown； 文件名最好可以描述你博客缩写的关键词，因为他会作为 URL 链接 上述例子对应的链接就是 /2023/11/01/welcome-to-jekyll 2. 博客文件内容 # 他是 Markdown 文件，下面是例子：\n--- layout: post # 博客页面的模版 title: \u0026#34;Welcome to Jekyll!\u0026#34; # 博客标题 date: 2023-11-01 19:09:20 +0800 categories: jekyll update # 博客分类 --- 正文部分 正文部分可以使用 Markdown 语法编写你的博客\nFront-Matter # 被 --- 包围住的内容，我们称之为 Front-Matter，他的编写格式符合 Yaml 规范：\nlayout、title 是必须的； date 是可选的，格式为 YYYY-MM-DD HH:MM:SS +/-TTTT，如果设置了，会代替文件名上时间，作为 URL 链接的一部分 categories: 表示分类，如果设置了，会出现在 URL 链接，那上一个做例子，如果设置了，URL 链接就会变成 /jekyll/update/2023/11/01/welcome-to-jekyll tags：你还可以设置标签 四、部署到 github page 上 # 1. 创建 Github 代码库 # 创建一个 Github Repo，名叫 \u0026lt;你的 blog 名称\u0026gt;\n2. 将你的代码提交 Github 中2 # cd \u0026lt;你的创建 jekyll\u0026gt; git init . git checkout --orphan gh-pages # 你博客存放的分支，可以是任何分支，大家习惯用这个分支名 git add . # 把你刚才生成和编写的内容提交上 git remote add origin https://github.com/\u0026lt;你的git用户名\u0026gt;/\u0026lt;你的blog名称\u0026gt; git push -u origin gh-pages # 将你的代码推送到 Github 中 3. 设置发布分支3 # 登陆 Githubt，访问 https://github.com/\u0026lt;你的 Github 用户名\u0026gt;/\u0026lt;你的博客 Repo 名称\u0026gt;； 点击顶部导航栏的 Settings，进入左侧的导航栏 Code and aution 下的 Page 配置页面; 定位 Build and delpument 下； 选中 Source 中的 Deploy from a branch ； 在 Branch 中选择你的博客所在的分区，这里我们选择 gh-pages； 点击 Save 等待部署好，你可以提供通过 \u0026lt;你的 Github 用户名\u0026gt;.github.io/\u0026lt;你的博客 Repo 名称\u0026gt;\nTesting your GitHub Pages site locally with Jekyll - GitHub Docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCreating a GitHub Pages site - GitHub Docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nConfiguring a publishing source for your GitHub Pages site - GitHub Docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-10-27","permalink":"/n3xtchen/2023/10/27/github-page-jekyll-build-n-deployment/","section":"时间线","summary":"这一阵子，把我的 Gihub Page 迁移到 Hugo，但是总有点舍不得，决定写点东西重温一下。","title":"Jekyll：GitHub Page 的本地环境搭建和部署"},{"content":"","date":"2023-10-26","permalink":"/n3xtchen/posts/","section":"时间线","summary":"","title":"时间线"},{"content":"","date":"2023-10-26","permalink":"/n3xtchen/tags/","section":"标签","summary":"","title":"标签"},{"content":"","date":"2023-10-26","permalink":"/n3xtchen/categories/","section":"分类页","summary":"","title":"分类页"},{"content":" 在这里，一同享受技术为我们带来无限乐趣！ ","date":"2023-10-24","permalink":"/n3xtchen/","section":"","summary":" 在这里，一同享受技术为我们带来无限乐趣！ ","title":""},{"content":"从 2013 年开始，我已经使用 Jekyll 十个年头了，当时对 Ruby 极其狂热，也是我选择他的原因，中间也换过一次主题，其实没什么不好，迁移的原因是因为 hugo-obsidian 用起来挺不错（虽然 Jekyll 也有类似，但是感觉不够丝滑），我可以使用无缝使用 Obsidian 编写我的博客。\n想了太多，做的太少，琢磨小半年了，终于动起来，其实也不麻烦，前前后后从动手到完成也就花了 4～5天的时间。下面， 我分享下整个迁移过程、遇到的坑和相应解决方案。\n一、创建站点 # 执行命令 hugo new site n3xtchen（n3xtchen 可以替换成你想要的任何名称，就是你 Hugo 项目的根目录），下面是产生的内容\nn3xtchen/ ├── README.md ├── archetypes # 文档模版，被 hugo new content 使用 │ └── default.md ├── assets # 用于存储 图片/样式（css/sass）/javascript/typescript ├── hugo.toml # 配置文件 ├── content # 文档存储路径 ├── data ├── i18n # 多语言支持 ├── layouts ├── static # 该目录下的内容直接复制到 public 目录中 └── themes # 主题目录 二、安装主题（使用 hugo mod） # 1. 初始化模块： # $ hugo mod init github.com/n3xtchen/n3xtchen go: creating new go.mod: module github.com/n3xtchen/n3xtchen go: to add module requirements and sums: go mod tidy $ tree . ├── ... ├── go.mod # 产生的新文件 ├── ... 实际底层是执行 go init\n==注意：需要检查下 go.mod 中 go 的版本格式，必须是 xx.yy，在我的系统中，版本格式 go 1.21.3，那我就需要改成 go 1.21，因为我使用 github action 的 hugo(peaceiris/actions-hugo@v2)编译器不支持这种格式（xx.yy.zz） ==\n2. 增加主题模块配置（我使用的主题是 blowfish） # $ mkdir config/_default $ tee -a config/_default/module.toml \u0026lt;\u0026lt;END [[imports]] path = \u0026#34;github.com/nunocoracao/blowfish/v2\u0026#34; END $ tree . ├── ... ├── config # 产生的新文件 │ └── _default │ └── modules.toml ├── ... 3. 下载主题 # 启动 hugo 服务，就会自动下载主题：\n$ hugo server go: no module dependencies to download hugo: downloading modules … go: added github.com/nunocoracao/blowfish/v2 v2.43.0 # 说明他在获取主题文件 hugo: collected modules in 11799 ms Watching for changes in /Users/nextchen/Dev/project_pig/n3xtchen/{archetypes,assets,content,data,i18n,layouts,static} Watching for config changes in /Users/nextchen/Dev/project_pig/n3xtchen/config/_default, /Users/nextchen/Dev/project_pig/n3xtchen/go.mod Start building sites … hugo v0.119.0-b84644c008e0dc2c4b67bd69cccf87a41a03937e+extended darwin/arm64 BuildDate=2023-09-24T15:20:17Z VendorInfo=brew | EN -------------------+----- Pages | 7 Paginator pages | 0 Non-page files | 0 Static files | 8 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 29 ms Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at //localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop hugo 模块的缓存目录下：${缓存根目录}/modules/filecache/modules/pkg/mod/ 查看缓存 目录的方法：hugo config | greo cache_dir blowfish 的存储位置：${hugo 模块的缓存目录}/github.com/nunocoracao/blowfish\n$ tree . ├── ... ├── .hugo_build.lock # hugo 编译的锁文件 ├── go.sum # go 依赖模块的 hash 值 ├── ... 下载和复制主题相关的配置模版 # 删除默认配置：rm hugo.toml 复制 blowfish 的配置模版： cp ${hugo 模块的缓存目录}/github.com/nunocoracao/blowfish/v2@v2.43.0/config/_default/{config.toml,languages.en.toml,markup.toml,menus.en.toml,params.toml} config/_default/ ==注意不要覆盖：module.toml== 将 config/_default/config.toml 的主题设置成 github.com/nunocoracao/blowfish/v2 ，==如果使用 hugo module 安装主题的时候，这一步很重要，否者 gh-page 部署的时候，样式会丢失！ $ tree . ├── ... ├── config │ └── _default │ ├── config.toml # 新增 │ ├── languages.en.toml # 新增 │ ├── markup.toml # 新增 │ ├── menus.en.toml # 新增 │ ├── module.toml # 不能被覆盖 │ └── params.toml # 新增 ├── ... ├── hugo.toml # 该文件会被删除 ├── ... 版本控制应该改忽略的文件： Hugo.gitignore\n三、自定义主页和中文支持 # 1. 增加中文支持 # $ tee config/_default/languages.zh-CN.toml \u0026lt;\u0026lt;END languageCode = \u0026#34;zh-CN\u0026#34; languageName = \u0026#34;Simplified Chinese (China)\u0026#34; weight = 1 title = \u0026#34;N3xtChen 的博客\u0026#34; # 你博客的标题 [author] # 添加作者信息 name = \u0026#34;n3xtchen\u0026#34; image = \u0026#34;/images/author.jpg\u0026#34; # 存储在 assets 下 headline = \u0026#34;和你分享有趣的技术！\u0026#34; bio = \u0026#34;Sharing Funny Tech With You\u0026#34; links = [ { twitter = \u0026#34;https://twitter.com/mN3XT\u0026#34; }, { github = \u0026#34;https://githhub.com/n3xtchen\u0026#34; } ] [params] displayName = \u0026#34;CN\u0026#34; isoCode = \u0026#34;zh-cn\u0026#34; rtl = false dateFormat = \u0026#34;2006-01-02\u0026#34; END $ sed -i \u0026#39;s/\\(defaultContentLanguage =\\).*/\\1 \u0026#34;zh-CN\u0026#34;/g\u0026#39; config/_default/config.toml 2. 生成主页 # 首页增加 Banner\n$ hugo new content _index.md Content \u0026#34;/Users/nextchen/Dev/project_pig/n3xtchen/content/_index.md\u0026#34; created $ tee -a content/_index.md \u0026lt;\u0026lt;END {{\u0026lt; alert icon=\u0026#34;rss\u0026#34; \u0026gt;}} 在这里，一同享受技术为我们带来无限乐趣！ {{\u0026lt; /alert \u0026gt;}} END 3. 定制主页 # 修改的功能：\n首页展示近期的发表文章（5条）和显示更多文章链接 文章列表显示分类信息 文章内显示分类信息、目录和提供社交分享链接 具体修改如下（config/_default/params.toml）：\n... [homepage] ... showRecent = true # 需要修改，打开，在主页显示近期发布的文章 showRecentItems = 5 showMoreLink = true. # 需要修改，打开，在主页显示更多文章按钮 showMoreLinkDest = \u0026#34;/posts\u0026#34; ... [article] ... showTableOfContents = true # 需要修改，显示文章目录 # showRelatedContent = false # relatedContentLimit = 3 showTaxonomies = true # 需要修改，显示分类和标签 showAuthorsBadges = false showWordCount = true sharingLinks = [ \u0026#34;linkedin\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;reddit\u0026#34;, \u0026#34;pinterest\u0026#34;, \u0026#34;facebook\u0026#34;, \u0026#34;email\u0026#34;, \u0026#34;whatsapp\u0026#34;, \u0026#34;telegram\u0026#34;] # 需要修改，提供分享链接 ... 4. 定制网站图标 # 首先，将你制作好的图标保存到 static 中； 然后，在 layouts/partials/ 创建 favicons.html，内容如下\n\u0026lt;link rel=\u0026#34;icon\u0026#34; href=\u0026#34;{{\u0026#34;favicon.png\u0026#34; | relURL}}\u0026#34; type=\u0026#34;image/png\u0026#34;\u0026gt; 5. 定制菜单 # 在顶部菜单栏，增加博客、分类页和标签页入口\n$ tee config/_default/menus.zh-CN.toml \u0026lt;\u0026lt;END [[main]] name = \u0026#34;时间线\u0026#34; pageRef = \u0026#34;posts\u0026#34; weight = 10 [[main]] name = \u0026#34;分类\u0026#34; pageRef = \u0026#34;categories\u0026#34; weight = 20 [[main]] name = \u0026#34;标签\u0026#34; pageRef = \u0026#34;tags\u0026#34; weight = 30 END 这个时候，相应页面使用的默认模版，可以创建对应 section 页面来定制\nhugo new content posts/_index.md # 定制博客列表主页 hugo new content categories/_index.md # 定制分类页主页 hugo new content tags/_index.md # 定制标签页主页 四、Jekyll 迁移文档 # 1. 将 Jekyll 的文档转化成 Hugo 格式 # $ git checkout gh-pages $ mkdir /tmp/jekyll_to_hugo $ hugo import jekyll . /tmp/jekyll_to_hugo $ cp /tmp/jekyll_to_hugo/content/{post,draft}/* content/posts/ 2. 保持和 Jekyll 一样链接格式（建议） # ==如果你之前的文章被其他站点引用，或者你想保持原有的搜索排名，建议沿用之前的链接格式！==\n功能如下（jekyll 的习惯）：\n将文件名的日期作为 date 的值1 链接格式：/年/月/日/文件名 文件名：2021-01-01-file_name，对应的链接为：/2021/01/01/file_name $ tee -a config/_default/config.toml \u0026lt;\u0026lt;END [frontmatter] date = [\u0026#39;:filename\u0026#39;, \u0026#39;:default\u0026#39;] # 如果文件名有日期，读取文件名，否则读取 frontmatter 中的日期 [permalinks] posts = \u0026#39;/:year/:month/:day/:slug/\u0026#39; # slug 获取文件名时，忽略日期 END 3. 去除 Jekyll-Bootstrap （可选） # ==如果你使用了 JB，可以往下，否则跳过！==\n我之前使用 Bootstrap 主题，所以文章主体头部都会加上 {% include JB\\/setup %}，现在不需要了，所以需要去除，不然都会被展示出来\nsed -i \u0026#39;\u0026#39; \u0026#39;/{% include JB\\/setup %}/d\u0026#39; content/posts/* 4. 增加 mathjax 支持（可选）2 # ==如果你有需要公式渲染需求，可以往下，否则跳过！==\n复制 blowfish 的 head.html，用于后续的修改\n$ mkdir layouts/partials/ $ cp ${hugo 模块的缓存目录}/github.com/nunocoracao/blowfish/v2@v2.43.0/layouts/partials/head.html layouts/partials/ 在 layouts/partials 下创建 mathjax.html 实现 Latex 渲染功能，内容如下3：\n\u0026lt;style\u0026gt; code.has-jax { -webkit-font-smoothing: antialiased; background: inherit !important; border: none !important; font-size: 100%; } \u0026lt;/style\u0026gt; \u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [[\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;]], displayMath: [[\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;]], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: [\u0026#39;script\u0026#39;, \u0026#39;noscript\u0026#39;, \u0026#39;style\u0026#39;, \u0026#39;textarea\u0026#39;, \u0026#39;pre\u0026#39;] } }; window.addEventListener(\u0026#39;load\u0026#39;, (event) =\u0026gt; { document.querySelectorAll(\u0026#34;mjx-container\u0026#34;).forEach(function(x){ x.parentElement.classList += \u0026#39;has-jax\u0026#39;}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 修改 head.html，让浏览器启动 Mathjax 渲染功能：\n\u0026lt;head\u0026gt; ... {{/* 增加 mathjax3 支持 */}} {{ if .Params.mathjax }}{{ partial \u0026#34;mathjax.html\u0026#34; }}{{ end }} # 新增的 \u0026lt;/head\u0026gt; 在 config/_default/params.toml 打开配置\n... mathjax = true # 新增的 [header] ... 我这边仍然出现如下问题，需要通过修改文档才能解决： # ==当行公式会被识别成公式块== 原因是 Jekyll 单行 Latex $$x_i$$ 会被解析成 \\(x_i\\)，而在 Hugo 就不会自动转，所以需要修改成 $x_i$ 下面代码中是找出可能出现问题的文件: grep -rnw '\\$\\$' content/posts ==// 不会换行== 原因是 Hugo 转移 /，所以需要 //// 替换他 五、部署到 github-page # 创建 .github/workflows/hugo-gh-page.yml4,内容如下：\nname: Hugo GitHub Pages on: push: branches: - main # 你 Hugo 项目所在的分支 jobs: build-deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - name: Checkout uses: actions/checkout@v3 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#34;latest\u0026#34; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == \u0026#39;refs/heads/main\u0026#39; }} # 你 Hugo 项目所在的分支 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: gh-pages # 你配置的 github-page 的分支 publish_dir: ./public github-page 分支的设置： Configuring a publishing source for your GitHub Pages site - GitHub Docs\nConfigure Hugo | Hugo-Configure dates\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRender LaTeX math expressions in Hugo with MathJax 3 · Geoff Ruddock\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMathJax | Beautiful math in all browsers.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHosting \u0026amp; Deployment · Blowfish\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-10-18","permalink":"/n3xtchen/2023/10/18/jekyll-migrate-to-hugo/","section":"时间线","summary":"从 2013 年开始，我已经使用 Jekyll 十个年头了，当时对 Ruby 极其狂热，也是我选择他的原因，中间也换过一次主题，其实没什么不好，迁移的原因是因为 hugo-obsidian 用起来挺不错（虽然 Jekyll 也有类似，但是感觉不够丝滑），我可以使用无缝使用 Obsidian 编写我的博客。","title":"终于我的 Gihub Page 从 Jekyll 迁移到 Hugo"},{"content":"为了玩大模型，努力释放 Macbook 的空间，蚊子腿也是肉。现在看下我们 Homebrew 的目录结构：\n/opt/homebrew ├── [ 8.9 GiB] Cellar/ │ ├── [ 1.5 GiB] llvm │ ├── [ 1.3 GiB] rust │ ├── [527.7 MiB] qumu │ ├── [526.8 MiB] boost │ └── ... ├── [974.8 MiB] Library │ ├── [930.5 MiB] Taps │ │ ├─── [930 MiB] homebrew │ │ │ ├── [487.4 MiB] homebrew-core │ │ │ ├── [384.8 MiB] homebrew-cask │ │ │ └── ... │ │ └── ... │ └── ... └── ... ...: 代表小于 500 GiB 的文件目录\n目录占用情况 # 现在我们来分析分析：\n占用最大是 Cellar：用来存储我们的安装的程序 占用第二是 Library: 包含 Homebrew 的主程序和软件源 缓存目录（brew --cache）：/Users/\u0026lt;你的用户名\u0026gt;/Library/Caches/Homebrew 其他的占用都小于 500 GiB 开始释放空间 # 1. 清楚无用的依赖文件 # 卸载作为其他软件的依赖软件，并且它不会被使用。\n$ brew autoremove ==\u0026gt; Autoremoving 5 unneeded formulae: abseil cmocka heroku/brew/heroku-node lua@5.3 protobuf Uninstalling /opt/homebrew/Cellar/cmocka/1.1.7... (17 files, 194.7KB) Uninstalling /opt/homebrew/Cellar/heroku-node/14.19.0... (6 files, 73.5MB) Uninstalling /opt/homebrew/Cellar/lua@5.3/5.3.6... (28 files, 407.2KB) Uninstalling /opt/homebrew/Cellar/protobuf/23.4... (389 files, 12.2MB) Uninstalling /opt/homebrew/Cellar/abseil/20230125.3... (716 files, 10.0MB) 2. 清除缓存 # 清除旧版的缓存：brew cleanup -s，这个操作只会保留安装软件的最新安装包，并把旧版本的安装包删除 清楚所有的缓存: rm -r \u0026quot;$(brew --cache)\u0026quot; 3. 清除 Taps（适用于 3.6 以上的版本） # 4.0.0 — Homebrew\n从 Homebrew 3.6 开始，为了加速官方维护的 Tap（第三方软件源）的更新，将更新方式从 git clone 迁移到 json 下载的方式。\n官方维护的 Tap ：\nhomebrew/bundle homebrew/cask 从上面文件目录看，除了安装软件本身，第二大占用就是官方的这两个 Tap 了。\n$ brew untap homebrew/cask homebrew/core Untapping homebrew/cask... Untapped 4255 casks (4,329 files, 371.3MB). Untapping homebrew/core... Untapped 3 commands and 6674 formulae (7,043 files, 474.5MB). 结语 # 运气好可以省出一个 7 亿参数规模的 Llama2 模型的空间，如果你的 Homebrew 平时不怎么管理的，那估计能省更多空间出来。祝好运！\n","date":"2023-08-04","permalink":"/n3xtchen/2023/08/04/homebrew-free-disk/","section":"时间线","summary":"为了玩大模型，努力释放 Macbook 的空间，蚊子腿也是肉。现在看下我们 Homebrew 的目录结构：","title":"Homebrew 空间释放"},{"content":"","date":"2020-12-04","permalink":"/n3xtchen/categories/scala/","section":"分类页","summary":"","title":"Scala"},{"content":"","date":"2020-12-04","permalink":"/n3xtchen/tags/spark/","section":"标签","summary":"","title":"spark"},{"content":"Jupyter 和 Apache Zeppelin 是一个数据处理体验比较好的地方。不幸的是，notebooks 的特点决定了他不擅长组织代码，包括去耦合（decomposition）和可读性。我们要将代码复制到 IDE 上，然后编译成 JAR，但是效果不是很好。接下来，我们将会讲如何在 IDE 中编写可读性更高的代码。\n1. 编写基础代码 # 这是一个简单的例子：\n下载杂货店数据文件 过滤出水果 格式化名称 统计每一个水果的数量 看看代码实现：\nval spark = SparkSession .builder .appName(\u0026quot;MyAwesomeApp\u0026quot;) .master(\u0026quot;local[*]\u0026quot;) .getOrCreate() import spark.implicits._ val groceries = spark.read .option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;) .option(\u0026quot;header\u0026quot;, \u0026quot;true\u0026quot;) .csv(\u0026quot;some-data.csv\u0026quot;) val sumOfFruits = groceries .filter($\u0026quot;type\u0026quot; === \u0026quot;fruit\u0026quot;) .withColumn(\u0026quot;normalized_name\u0026quot;, lower($\u0026quot;name\u0026quot;)) .groupBy(\u0026quot;normalized_name\u0026quot;) .agg( sum(($\u0026quot;quantity\u0026quot;)).as(\u0026quot;sum\u0026quot;) ) val fruits = groceries.filter($\u0026quot;type\u0026quot; === \u0026quot;fruit\u0026quot;) val normalizedFruits = fruits.withColumn(\u0026quot;normalized_name\u0026quot;, lower($\u0026quot;name\u0026quot;)) val sumOfFruits = normalizedFruits .groupBy(\u0026quot;normalized_name\u0026quot;) .agg( sum(($\u0026quot;quantity\u0026quot;)).as(\u0026quot;sum\u0026quot;) ) sumOfFruits.show() 2. 提取方法 # 创建方法和每一步业务关联，如果你使用的是 IDE，从选中的代码中创建方法应该很简单。\ndef main(args: Array[String]) { val spark = SparkSession .builder .appName(\u0026quot;MyAwesomeApp\u0026quot;) .master(\u0026quot;local[*]\u0026quot;) .getOrCreate() import spark.implicits._ val groceries: DataFrame = getGroceries val fruits: Dataset[Row] = filterFruits(groceries) val normalizedFruits: DataFrame = withNormalizedName(fruits) val sumOfFruits: DataFrame = sumByNormalizedName(normalizedFruits) sumOfFruits.show() } private def sumByNormalizedName(normalizedFruits: DataFrame) = { val sumOfFruits = normalizedFruits .groupBy(\u0026quot;normalized_name\u0026quot;) .agg( sum(($\u0026quot;quantity\u0026quot;)).as(\u0026quot;sum\u0026quot;) ) sumOfFruits } private def withNormalizedName(fruits: Dataset[Row]) = { val normalizedFruits = fruits.withColumn(\u0026quot;normalized_name\u0026quot;, lower($\u0026quot;name\u0026quot;)) normalizedFruits } private def filterFruits(groceries: DataFrame) = { val fruits = groceries.filter($\u0026quot;type\u0026quot; === \u0026quot;fruit\u0026quot;) fruits } private def getGroceries: DataFrame = { val groceries = spark.read .option(\u0026quot;inferSchema\u0026quot;,\u0026quot;true\u0026quot;) .option(\u0026quot;header\u0026quot;,\u0026quot;true\u0026quot;) .csv(\u0026quot;some-data.csv\u0026quot;) groceries } main 函数的代码已经更可读了吧。。。但是这个代码无法执行。我们需要在有些方法中使用 SparkSession 和 spark.implicits._ 。但是这些值没在方法的作用于内。\nprivate def getGroceries: DataFrame = { val groceries = spark.read .option(\u0026quot;inferSchema\u0026quot;,\u0026quot;true\u0026quot;) .option(\u0026quot;header\u0026quot;,\u0026quot;true\u0026quot;) .csv(\u0026quot;some-data.csv\u0026quot;) groceries } 2. 无尽的 SparSession # 我们可以通过传参的方式来解决这个问题。但是，这种方法不够优雅，而且蛋疼。我们还需要每次都要导入 spark.implicits._。但是程序员毕竟还是懒惰的。\nprivate def sumByNormalizedName(normalizedFruits: DataFrame, spark: SparkSession) = { import spark.implicits._ val sumOfFruits = normalizedFruits .groupBy(\u0026quot;normalized_name\u0026quot;) .agg( sum(($\u0026quot;quantity\u0026quot;)).as(\u0026quot;sum\u0026quot;) ) sumOfFruits } private def withNormalizedName(fruits: Dataset[Row], spark: SparkSession) = { import spark.implicits._ val normalizedFruits = fruits.withColumn(\u0026quot;normalized_name\u0026quot;, lower($\u0026quot;name\u0026quot;)) normalizedFruits } private def filterFruits(groceries: DataFrame, spark: SparkSession) = { import spark.implicits._ val fruits = groceries.filter($\u0026quot;type\u0026quot; === \u0026quot;fruit\u0026quot;) fruits } private def getGroceries(spark: SparkSession): DataFrame = { val groceries = spark.read .option(\u0026quot;inferSchema\u0026quot;,\u0026quot;true\u0026quot;) .option(\u0026quot;header\u0026quot;,\u0026quot;true\u0026quot;) .csv(\u0026quot;some-data.csv\u0026quot;) groceries } 3. 封装你的 SparkSession # 我们提供一种稍稍不同的 SparkSession 访问方式，这样代码就更简洁了。\npackage org.nextchen.demo.base import org.apache.spark.sql.SparkSession trait SparkJob { val spark: SparkSession = SparkSession .builder .appName(\u0026quot;SomeApp\u0026quot;) .master(\u0026quot;local[*]\u0026quot;) } object SparkJob extends SparkJob {} 现在，我们可以在应用中引入 SparkJob 和 spark.implicits._。这样，代码看起来好多了。我们也可以复用它。\nimport org.apache.spark.sql._ import org.apache.spark.sql.functions._ import org.nextchen.demo.base.SparkJob import org.nextchen.demo.base.spark.implicits._ object NiceApp { val spark = SparkJob.spark def main(args: Array[String]) = { val groceries: DataFrame = getGroceries val fruits: Dataset[Row] = filterFruits(groceries) val normalizedFruits: DataFrame = addNormalizedNameColumn(fruits) val sumOfFruits: DataFrame = sumByNormalizedName(normalizedFruits) sumOfFruits.show() } private def sumByNormalizedName(normalizedFruits: DataFrame) = { val sumOfFruits = normalizedFruits .groupBy(\u0026quot;normalized_name\u0026quot;) .agg( sum(($\u0026quot;quantity\u0026quot;)).as(\u0026quot;sum\u0026quot;) ) sumOfFruits } private def addNormalizedNameColumn(fruits: Dataset[Row]) = { val normalizedFruits = fruits.withColumn(\u0026quot;normalized_name\u0026quot;, lower($\u0026quot;name\u0026quot;)) normalizedFruits } private def filterFruits(groceries: DataFrame) = { val fruits = groceries.filter($\u0026quot;type\u0026quot; === \u0026quot;fruit\u0026quot;) fruits } private def getGroceries: DataFrame = { val groceries = spark.read .option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;) .option(\u0026quot;header\u0026quot;, \u0026quot;true\u0026quot;) .csv(\u0026quot;some-data.csv\u0026quot;) groceries } } 4. 隐式类（Implicit class） # 如果你深入使用过动态类型语言（如 Python、Ruby）的话，应该对 猴子布丁（Monkey Patch） 的概念不会陌生，你可以动态为存在的类型添加方法，而不用改变它。隐式类就是 Scala 的猴子布丁，C# 的 Extension Method 也是类似的概念。不理解没关系，看看例子：\nval numberA = 1 val numberB = 2 val sum = sum(numberA, numberB) ... def sum(Int numberA, Int numberB): Int = { return numberA + numberB } 我们可以写成\nval numberA = 1 val numberB = 2 val sum = numberA.add(numberB) ... implicit class MyInt(numberA: Int) { def add(numberB: Int) = numberA + numberB } 调用的时候，可读性的巨大差别一目了然：\nsum(A, sum(B, sum(C,sum(D,...)))) // VS A.add(B).add(C).add(D)... // scala 可以忽略点号，可以写成 A add B add C add C 下面是利用隐式转换重新组织的代码：\npackage org.nextchen.demo.extensions import org.apache.spark.sql._ import org.apache.spark.sql.functions._ import org.nextchen.demo.base.SparkJob.spark.implicits._ object GroceryDataFrameExtensions { implicit class RichDataFrame(df: DataFrame) { def sumByNormalizedName: DataFrame = { val sumOfFruits = df .groupBy(\u0026quot;normalized_name\u0026quot;) .agg( sum(($\u0026quot;quantity\u0026quot;)).as(\u0026quot;sum\u0026quot;) ) sumOfFruits } def addNormalizedNameColumn: DataFrame = { val normalizedFruits = df.withColumn(\u0026quot;normalized_name\u0026quot;, lower($\u0026quot;name\u0026quot;)) normalizedFruits } def filterFruits: DataFrame = { val fruits = df.filter($\u0026quot;type\u0026quot; === \u0026quot;fruit\u0026quot;) fruits } } } 将代码逻辑移到了另一个对象中，这代码读起来就像读散文，不是吗？\npackage org.nextchen.demo import org.apache.spark.sql.DataFrame import pl.wiadrodanych.demo.NiceApp.spark import pl.wiadrodanych.demo.extensions.GroceryDataFrameExtensions._ object CoolApp { def main(args: Array[String]) = { val result = getGroceries .filterFruits .addNormalizedNameColumn .sumByNormalizedName result.show } private def getGroceries: DataFrame = { val groceries = spark.read .option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;) .option(\u0026quot;header\u0026quot;, \u0026quot;true\u0026quot;) .csv(\u0026quot;some-data.csv\u0026quot;) groceries } } 回头看一下我们的需求：\n下载杂货店数据（getGroceries） 过滤水果（filterFruits） 格式化名称（addNormalizedNameColumn） 统计每一个水果的数量（sumByNormalizedName） 看出来吧，代码即文档，^_^。\n友情提醒：隐式转换虽好，不可滥用。不是最佳实践，请慎用！请慎用！请慎用！不然对代码维护造成灾难。\n","date":"2020-12-04","permalink":"/n3xtchen/2020/12/04/readable-scala-code-in-spark/","section":"时间线","summary":"Jupyter 和 Apache Zeppelin 是一个数据处理体验比较好的地方。不幸的是，notebooks 的特点决定了他不擅长组织代码，包括去耦合（decomposition）和可读性。我们要将代码复制到 IDE 上，然后编译成 JAR，但是效果不是很好。接下来，我们将会讲如何在 IDE 中编写可读性更高的代码。","title":"提高 Scala 代码的可读性（For Spark）"},{"content":"","date":"2020-09-29","permalink":"/n3xtchen/tags/k8s/","section":"标签","summary":"","title":"k8s"},{"content":"","date":"2020-09-29","permalink":"/n3xtchen/categories/k8s/","section":"分类页","summary":"","title":"k8s"},{"content":"K8S + ContainerD = 大坑，谁折腾谁知道（公有云的好处就体现出来）！各种链接不上，各种下载慢，谁经历谁崩溃！ 怀抱的极客精神（其实是犯贱），明知山有虎（坑），偏向虎山行（坑上走）。\n吐槽完毕，进入正题。。。\nContainerD 是什么？ # 容器运行时（Container Runtime）是 Kubernetes（k8s） 最重要的组件之一，负责管理镜像和容器的生命周期。 Kubelet 通过 Container Runtime Interface (CRI) 与容器运行时交互，以管理镜像和容器。\n他的优点（取代 docker 的主要原因）：调用链更短，组件更少，更稳定，占用节点资源更少。\n性能是优点，但是功能确实鸡肋，Biggest Problem 就是不支持外部源，在中国大陆上，意味着什么？你知道的。\nMicroK8s 是什么？ # MicroK8s是一个轻量级的Kubernetes环境。与Minikube不同，它不需要VirtualBox，因此可以在虚拟服务器上运行。 它是一个轻巧的单节点，并具有Istio，Knative 和 Kubeflow 等全面功能，非常适合学习Kubernetes。\n为什么不使用其他方案呢？因为他可以生产部署和应用。\nmulitpass 是什么？ # Multipass是一个开源命令行实用程序，允许用户协调Ubuntu Linux虚拟机的创建，管理和维护，以简化应用程序的开发。 它可以在Linux和macOS操作系统上使用，并且截至今天，它也可用于Windows平台。\n这个是官方的定义，简单的说，就把它当作 virtualbox/docker。\n现在正式进入教程。\n一、环境准备 # brew cask install multipass brew install ubuntu/microk8s/microk8s 开始安装 MicroK8s：\nmicrok8s install -c 6 -m 10g 如果成功了，在这里故事就结束了，^_^\n简单的东西永远都不容易。Ubuntu 的镜像没有配置，可能会导致安装过程报错，或者组件缺失。\n二、手动安装 MicroK8s # 其实原理很简单，microk8s install 内部脚本分三步；\n1. 安装容器 # 下面谈谈比较稳健的方式：\nmultipass launch -c 6 -m 10g -d 50g -n microk8s-vm bionic 参数都代表\u0026quot;\n-c 6：cpu 数量 -m 10g：内存数量 -d 50g:：最大磁盘大小 -n 容器名称：这里必须使用 microk8s-vm，不然 Mac Os 端就没办法使用 microk8s 的命令了 bionic：这个是发行版别名，我是用的 ubuntu 18.04(和 microk8s 移植)，可以通过 multipass find 查找其他可用的发行版 安装完成后，通过如下命令验证：\n$ multipass list Name State IPv4 Image ...(忽略其他镜像) microk8s-vm Running 192.168.64.2 Ubuntu 18.04 LTS 10.1.36.0 10.1.36.1 提示：安装过程可能会很慢，因为 multipass 没有中国源，Linux 支持导入，但是 MacOs 和 Windows 目前都不支持文件导入的方式，寻找其他替代方案\n2. Ubuntu 源配置（默认的源速度慢，不稳定） # 这个步骤就不解释，直接上代码。\n目前，我使用的 阿里云，直接执行如下代码\n进入容器:\nmultipass shell microk8s-vm 配置源:\nsed -i s/archive.ubuntu.com/mirrors.aliyun.com/g /etc/apt/sources.list sed -i s/security.ubuntu.com/mirrors.aliyun.com/g /etc/apt/sources.list apt-get update apt-get upgrade -y 3. 手动安装 microk8s # sudo snap install microk8s --classic 默认安装最新稳定版，现在是 v1.19.2（和 k8s 版本对应）\n你可以指定自己安装：\nsudo snap install microk8s --classic —channel=1.18/stable 现在查看下安装成功与否（还是在容器内操作）：\nubuntu@microk8s-vm:~$ snap list Name Version Rev Tracking Publisher Notes microk8s v1.18.15 2034 1.18/stable canonical✓ classic 三、配置 MicroK8s # 配置 docker.io 镜像 # 文件：/var/snap/microk8s/current/args/containerd-template.toml\n配置所在位置：\n1.19.2 之前（不包括）： [Plugin] [plugins.cri.registry] [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.\u0026ldquo;docker.io\u0026rdquo;] 1.19.2 之后： [plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026rdquo;] [plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026rdquo;.registry] [plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026rdquo;.registry.mirrors] [plugins.\u0026ldquo;io.containerd.grpc.v1.cri\u0026rdquo;.registry.mirrors.\u0026ldquo;docker.io\u0026rdquo;] 我添加的镜像地址如下：\nendpoint = [ \u0026quot;https://3laho3y3.mirror.aliyuncs.com\u0026quot;, \u0026quot;http://f1361db2.m.daocloud.io\u0026quot;, \u0026quot;https://mirror.ccs.tencentyun.com\u0026quot;, \u0026quot;https://hub-mirror.c.163.com\u0026quot;, \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;, \u0026quot;https://registry-1.docker.io\u0026quot; ] 添加完，需要重启 microk8s。\n以为现在就结束，那就天真了，后面才是真正的干货，^_^\n四、依赖包问题: failed to resolve reference \u0026ldquo;k8s.gcr.io/pause:3.1\u0026rdquo; # 所有在自建 K8S 的时候，大部分人都在这个部分开始放弃了，这里要感谢 GFW。\n国内，无法使用 google 官方的镜像，所以安装的时候肯定会报错。\n会发现总是 Pending：\n$ microk8s kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-847c8c99d-svsvm 0/1 Pending 0 72m kube-system calico-node-vvpvw 0/1 Init:0/3 0 72m 继续挖下去，发现已经报错了：\n$ microk8s kubectl describe pod calico-node-vvpvw --namespace=kube-system …. Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 25m (x19 over 62m) kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to get sandbox image \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to pull image \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to pull and unpack image \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to resolve reference \u0026quot;k8s.gcr.io/pause:3.1\u0026quot;: failed to do request: Head \u0026quot;https://k8s.gcr.io/v2/pause/manifests/3.1\u0026quot;: dial tcp 64.233.189.82:443: i/o timeout 目前，containerD 的功能体验和 docker 还是有些差距，这个时候，需要借助 docker（找一台装有 docker 的机子）\n1. 找一个国内靠谱且速度快的源 # 下面是我的选择（docker 源怎么修改，自行谷歌）\ngcr.azk8s.cn/google-containers registry.cn-hangzhou.aliyuncs.com \u0026lt;这个是我的选择\u0026gt; 2. 拉去镜像 # docker pull registry.cn-hangzhou.aliyuncs.com/pause:3.1 3. 修改镜像域名，因为 k8s 只认 k8s.gcr.io # docker tag registry.cn-hangzhou.aliyuncs.com/pause:3.1 k8s.gcr.io/pause:3.1 4. 导出镜像 # docker save k8s.gcr.io/pause \u0026gt; pause.tar 5. 传输你的 microK8s 所在服务器 # multipass transfer pause.tar \u0026quot;microk8s-vm:pause.tar\u0026quot; 6. 导入镜像 # microk8s.ctr —namespace k8s.io image import pause.tar 提示：这个方法同样适用于 k8s 的安装，实现曲线救国，\n重复上述的步骤，将下面的包一一下载：\n$ ls -l k8s.io/ total 368852 -rw-rw-r-- 1 ubuntu ubuntu 75337216 Sep 29 09:01 heapster-amd64.tar -rw-rw-r-- 1 ubuntu ubuntu 154733056 Sep 29 09:01 heapster-grafana-amd64.tar -rw-rw-r-- 1 ubuntu ubuntu 12775936 Sep 29 09:01 heapster-influxdb-amd64.tar -rw-rw-r-- 1 ubuntu ubuntu 41241088 Sep 29 09:01 k8s-dns-dnsmasq-nanny-amd64.tar -rw-rw-r-- 1 ubuntu ubuntu 50545152 Sep 29 09:01 k8s-dns-kube-dns-amd64.tar -rw-rw-r-- 1 ubuntu ubuntu 42302976 Sep 29 09:01 k8s-dns-sidecar-amd64.tar -rw-rw-r-- 1 ubuntu ubuntu 754176 Sep 29 09:02 pause.tar 因为这些组件依赖这些包:\nkube-system: Dns coredns-86f78bb79c-rm629 k8s.gcr.io/pause:3.1 kube-system: Dashboard metrics-server k8s.gcr.io/metrics-server-amd64:v0.3.6 heapster-v1.5.2 k8s.gcr.io/heapster-amd64:v1.5.2 dashboard-metrics-scraper kubernetes-dashboard k8s.gcr.io/pause:3.1 monitoring-influxdb-grafana k8s.gcr.io/heapster-grafana-amd64:v4.4.3 k8s.gcr.io/heapster-influxdb-amd64:v1.3.3 最后验证 MicroK8s 安装成功与否 # 执行如下命令:\nubuntu@microk8s-vm:~$ microk8s status microk8s is Running\u0026lt;这个才是代表安装成功\u0026gt; addons: dashboard: enabled dns: enabled cilium: disabled fluentd: disabled gpu: disabled helm: disabled helm3: disabled ingress: disabled istio: disabled jaeger: disabled knative: disabled kubeflow: disabled linkerd: disabled metallb: disabled metrics-server: disabled prometheus: disabled rbac: disabled registry: disabled storage: disabled Dashboard 端口转发 # 因为 Dashboard 默认只允许在内网访问\nmicrok8s.kubectl port-forward -n kube-system service/kubernetes-dashboard 10443:443 --address 0.0.0.0 ","date":"2020-09-29","permalink":"/n3xtchen/2020/09/29/microk8s-starter/","section":"时间线","summary":"K8S + ContainerD = 大坑，谁折腾谁知道（公有云的好处就体现出来）！各种链接不上，各种下载慢，谁经历谁崩溃！ 怀抱的极客精神（其实是犯贱），明知山有虎（坑），偏向虎山行（坑上走）。","title":"Kubernetes/MicroK8s 安装(For ContainerD and OsX)"},{"content":"","date":"2020-09-05","permalink":"/n3xtchen/categories/spark/","section":"分类页","summary":"","title":"Spark"},{"content":"状态（State）在流计算是一个宽泛概念的词汇；继续之前，我们先明确下个定义。状态（State）字面意思就是“中间信息（Intermediate Information）”。\n从数据角度看，流计算主要有两种处理方法：\n无状态（Stateless）：每一个进入的记录独立于其他记录。不同记录间没有任何关系，他们可以独立处理和持久化。例如：map、fliter、静态数据 join 等等。 有状态（Stateful）：处理进入的记录依赖于之前记录处理的结果。因此，我们需要维护不同数据处理之间的中间信息。每一个进入的记录都可以读取和更新这个信息。我们把这个中间信息称作状态（State）。例如，独立键的计数聚合，去重等等。 状态处理也分为两种：\n过程状态：它是流计算的元数据（metadata）；追踪历史至今被处理的数据。在流的世界中，我们称之为 Checkpoint 或者保存数据的偏移（offset）。为了防止重启，升级或者任务失败，它需要容错性（fault tolerance）。这个信息是任何高可靠流处理的基本，同时被无状态和状态处理需要。 数据状态：这些中间数据源自数据（目前为止处理过的），它需要在记录之间维护。这个只在 Stateful 模式下，需要处理。 状态储存方式的选择 # 为了维护流处理中的状态，我们需要选择一种存储器方式，当然有很多可选的是方式，归纳下常用的几种：\n内存，如 HashMap 文件系统，如 HDFS 分布式数据库，如 *Cassandra 嵌入式存储，最流行的单属 Facebook 的 RocksDb 方式可以多种多样，但是稳定可靠是关键，效率同样也是考量的标准之一，整个流计算的发展就是围绕着稳定和高效不断迭代进化的。\n接下来，我们要进入正题了，介绍下 Spark 中几种状态的处理机制和存储方式。\n老派：DStream/Spark Streaming # 在 Spark Streaming 或者 Structured Streaming 的 DStream 中，每一个微批处理状态都会和 Checkpoint* 元数据中一起维护；当每个批处理任务结束的时候，同步（synchronous）完成状态的维护（即使该微批处理没有任何状态操作），同时加大了任务的延迟。\n状态没有进行增量持久化，每一次都是全量快照，导致不必要的开销（主要是序列化和持久化（I/O））。\n当数据很大的时候，这个设计带来问题将会更加凸显。\n我们生活在一个不断进化的世界中。因为旧的东西不够好，导致有新的东西不断涌现，取而代之。\n新潮：Structured Streaming # Structured Streaming （Spark 的第二代基于 SQL 的流计算）的出现，除了带来更多的新特性，同时解决了上代遗留下来的坑，状态管理就是其中之一。\n状态管理从 Checkpoint* 中解藕出来，不再是 tasks/jobs 的一部分；是异步（asynchronous）的，同时支持增量持久化。\n现在，让我们深入了解下 Spark 2.3 的状态管理机制。\nStructured Streaming 有且只有提供了一种默认的状态存储：基于 HDFS 的状态管理（其实，Databrick（商业）、Quole（开源）已经提供了基于 RocksDB 的实现，大家可以了解下）。\n每一个聚合的 RDD 在各自执行器（Executor）的内存中维护一个版本化键值存储结构（内存中的 HashMap）。这个存储是唯一的：checkpointPath + operatorId + partitionId\ncheckpointPath：流查询的 Checkpoint* 路径 operatorId：流查询中的每一个聚合操作（如 groupBy）内部会被分配一个整型值 partitionId：聚合操作之后会生成聚合 RDD 分区 ID 版本的值就是 batchId；\n第一个之外的每一个微批（micro-batch）的分区都会先从前一个微批处理器拷贝 HashMap （同一分区的最后一个微批（micro-batch）），并更新它。微批处理结束后会更新后的 HashMap 将会传给下一个微批（micro-batch），就这样不断重复的执行下去知道处理结束；\n同样，某个微批（micro-batch）的某个分区，会有一个文件以容错（fault-tolerant）的方式记录该微批处理的变更。这个文件称之为 版本化增量文件。它只包含相关分区的特定批次的状态变更。因此每个批次的分区数和 增量文件 数是相等的；它对应的路径：checkpointPath/state/operatorId/partitionId/${版本}.delta\n分区任务计划在执行器（Executor） 上执行，在该执行程序中存在与以前的 microBatch 相同的分区的 HashMap。这个是由 Driver 决定的，在执行器（Executor）上保存有关于状态存储的足量数据；\n在微批处理的任务中，键的变更异步执行的，并且具有事务，同时会输出版本化的增量文件；\n关于状态管理的其他操作（如快照，清楚、删除，文件的管理等等）在 执行器（Executor） 的隔离守护线程（称之为 MaintenanceTask）中异步完成的。一个 执行器（Executor） 一个线程；\n如果任务成功了，输出流将会关闭，版本增量文件将会提交并持久化到文件系统（如HDFS）中。内存中版本化的 HashMap 会被加到提交过 HashMap 列表中，该分区的版本号会加1。新的版本ID将会在该分区的下一个批次中使用；\n如果分区任务失败了，相关内存中 HashMap 会被抛弃，增量文件输出流会被切削。这样，不会有任何的变化会在内存或者文件中被记录。整个任务将会重试；\n就像之前说的，每一个 执行器（Executor） 都有一个独立线程（MaintanenceTask），他会在等间隔时长（默认 60 秒）执行，为每个分区完成的状态进行异步地快照，将最新的版本 HashMap 持久化到磁盘中（文件名：version.snapshot，路径:checkpointLocation/state/operatorId/partitionId/${version}.snapshot）。一次没几个批次，就有一个分区的快照文件被这个线程创建，代表该版本的完整状态。这个线程会删掉比这个版本旧的增量和快照文件；\n注意：相同的执行器（Executor）不会有多线程来把状态写到增量文件中。但是在特定场景（如果推测执行）下可以有多个执行器（Executor）同时将同一个状态载入到内存中。这个意味着只能有一个线程写内存中的 HashMap，但是可以有不同 执行器（Executor） 的多个线程写到同一个增量文件中。\n当前实现的优点和缺点 # 如大家所知，软件开发没有银弹。每一种设计都有他们的优缺点。\n优点：\n有更好拓展性的抽象和接口。可以实现自己想要的任何存储方式，如现在流行的 RocksDB 取代内存的方式已经在付费版的 Spark 中得到实现； 不像早期的 DStream，高效，没有和执行器（Executor）任务强绑定； 增量状态的 Checkpoint； 缺点：\n默认的状态存储方式占用了执行器（Executor） 内存。执行器（Executor）任务的内存没有和状态存储隔离。当运行任务、状态数据成倍增长，超过执行器（Executor）的可用内存时，将会导致垃圾回收（GC），甚至内存溢出； 每一个执行器（Executor）只有单线程负责快照和数据清理。对于状态数据量过大或者单执行器（Executor）分区数过多的时候，这个线程将会不堪重负，很有可能会导致延迟。 和其他流系统对比 # 如果不和其他的流计算框架对比，这篇文章显得不完整。像 Flink 和 Kafka Streams 这样的开源流计算框架使用 RocksDB 来放开内存限制。RocksDB 解决了内存问题，但是在节点失败的时候，没有容错性。\nKafka Streams 使用 RocksDB 作为无限制的本地存储。对于容错性，Kafka Streams 依赖于 Kafka。他们为每一个更新写变更日志到内部的 Kafka 主题中，它会进行压缩，最终变成单个快照日志文件。防止失败和重启，RocksDB 可以从这个 Kafka 主题中恢复数据。\nFlink 则使用另一种方式，为了容错，独立实现了快照策略。 Flink 会定时快照 RocksDB 的数据，并拷贝到可靠的文件系统中（如 HDFS）。RocksDB 会在失败的时候从最新的快照还原。最后一次快照和失败之间将会存在一些数据将不会固化到快照中。为了还原，Flink 将从快照的时间点开始处理数据，保证未考虑的数据会被重新处理。记住，只有像 Kafka 和 Kinesis 这类可回放的数据源才能实现这些。\nStorm/Triden 依赖外部的存储（如 Cassandra/Redis）作为状态管理，来解决可依赖额容错，但是规模化后会影响性能。外部存储会大量的网络调用，将会对流处理增加延迟。这就是为什么大部分流系统使用嵌入式本地存储。\n结语 # 对比旧的 DStream 实现，Structured Streaming 当前状态管理的实现已经有很大的进步。他解决的早期的问题，是一个很成熟的设计。为了和其他流系统一较高下，就需要一个稳定的状态存储的实现。\n引用自： State Management in Spark Structured Streaming\n","date":"2020-09-05","permalink":"/n3xtchen/2020/09/05/spark-sss-state-management/","section":"时间线","summary":"状态（State）在流计算是一个宽泛概念的词汇；继续之前，我们先明确下个定义。状态（State）字面意思就是“中间信息（Intermediate Information）”。","title":"Spark 流计算状态管理进化史"},{"content":"","date":"2020-09-04","permalink":"/n3xtchen/tags/arduino/","section":"标签","summary":"","title":"arduino"},{"content":"","date":"2020-09-04","permalink":"/n3xtchen/categories/rust/","section":"分类页","summary":"","title":"Rust"},{"content":"土壤湿度监测代码如下：\n#![no_std] #![no_main] extern crate panic_halt; use arduino_uno::adc; use ssd1306::{mode::TerminalMode, Builder, I2CDIBuilder}; use arduino_uno::prelude::*; // This example opens a serial connection to the host computer. On most POSIX operating systems (like GNU/Linux or // OSX), you can interface with the program by running (assuming the device appears as ttyACM0) // // $ sudo screen /dev/ttyACM0 9600 #[arduino_uno::entry] fn main() -\u0026gt; ! { let dp = arduino_uno::Peripherals::take().unwrap(); let mut pins = arduino_uno::Pins::new(dp.PORTB, dp.PORTC, dp.PORTD); let mut serial = arduino_uno::Serial::new(dp.USART0, pins.d0, pins.d1.into_output(\u0026amp;mut pins.ddr), 9600); let mut adc = adc::Adc::new(dp.ADC, Default::default()); let mut a0 = pins.a0.into_analog_input(\u0026amp;mut adc); ufmt::uwriteln!(\u0026amp;mut serial, \u0026quot;Hello World!\\r\u0026quot;).void_unwrap(); let interface = I2CDIBuilder::new().init(i2c); let mut disp: TerminalMode\u0026lt;_\u0026gt; = Builder::new().connect(interface).into(); disp.init().unwrap(); let _ = disp.clear(); /* Endless loop */ loop { let value: u16 = nb::block!(adc.read(\u0026amp;mut a0)).void_unwrap(); let _ = disp.write_str(value.to_string()); } } ","date":"2020-09-04","permalink":"/n3xtchen/2020/09/04/rust-arduino---soil/","section":"时间线","summary":"土壤湿度监测代码如下：","title":"让 Rust 嵌入你的生活: 你的植物渴了吗？(Arduino)"},{"content":"","date":"2020-08-22","permalink":"/n3xtchen/tags/iot/","section":"标签","summary":"","title":"IOT"},{"content":"已经有一段时间没碰板子了，怎么想写一篇关于 Arduino 的文章呢？这要追溯到一个月前， rust-avr 分支合并到 upstream 了，可以通过 nightly 版来直接使用它来开发 Arduino 程序；一直以来都是用 C 来写，Rust 的诱惑力，你懂得！你现在只要完成以下两步就可以为 AVR 微控面板编译 Rust 程序了：\n在 .cargo/config.toml 将平台（Target）指定为 avr-unknonw-unknonw,unknonw 执行 cargo +nightly build 。 是不是很惊喜？^_^\n本人喜欢硬件，也对 Rust 感兴趣，两者一起弄，也是蛮不错的体验。所以后续将会有更多关于 Arduino/ESP8266 的 Rust 的文章。\n在动手之前，我们先了解一些背景，我们文章针对使用 Rust 进行嵌入式开发的新手。如果可以看完全文，建议也可以阅读下 《embedded rust book》的基础章节。\n下面是我的软硬件环境：\nmacOS Catalina(MBPR 16，在 Ubuntu 和 Arch 下也可以正常编译)\nrustc 1.48.0-nightly (d006f5734 2020-08-28)：如果还没有安装，可以使用下面的命令进行安装：\n$ brew install rustup $ rustup-init $ rustup install nightly $ rustup component add rust-src $ rustc --version --verbose rustc 1.48.0-nightly (d006f5734 2020-08-28) binary: rustc commit-hash: d006f5734f49625c34d6fc33bf6b9967243abca8 commit-date: 2020-08-28 host: x86_64-apple-darwin release: 1.48.0-nightly LLVM version: 11.0 Arduino Uno: 在业余爱好者群体中，最为流行的嵌入式解决方案；它是基于 ATmega328P 架构的 AVR 单片机\nAVR 单片机简史: 1997 年由 ATMEL 公司研发出的增强型内置 Flash 的 RISC(Reduced Instruction Set Computer) 精简指令集高速 8 位单片机。随着 2016 年，Atmel 被 Microchip 收购， AVR 随即成为 Microchip 的主力 8 位单片机产品之一。\n接下来，我们将借用 Arduino 界的 Hello Word（即控制 LED 灯闪烁）程序来讲解整个开发过程。 虽然很简单，但是对于新手的话，还有有很多要关注的点。\n设置你的项目 # 首先创建一个 Rust 项目：\n$ cargo new rust-arduino-blink 我们需要针对 AVR 平台交叉编译我们的项目（平台标识：avr-unknonw-unknonw）。为了完成这个操作，我们需要使用 nightly 版的编译工具链，因为我们的项目需要依赖一些不稳定的特性：\n$ rustup override set nightly 上述命令只会对执行这个命令的目录内有效。\n然后，我们需要安装一些包：\navr-gcc：链接接器 avrdude：用来上传固件到芯片 在 macOS 下，你需要先执行：\n$ brew tap osx-cross/avr 然后执行安装命令安装指定的依赖：\n$ brew install avr-gcc avrdude 接下来，在 cargo.toml 中添加依赖包：\n[dependencies] # A panic handler is needed. This is a crate with the most basic one. panic-halt = \u0026quot;0.2.0\u0026quot; [dependencies.arduino-uno] git = \u0026quot;https://github.com/Rahix/avr-hal\u0026quot; [profile.dev] panic = \u0026quot;abort\u0026quot; lto = true opt-level = \u0026quot;s\u0026quot; [profile.release] panic = \u0026quot;abort\u0026quot; codegen-units = 1 debug = true lto = true opt-level = \u0026quot;s\u0026quot; avr-hal 是一个 Rust 包组合（Cargo Workspace），包含各种各样芯片的驱动包（Crate），arduino-uno 就是其中之一。感谢 Rahix 把它们整合在一起。\n我们需要为 AVR 平台添加编译元数据。我们将在项目根目录下创建一个文件 arv-atmega328.json，包含的内容如下：\n{ \u0026quot;llvm-target\u0026quot;: \u0026quot;avr-unknown-unknown\u0026quot;, \u0026quot;cpu\u0026quot;: \u0026quot;atmega328p\u0026quot;, \u0026quot;target-endian\u0026quot;: \u0026quot;little\u0026quot;, \u0026quot;target-pointer-width\u0026quot;: \u0026quot;16\u0026quot;, \u0026quot;target-c-int-width\u0026quot;: \u0026quot;16\u0026quot;, \u0026quot;os\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;target-env\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;target-vendor\u0026quot;: \u0026quot;unknown\u0026quot;, \u0026quot;arch\u0026quot;: \u0026quot;avr\u0026quot;, \u0026quot;data-layout\u0026quot;: \u0026quot;e-P1-p:16:8-i8:8-i16:8-i32:8-i64:8-f32:8-f64:8-n8-a:8\u0026quot;, \u0026quot;executables\u0026quot;: true, \u0026quot;linker\u0026quot;: \u0026quot;avr-gcc\u0026quot;, \u0026quot;linker-flavor\u0026quot;: \u0026quot;gcc\u0026quot;, \u0026quot;pre-link-args\u0026quot;: { \u0026quot;gcc\u0026quot;: [\u0026quot;-Os\u0026quot;, \u0026quot;-mmcu=atmega328p\u0026quot;] }, \u0026quot;exe-suffix\u0026quot;: \u0026quot;.elf\u0026quot;, \u0026quot;post-link-args\u0026quot;: { \u0026quot;gcc\u0026quot;: [\u0026quot;-Wl,--gc-sections\u0026quot;] }, \u0026quot;singlethread\u0026quot;: false, \u0026quot;no-builtins\u0026quot;: false, \u0026quot;no-default-libraries\u0026quot;: false, \u0026quot;eh-frame-header\u0026quot;: false } 并且在 .cargo/config.toml 引用它：\n[build] target = \u0026quot;avr-atmega328p.json\u0026quot; [unstable] build-std = [\u0026quot;core\u0026quot;] 这样子，我们的构建配置就算完成了。\n让我们开始写一些代码 # 现在我们把依赖放一边，让我们加一点代码到 main.rs 中，后续将逐步完善它：\n// main.rs #![no_std] #![no_main] 首先，我们需要制定一些全局属性让编译器知道我们在不一样的环境里面。我们使用的是嵌入式环境，很多函数在标准库中是没有的，比如堆内存分配接口，线程以及网络接口等等。因此，我们需要在头部添加 #![no_std] 属性。 我们还需要使用 #![no_main] 重载默认入口（fn main()），因为我们将提供和定义自己的入口程序。我们使用 arduino_uno 包中提供的宏，来定义访问入口。通常，支持芯片的包都会为你提供入口宏。\n然后，我们使用 use 在域内引入需要的依赖：\nextern crate panic_halt; use arduino_uno::prelude::*; use arduino_uno::hal::port::portb::PB5; use arduino_uno::hal::port::mode::Output; 注意到 panic_halt 包了吗？这种情况，恐慌（Panic）会导致程序或当前线程通过进入无限循环而暂停.\n当出现 panic 时，程序默认会开始 展开（unwinding），这意味着 Rust 会回溯栈并清理它遇到的每一个函数的数据，不过这个回溯并清理的过程有很多工作。另一种选择是直接终止（abort），这会不清理数据就退出程序。\n从应对用户交互到对安全问题（导致的崩溃），嵌入式并没有一种通用的方法来解决所有 panic 行为，但是还是抽象出几种常用行为，并封装到 #[panic_handler] 函数的包中. 其中包含：\npanic-abort：紧急情况会导致执行中止指令. panic-halt：恐慌会导致程序或当前线程通过进入无限循环而暂停. panic-itm：恐慌消息是使用ITM（ARM Cortex-M特定的外围设备）记录的. panic-semihosting：紧急消息使用半主机技术记录到主机. 让我们继续：\n#[arduino_uno::entry] fn main() -\u0026gt; ! { } 我们给我们的 main 函数加上了 entry 的注解，表示这个函数作为程序的入口。! 在 Rust 中称作 Never 类型，意味程序不会返回任何东西，类似 C 语言的 void。\n为了实现 LED 灯闪烁，我们需要加入几行代码，控制相关引脚（pin）端口电压。现在，我们看一下 ATmega328P 芯片的引脚图：\n在上图中，你可以看到芯片中各种各样引脚（pin）。大部分微控制器允许设备对引脚进行读写。他们中一些被分类为 I/O 端口。一个端口代表标准接口的一组引脚（pin）。这些端口受相应端口寄存器控制，也可以认为这是一组可以被代码变更的字节变量。\n在 ATmega328P 的例子中，我们有三组寄存器：\nC - 模拟引脚（pin） 0 到 5 D - 数字引脚（pin） 0 到 7 B - 数字引脚（pin） 8 到 13 具体说明见： port Manipulation\n如果你现在手上有 uno，你可以看到数字引脚（pin） 13 和内置的 LED 是相连的。我们需要在我们的代码中访问这个引脚（pin）来操控 LED。例如，把它设置成 high 或者 low。\n现在，让我们加一些代码：\n#[arduino_uno::entry] fn main() -\u0026gt; ! { let peripherals = arduino_uno::Peripherals::take().unwrap(); let mut pins = arduino_uno::Pins::new( peripherals.PORTB, peripherals.PORTC, peripherals.PORTD, ); let mut led = pins.d13.into_output(\u0026amp;mut pins.ddr); loop { stutter_blink(\u0026amp;mut led, 25); } } 上述代码的做了很多事情。\n首先，我们创建了一个叫 Peripherals 的实例，它是 uno 外围设备列表。Peripherals 是连接你的芯片和外部设备、传感器等等的桥梁，比如计时器、计数器、串口等等。嵌入式处理器和外围设备是通过一系列控制器和状态寄存器进行沟通的。\n我们通过传递外围设备实例提供的端口，创建了一个新的 Pin 实例。然后定义一个 led 变量保存 LED 连接的引脚号。传递 ddr 寄存器给 d13 的 into_ouput 方法来配置引脚（pin） 13。\nDDR 集群器可以将端口的引脚（pin）指定为输入或者输出。DDR 是一个 8 位寄存器，一位代表 I/O 端口的一个引脚（pin）。举个例子，DDRB 的第一位（bit 0）将决定 PB0 是一个输入或者输出，最后一位（bit 7）将决定 PB7 是一个输入或者输出。为了深入理解 DDR 寄存器，那需要做更多的阅读，不在这里展开了。\n接下来，通过在循环（loop {}）内调用 stutter_blink 函数来决定灯闪烁的次数（这里，我们设置为 25 次）。\n这里是 stutter_blink 的定义：\nfn stutter_blink(led: \u0026amp;mut PB5\u0026lt;Output\u0026gt;, times: usize) { (0..times).map(|i| i * 10).for_each(|i| { led.toggle().void_unwrap(); arduino_uno::delay_ms(i as u16); }); } stutter_blink 函数的功能就是通过一个毫秒延迟（delay_ms）器调用来开关 led。这些在一个迭代器内完成。我通过 0..times 来指定一个范围，map 来逐步放大倍数，来产生渐变延迟的效果。我们当然可以使用 for 循环完成这些，并且可读性更好，但是这里为了演示 Rust 中更多高级接口和抽象。我们可以 0 成本在嵌入式系统使用函数式编程（FP）。据我所知，嵌入式领域只有 Rust 才有。\n这里是完整的代码：\n// main.rs #![no_std] #![no_main] extern crate panic_halt; use arduino_uno::prelude::*; use arduino_uno::hal::port::portb::PB5; use arduino_uno::hal::port::mode::Output; fn stutter_blink(led: \u0026amp;mut PB5\u0026lt;Output\u0026gt;, times: usize) { (0..times).map(|i| i * 10).for_each(|i| { led.toggle().void_unwrap(); arduino_uno::delay_ms(i as u16); }); } #[arduino_uno::entry] fn main() -\u0026gt; ! { let peripherals = arduino_uno::Peripherals::take().unwrap(); let mut pins = arduino_uno::Pins::new( peripherals.PORTB, peripherals.PORTC, peripherals.PORTD, ); let mut led = pins.d13.into_output(\u0026amp;mut pins.ddr); loop { stutter_blink(\u0026amp;mut led, 25); } } 让我尝试完整编译它：\n$ cargo build 如果一切都安好，你将会看到 target/avr-atmega328p/debug/ 目录下生成了一个 elf 文件 rust-arduino-blink.elf。 这就是我们要写入 uno 中的二进制文件。为了刷 elf 文件，我们需要使用 avrdude 工具。让我们在根目录创建一个名为 falsh.sh 的脚本文件，构建完之后将它刷到 uno 固件中：\n#! /usr/bin/zsh set -e if [ \u0026quot;$1\u0026quot; = \u0026quot;--help\u0026quot; ] || [ \u0026quot;$1\u0026quot; = \u0026quot;-h\u0026quot; ]; then echo \u0026quot;usage: $0 \u0026lt;path-to-binary.elf\u0026gt;\u0026quot; \u0026gt;\u0026amp;2 exit 1 fi if [ \u0026quot;$#\u0026quot; -lt 1 ]; then echo \u0026quot;$0: Expecting a .elf file\u0026quot; \u0026gt;\u0026amp;2 exit 1 fi cargo build avrdude -q -C/etc/avrdude.conf -patmega328p -carduino -P/dev/ttyACM0 -D \u0026quot;-Uflash:w:$1:e\u0026quot; 有了它，我们现在就可以执行了（确保你的 Uno 已经连接到的 USB了）：\n./flash.sh target/avr-atmega328p/debug/rust-arduino-blink.elf 我们的第一个运行 Rust 程序的 Arduino 程序完成了！\n引用自： How to run Rust on Arduino Uno - Our first blink\n","date":"2020-08-22","permalink":"/n3xtchen/rust/2020/08/22/rust-arduino-our-first-blink/","section":"时间线","summary":"已经有一段时间没碰板子了，怎么想写一篇关于 Arduino 的文章呢？这要追溯到一个月前， rust-avr 分支合并到 upstream 了，可以通过 nightly 版来直接使用它来开发 Arduino 程序；一直以来都是用 C 来写，Rust 的诱惑力，你懂得！你现在只要完成以下两步就可以为 AVR 微控面板编译 Rust 程序了：","title":"如何在 Arduino Uno 中运行 Rust：让你的 LED 灯闪起来"},{"content":"","date":"2020-08-17","permalink":"/n3xtchen/tags/wasm/","section":"标签","summary":"","title":"wasm"},{"content":"你现在找到的大部分 WebAssembly 教程和例子都是聚焦在浏览器之中，比如如何加速网页或者网页应用各种各样的功能。无论如何，WebAssembly 真正强大的领域但是被提及的很少：浏览器之外的领域；也是我们接下来系列关注的点。\n什么是 WebAssembly？ # 网页界的朋友总是爱给新生事物起名字，但是总是起的不好（web-gpu 是另一个例子）。WebAssembly 既不是网页（Web）也不是汇编器（Assembly），而是从 C++、C#、Rust 之类的编程语言编译出来的字节码。简单来说，你可以写一些 Rust 代码，然后把它编译成 WebAssembly 字节码，最后在 WebAssembly 虚拟机中运行该代码。\n它真正强大的原因是你不用再自己编写垃圾回收代码，而是将 Rust 或者 C++ 作为脚本语言来使用。因为它可以像 LUA/JavaScript 那样不在需要自行垃圾回收，WebAssembly 具有更可预测和更稳定的性能。\n它是相对较新的食物，所以还很粗糙，尤其在浏览器之外的场景。我的使用经验中，把这些最粗糙的点一一下来，这就是我写这些博客的原因。记录我的发现，希望能帮助到可能对这个项目感兴趣的人。\n为什么我们要在浏览器之外运行 WebAssembly # 浏览器之外，它的主要优势就是可以用不用妥协安全的前提下提供系统级别的访问。它是通过 WASI 完成的；Web Assembly System Interface（WASI 全称，大致意思就是 WebAssembly 系统接口）是一个类 C 的函数集，在安全的方式提供功能性的系统访问，如 fd_read、rand、fd_write、线程(WIP) 等等。\n这里举出一些你可能使用的场景（当然是非浏览器场景）：\n视频游戏的脚本语言； 最低负载运行代码，就像 Fastly/Cloudflare 的边缘计算（compute-at-edge）场景； 最低运行负载在 IOT 设备安全地运行易于更新的代码； 不需要 JIT 就能在你的环境中获取极快的性能； 前置要求 # 为了保证本次探险的最佳体验，我建议使用 Visual Studio Code 作为你的 IDE，并且安装如下扩展：\nrust-analyzer：自动补齐和其他很棒的功能 Code-LLDB：使用 LLDB 进行代码调试 WebAssembly by the WebAssembly foundation：允许你反编译和查看 .wasm 字节码 选择一个虚拟机 # 首先，你需要一个可以运行 WebAssembly 程序的虚拟机（VM）。这个虚拟机（VM）可以被嵌入，因此你可以把它加到你的游戏引擎，或者在你的主程序中调用它。这些提供一些选择：WASM3，Wasmtime，WARM 等等。他们的特性各不相同，比如支持 JIT，使用尽可能少的内存等等；你需要选择其中一个来匹配你的目标平台和场景。\n选择虚拟机（VM）时你仅需关注运行属性和调试异常。支持无缝衔接的调试模式的虚拟机（VM）只有 Wasmtime。所以即使你由于某种限制，不打算把它部署在你的程序，我也建议你使用它作为调试虚拟机（VM）。无论何时你想要代码调试 WASM 代码，你都可以在 Wasmtime 载入它。\n编写你的第一个 WebAssembly 程序 # 首先，我们需要创建一个 lib 项目：\ncargo new --lib wasm_examply 在 Cargo.toml 中加入如下代码：\n[lib] crate-type = [\u0026quot;cdylib\u0026quot;] 现在我们编辑 lib.rs：\n#[no_mangle] extern \u0026quot;C\u0026quot; fn sum(a: i32, b: i32) -\u0026gt; i32 { let s = a +b println!(\u0026quot;FROM WASM: Sum is: {:?}, s); s } 这个函数接受两个数字，相加，返回结果之前打印结果。WebAssembly 在模块载入之前，不需要定义默认函数，因此，你可以在主程序中通过它的签名来获取一个函数，然后执行它（这个类似 dlopen/dlsym 的工作机制）。\n我们使用 [#no_mangle] 和 pub extern \u0026quot;C\u0026quot; 把 sum 这个函数在 C 中可调用。如果你在浏览器教程中编译它，你可能会提示我们不在需要使用 wasm-bindgen。\n如何编译它呢？ # Rust 支持两种个目标平台（Target）：\nwasm32-unknown-unknown：标准的 WebAssembly 系统。你可以把它当作 WebAssembly 的 #no-std；它主要用于浏览器，它假设任何系统调用都不可用 wasm32-wasi：假设虚拟机（VM）暴露了 WASI 功能，允许标准库的不同实现可以被使用（这个实现的可用性依赖于 WASI 函数）。 你可以看一下 Rust 标准库的可用实现：https://github.com/rust-lang/rust/tree/master/library/std/src/sys。这个是你在运行 WebAssembly VM 时，Rust 程序可用的 WASI 函数：https://github.com/rust-lang/rust/tree/master/library/std/src/sys/wasi。\n现在我们编译成 wasm32-wasi：\n# 只要执行一次 $ rustup target add wasm32-wasi # Compile for the wasm32-wasi target. $ cargo build --target wasm32-wasi 但是 println! 如何工作呢？ # 你可能已经注意到我们调用 print! 时，期望程序可以工作，并且打印到终端中，但是 WebAssembly 程序如何知道怎么运行呢？\n这就是我们使用 wasm32-wasi 的原因。这个目标平台为 Rust 标准库选择目标系统存在对应版本的函数。打印到终端意味着写到一个特殊的文件标识符中。包括 wasm32-wasi 在内，大部分的虚拟机（VM）默认都允许，因此我们不需要做特殊的设置。\n如果你在 VSCode 安装了对应插件，你只需要右击选择 target/wasm32-wasi/debug/wasm_example.wasm，然后选择 Show WebAssembly，将会像下面一样，有一个新的文件被自动打开：\n(module .... (type $t15 (func (param i64 i32 i32) (result i32))) (import \u0026quot;wasi_snapshot_preview1\u0026quot; \u0026quot;fd_write\u0026quot; (func $_ZN4wasi13lib_generated22wasi_snapshot_preview18fd_write17h6ec13d25aa9fb6acE (type $t8))) (import \u0026quot;wasi_snapshot_preview1\u0026quot; \u0026quot;proc_exit\u0026quot; (func $__wasi_proc_exit (type $t0))) (import \u0026quot;wasi_snapshot_preview1\u0026quot; \u0026quot;environ_sizes_get\u0026quot; (func $__wasi_environ_sizes_get (type $t2))) (import \u0026quot;wasi_snapshot_preview1\u0026quot; \u0026quot;environ_get\u0026quot; (func $__wasi_environ_get (type $t2))) (func $_ZN4core3fmt9Arguments6new_v117hb11611244be67330E (type $t9) (param $p0 i32) (param $p1 i32) (param $p2 i32) (param $p3 i32) (param $p4 i32) (local $l5 i32) (local $l6 i32) (local $l7 i32) (local $l8 i32) (local $l9 i32) (local $l10 i32) global.get $g0 local.set $l5 ... 这是一个 wat （全称是 WebAssembly text format）文件。它有点像反编译的 x64/ARM ASM 指引，丑陋难以理解。由于 WebAssembly 的创建者不能决定文本格式，所以他们只能用丑陋的 S-表达式 来展现。\n这个导入语句告诉我们 WASM 程序需要如下函数存在于 wasi_snapshot_preview1 命名空间内才能运行：proc_exit、fd_write、environ_get、environ_sizes_get。所有的导入或到处的函数需要一个命名空间。wasi_snapshot_preview1 是 WASI 的命名空间，因此你可以把它当作这些函数的预留命名空间。println! 需要 wasi_snapshot_preview1::fd_write 来输出到标准输出。\n宿主（host）程序 # 你可以选择任何包含 WASI 的虚拟机（VM）。为了展示如何调试 WebAssembly，我将使用 Wasmtime（这是唯一一个支持调试的虚拟机（VM））。\n这个程序从路径 examples/wasm_example.wasm 载入 wasm 二进制文件。这个文件是你之前编译好的，你可以在 wasm_example/target/wasm32-wasi/debug/wasm_example.wasm。运行宿主程序的时候，请确保它的放置位置正常。\n这里是宿主程序（Rust）的完整代码，它包含初始化 Wasmtime 虚拟机（VM），载入模块，链接 WASI，装载和执行从 WASM 模块带出的 sum 函数:\nuse std::error::Error; use wasmtime::*; use wasmtime_wasi::{Wasi, WasiCtx}; fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { // Store 在某种场景下是一种全局对象，常用更多方式就是传递给大部分构造函数 let engine = Engine::new(Config::new().debug_info(true)); let store = Store::new(\u0026amp;engine); // 从文件载入模块 let module = Module::from_file(\u0026amp;engine, \u0026quot;examples/wasm_example.wasm\u0026quot;)?; // 链接 WASI 模块到我们的虚拟机。Wasmtime 我们决定 WASI 是否可见 // 因此我们需要在这里载入它， 我们的模块需要某个函数在 wasi_snapshot_preview1 命名空间里可见 // 这个操作使得 println! 可用。（它使用的是 fd_write） let wasi = Wasi::new(\u0026amp;store, WasiCtx::new(std::env::args())?); let mut imports = Vec::new(); for import in module.imports() { if import.module() == \u0026quot;wasi_snapshot_preview1\u0026quot; { if let Some(export) = wasi.get_export(import.name()) { imports.push(Extern::from(export.clone())); continue; } } panic!( \u0026quot;couldn't find import for `{}::{}`\u0026quot;, import.module(), import.name() ); } // 获取模块后，我们需要初始化它 let instance = Instance::new(\u0026amp;store, \u0026amp;module, \u0026amp;imports)?; // 通过 instance 获取 sum 签名 let main = instance.get_func(\u0026quot;sum\u0026quot;) .expect(\u0026quot;`main` was not an exported function\u0026quot;); // 将签名转化成可调用的函数 let main = main.get2::\u0026lt;i32, i32, i32\u0026gt;()?; let result = main(5, 4)?; println!(\u0026quot;From host: Answer returned to the host VM: {:?}\u0026quot;, result); Ok(()) } 这个项目需要的依赖：\n[dependencies] wasmtime = \u0026quot;0.19\u0026quot; wasmtime-wasi = \u0026quot;0.19\u0026quot; anyhow = \u0026quot;1.0.28\u0026quot; 执行它，并且看到如下输出：\n$ cargo run Compiling wasm_host v0.1.0 (wasm_host) Finished dev [unoptimized + debuginfo] target(s) in 35.38s Running `target\\debug\\wasm_host.exe` From WASM: Sum is: 9 From host: Answer returned to the host VM: 9 我们发现 wasm 模块的 println! 可以正确地答应到终端，并且返回的结果就是预期的 9.\n结语 # 在这个教程中，我们已经学会如何在浏览器之外编译 WebAssembly 程序，配置一个宿主程序，载入入和运行你的 WASM 二进制代码，执行从 WASM 导出的函数。在下一个部分，我们将接触到代码调试，优化程序大小，从主程序虚拟机（VM）暴露函数给 WASM 程序，以及两个虚拟机（VM）间的内存共享。\n引用自： Webassembly Without The Browser Part 1\n","date":"2020-08-17","permalink":"/n3xtchen/2020/08/17/webassembly-without-the-browser-part1/","section":"时间线","summary":"你现在找到的大部分 WebAssembly 教程和例子都是聚焦在浏览器之中，比如如何加速网页或者网页应用各种各样的功能。无论如何，WebAssembly 真正强大的领域但是被提及的很少：浏览器之外的领域；也是我们接下来系列关注的点。","title":"面向非浏览器领域的 WebAssembly"},{"content":"","date":"2019-02-14","permalink":"/n3xtchen/tags/conda/","section":"标签","summary":"","title":"conda"},{"content":" 先忽悠上贼船 # Conda 是一个开源包管理系统和环境管理系统，有如下当家特点：\n多 Python 版本 的虚拟环境管理：virtualenv 是 Python 下一个包，它只能创建所在的 Python 版本的环境，看看 Conda：\nconda create --name mypy python=”随心所欲的指定版本，不用管 conda 的 python 版本“ 不仅仅管理 Python 的工具包，它也能安装非 Python 的包。安装 numpy，需要先装相关的 C 库？不存在，只有 pip 才需要，Conda 只要一条命令：\nconda install -c anaconda numpy 不说其他特性，足以秒杀 pip，virtualenv/virtualenv-wrapper (不用担心，上述的命令，将会在后面详细讲解)。\n想要更 Pythonic 吗？跟我来\n我的环境： # Ubuntu 18.04.2 LTS 是不很任性，没有任何前置要求。\n1. 下载和安装 Anaconda # 打开 Anaconda 官网，如下图：\n选择你操作系统对应的安装包和 Python 版本：\n系统选择是 Linux：我的作业系统，你们选择你们使用的系统，此教程，在 macOs 测试过（如果 Windows 的用户安装遇到问题，请在评论区告知，不甚感激） 选择 Python 3.7 version: 因为 Python 2 很快就要停止维护了，2020 年 Python 2 就要停止维护，不想到时痛苦，赶紧拥抱 Python 3 吧！不是我危言耸听， ichexw@N3xt-Studio:~ $ wget https://repo.anaconda.com/archive/Anaconda3-2018.12-Linux-x86_64.sh ichexw@N3xt-Studio:~ $ sh ./Anaconda3-2018.12-Linux-x86_64.sh ichexw@N3xt-Studio:~ $ ls ~/ anaconda3 根据他的提示完成安装，默认情况会安装用户根目录下，并在 ~/.bashrc 尾部看到：\n# added by Anaconda3 2018.12 installer # \u0026gt;\u0026gt;\u0026gt; conda init \u0026gt;\u0026gt;\u0026gt; # !! Contents within this block are managed by \u0026#39;conda init\u0026#39; !! __conda_setup=\u0026#34;$(CONDA_REPORT_ERRORS=false \u0026#39;/root/anaconda3/bin/conda\u0026#39; shell.bash hook 2\u0026gt; /dev/null)\u0026#34; if [ $? -eq 0 ]; then \\eval \u0026#34;$__conda_setup\u0026#34; else if [ -f \u0026#34;/root/anaconda3/etc/profile.d/conda.sh\u0026#34; ]; then . \u0026#34;/root/anaconda3/etc/profile.d/conda.sh\u0026#34; CONDA_CHANGEPS1=false conda activate base else \\export PATH=\u0026#34;/root/anaconda3/bin:$PATH\u0026#34; fi fi 它会让 Conda 命令在你的命令行下可用。\n接下来，我们要开始了。。。\n2. 更新到最新版本 # ichexw@N3xt-Studio:~ $ conda update -n base -c defaults conda 3. 环境管理 # 如果，你使用过 virtualenv，这个就不会陌生。实际上，Conda 集成了 virtualenv/virtualenv-wrapper 的功能，你不需要另外安装。\n3.1. 查看环境： # ichexw@N3xt-Studio:~ $ conda env list # conda environments: # base * /root/anaconda3 ichexw@N3xt-Studio:~ $ python --version Python 3.7.1 这些都是自带的。\n3.2. 创建环境 # 接着，我们创建一个名为 mypy 的环境，指定 Python 版本是 3.7（不用管是3.7.x，conda会为我们自动寻找3.7.x中的最新版本）\nichexw@N3xt-Studio:~ $ conda create --name mypy python=3.7 Solving environment: done ## Package Plan ## environment location: /root/anaconda3/envs/mypy added / updated specs: - python=3.7 The following packages will be downloaded: package | build ---------------------------|----------------- pip-19.0.1 | py37_0 1.8 MB ca-certificates-2019.1.23 | 0 126 KB setuptools-40.7.3 | py37_0 610 KB python-3.7.2 | h0371630_0 36.4 MB libedit-3.1.20181209 | hc058e9b_0 188 KB ------------------------------------------------------------ Total: 39.1 MB ...此处省略无数行 Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # # $ conda activate mypy # # To deactivate an active environment, use # # $ conda deactivate 再次查看下，创建已有的环境\n(mypy) ichexw@N3xt-Studio:~ $ conda env list # conda environments: # base * /root/anaconda3 mypy /root/anaconda3/envs/mypy 看到 mypy，说明我们创建成功了\n激活创建好的环境：\n(mypy) ichexw@N3xt-Studio:~ $ conda activate mypy 3.3. 安装 Python 包 # (mypy) ichexw@N3xt-Studio:~ $ conda install -c anaconda flask -n mypy 命令解释：\n-c: 代表安装的源，这里我们使用 anacoda 提供的 flask -n: 这个可以省略，默认把包安装到当前激活的环境中，我们也可以指定特定的环境 和 pip 差不多。\n3.4. 拷贝环境 # (mypy) ichexw@N3xt-Studio:~ $ conda create -n mypy-backup --clone mypy 3.4. 导出环境 # 熟悉 pip 的用户应该很熟悉下面的操作：\npip freeze \u0026gt; requirements.txt # 依赖备份 pip install -r requirements.txt # 从备份安装 看看 conda 中怎么做！\n(mypy) ichexw@N3xt-Studio:~ $ conda env export \u0026gt; environment.yml environment.yml 内容如下：\nname: mypy # 环境名称 channels: # 依赖的源 - anaconda - defaults dependencies: # 依赖 - ca-certificates=2019.1.23=0 - certifi=2018.11.29=py37_0 - click=7.0=py37_0 - flask=1.0.2=py37_1 - itsdangerous=1.1.0=py37_0 - jinja2=2.10=py37_0 - markupsafe=1.1.0=py37h7b6447c_0 - openssl=1.1.1=h7b6447c_0 - werkzeug=0.14.1=py37_0 - libedit=3.1.20181209=hc058e9b_0 - libffi=3.2.1=hd88cf55_4 - libgcc-ng=8.2.0=hdf63c60_1 - libstdcxx-ng=8.2.0=hdf63c60_1 - ncurses=6.1=he6710b0_1 - pip=19.0.1=py37_0 - python=3.7.2=h0371630_0 - readline=7.0=h7b6447c_5 - setuptools=40.7.3=py37_0 - sqlite=3.26.0=h7b6447c_0 - tk=8.6.8=hbc83047_0 - wheel=0.32.3=py37_0 - xz=5.2.4=h14c3975_4 - zlib=1.2.11=h7b6447c_3 prefix: /root/anaconda3/envs/mypy # 还原的位置 根据自己需求修改 environment.yml\n3.5. 根据 environment.yml 创建环境 # ichexw@N3xt-Studio:~ $ conda env create -f environment.yml 3.6. 删除一个已有的环境 # ichexw@N3xt-Studio:~ $ conda remove --name mypy --all It\u0026rsquo;s Over! 很容易上手吧！\n","date":"2019-02-14","permalink":"/n3xtchen/2019/02/14/py-conda/","section":"时间线","summary":"先忽悠上贼船 # Conda 是一个开源包管理系统和环境管理系统，有如下当家特点：","title":"更 Pythonic: 从 Conda 开始"},{"content":"","date":"2019-02-12","permalink":"/n3xtchen/categories/python/","section":"分类页","summary":"","title":"Python"},{"content":"开门见山，直接上例子：\ngif 动图 有如下特点：\n散点图的部分是不变的；线是移动的 X 轴标题每一祯改变一次 DEMO 的环境 # Ubuntu 18.04.2 LTS conda 4.6.3 Python 3.7.2 创建 virtualenv # ichexw at n3xt-Studio -\u0026gt; conda create --name matplot-gif python=3.7 ichexw at n3xt-Studio -\u0026gt; conda activate matplot-gif 安装必要的依赖 # 安装 matplotlib # (matplotlib-gif) ichexw at n3xt-Studio -\u0026gt; conda install matplotlib 安装 imagemagick # (matplotlib-gif) ichexw at n3xt-Studio -\u0026gt; conda install -c conda-forge imagemagick 代码实现 # import sys import numpy as np import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation # 创建图层和布局 fig, ax = plt.subplots() fig.set_tight_layout(True) # 查看图标的尺寸。如果你保存成 gif 的时候，你需要提供 DPI print(\u0026#39;fig size: {0} DPI, size in inches {1}\u0026#39;.format( fig.get_dpi(), fig.get_size_inches())) # 绘制一个散点图（不会重绘），和初始的线 x = np.arange(0, 20, 0.1) ax.scatter(x, x + np.random.normal(0, 3.0, len(x))) line, = ax.plot(x, x - 5, \u0026#39;r-\u0026#39;, linewidth=2) def update(i): label = \u0026#39;timestep {0}\u0026#39;.format(i) print(label) # 更新线和坐标轴标签 line.set_ydata(x - 5 + i) ax.set_xlabel(label) # 返回要重绘的对象 return line, ax if __name__ == \u0026#39;__main__\u0026#39;: # FunAnimation 将会在每一帧执行一次 update # frames: 帧数 # interval: 每帧的间隔 anim = FuncAnimation(fig, update, frames=np.arange(0, 10), interval=200) if len(sys.argv) \u0026gt; 1 and sys.argv[1] == \u0026#39;save\u0026#39;: # 如果第一参数是 save，教会保存成 gif # **重点** # dpi: 保存的尺寸 # writer: 使用的渲染器，我们制定成 imagemagick anim.save(\u0026#39;line.gif\u0026#39;, dpi=80, writer=\u0026#39;imagemagick\u0026#39;) else: # 否则直接展示 plt.show() ","date":"2019-02-12","permalink":"/n3xtchen/2019/02/12/matplotlib-gif/","section":"时间线","summary":"开门见山，直接上例子：","title":"极简教程: 使用 matplotlib 绘制 GIF 动图"},{"content":"完成度：20/26\n行为/心理学 # 错误的行为 (5星) 助推 (5星) 思想快与慢 (5星) 眨眼之间 (5星) 大开眼界 (5星) 反脆弱 (5星) 逆转 (5星) 上瘾 (3星) 技术类 # python 机器学习实践：测试驱动开发 (5星) 流畅的python (5星) pyspark 实战指南 (4星) 经济类 # 原则 (5星) 黑天鹅 (5星) 区块链技术驱动金融 (4星) 定位 (4星) 其他 # 奈飞工作手册 (5星) 富甲美国 (4星) 故事维思 (4星) 搞定I (4星) 科学前沿图谱 (2星) ","date":"2019-01-01","permalink":"/n3xtchen/2019/01/01/2018-readig/","section":"时间线","summary":"完成度：20/26","title":"Reading: 2018"},{"content":"","date":"2018-12-29","permalink":"/n3xtchen/categories/postgresql/","section":"分类页","summary":"","title":"PostgreSQL"},{"content":"PostgreSQL Anonymizer: 在 PostgreSQL 中隐藏或替换个人身份信息（PII）或者商业敏感信息数据。[项目地址：https://gitlab.com/daamien/postgresql_anonymizer]\n我坚信匿名化的声明：在数据库中敏感信息的存储位置和隐藏信息的规则应该直接通过数据定义语言（DLL）中直接声明。在 GDPR（如果不知为何物，可以关掉这个页面了） 的时代，开发者应该在表定义中指定匿名化的策略，就像指定数据类型、外键和约定一样。\n这个项目的原型设计展示了在 PostgreSQL 内直接实现数据隐蔽的强大能力。目前，他是基于 COMMENT 语法声明（可能是最无用的 PostgreSQL 语法）和一个事件触发器。在不远的将来，我希望为动态数据屏蔽引入新的语法（MS Server 已经这么做了）。\n使用这个扩展屏蔽用户信息或者永久修改敏感信息。现在已经有各式各样的屏蔽技术：randomization, partial scrambling, custom rules 等等。\n这里基本的例子：\n想想一个 people 的表：\n=# SELECT * FROM people; id | name | phone ------+----------------+------------ T800 | n3xtchen | 1351111111 第一步：激活屏蔽引擎\n=# CREATE EXTENSION IF NOT EXISTS anon CASCADE; =# SELECT anon.mask_init(); 第二步：声明屏蔽的用户\n=# CREATE ROLE skynet; =# COMMENT ON ROLE skynet IS 'MASKED'; 第三步：声明屏蔽规则\n# COMMENT ON COLUMN people.name IS 'MASKED WITH FUNCTION anon.random_last_name()'; # COMMENT ON COLUMN people.phone IS 'MASKED WITH FUNCTION anon.partial(phone,2,$$******$$,2)'; 第四步：查询屏蔽敏感信息的用户\n# \\! psql test -U skynet -c 'SELECT * FROM people;' id | name | phone -----+----------+------------ T800 | n3xtchen | 13******11 这个项目还在开发中。如果有什么好的主意和建议，不要吝啬，通过 github 的 issue 反馈给作者。\n","date":"2018-12-29","permalink":"/n3xtchen/2018/12/29/pgsql-anonymizer/","section":"时间线","summary":"PostgreSQL Anonymizer: 在 PostgreSQL 中隐藏或替换个人身份信息（PII）或者商业敏感信息数据。[项目地址：https://gitlab.","title":"PostgreSQL: 匿名化（Anonymizer）工具"},{"content":"","date":"2018-12-11","permalink":"/n3xtchen/categories/osx/","section":"分类页","summary":"","title":"osX"},{"content":"在 Os X 清理程序过程，遇到 .pkg 是相当头痛的事情，因为安装的文件不在一块，所以手动删不全；对于洁癖的我，是不可容忍的。\n幸好，苹果给我留了一条后路，pkguitl，虽然简陋但是够用。\n废话少说，看用法。\n先看，我们安装了哪些应用；打开终端，输入下面命令：\nichexw -\u0026gt; pkgutil --pkgs ... com.apple.pkg.MAContent10_AssetPack_0357_EXS_BassAcousticUprightJazz com.apple.pkg.GatekeeperConfigData.16U1642 com.apple.pkg.GatekeeperConfigData.16U1118 com.apple.pkg.MAContent10_AssetPack_0320_AppleLoopsChillwave1 com.apple.pkg.ChineseWordlistUpdate.14U1359 com.apple.pkg.ChineseWordlistUpdate.14U1365 net.wisevpn.wiseVPN com.rescuetime.RescueTime com.youku.mac com.silabs.driver.CP210xVCPDriver org.virtualbox.pkg.vboxkexts com.amazon.Kindle com.xiami.client com.microsoft.package.Fonts com.tinyspeck.slackmacgap cx.c3.theunarchiver com.apple.pkg.MobileAssets com.GoPro.pkg.GoProApp com.oracle.jre ... 可以看到， com.apple 打头的是系统自己的，其他就是自己安装的。\n我想要卸载旧版本的 java 包，找出它都被安装到哪里了；输入下面命令：\nichexw -\u0026gt; pkgutil --files com.oracle.jdk7u80 Contents Contents/Home Contents/Home/COPYRIGHT Contents/Home/LICENSE Contents/Home/README.html Contents/Home/THIRDPARTYLICENSEREADME-JAVAFX.txt Contents/Home/THIRDPARTYLICENSEREADME.txt Contents/Home/bin Contents/Home/bin/appletviewer Contents/Home/bin/apt ... 寻找文件的安装根目录：\nichexw -\u0026gt; pkgutil --file-info com.oracle.jdk7u80 volume: / path: com.oracle.jdk7u80 ichexw at ichexws-MBPR in / ○ cd / 删除对应的文件：\nichexw -\u0026gt; pkgutil --only-files --files com.oracle.jdk7u80 | tr '\\n' '\\0' | xargs -n 1 -0 sudo rm -i 让系统忘记这个 pkg\nichexwe -\u0026gt; sudo pkgutil --forget the-package-name.pkg Happy Ending!\n","date":"2018-12-11","permalink":"/n3xtchen/2018/12/11/os-x-pkgutil/","section":"时间线","summary":"在 Os X 清理程序过程，遇到 .","title":"OsX: 完全卸载攻略（pkg）"},{"content":"","date":"2018-12-06","permalink":"/n3xtchen/categories/algorithm/","section":"分类页","summary":"","title":"algorithm"},{"content":"","date":"2018-12-06","permalink":"/n3xtchen/tags/nn/","section":"标签","summary":"","title":"NN"},{"content":"为了更好的理解深度学习，我决定从零开始构建一个神经网络（Neural Network）。\n什么是神经网络？ # 起源：M-P 模型（单层神经元） # 所谓M-P模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型。简单点说，它是对一个生物神经元的建模。它实际上是两位科学家的名字的合称，1943年心理学家W.McCulloch和数学家W.Pitts合作提出了这个模型，所以取了他们两个人的名字（McCulloch-Pitts）。\n先来看看神经元的简化示意图：\n生物神经元模型 {:width=\u0026ldquo;800px\u0026rdquo;}\n神经元在结构上由细胞体、树突（输入）、轴突（输出）和突触4部分组成。\n每个神经元都是一个多输入单输出（轴突）的信息处理单元； 神经元输入（树突）分兴奋性输入和抑制性输入两种类型； 神经元具有空间整合特性和阈值特性（兴奋和抑制，超过阈值为兴奋，低于是抑制）； 神经元输入与输出间有固定的时滞，主要取决于突触延搁； 我们可以把一个神经元想象成一个水桶，这个水桶侧边接着很多条水管（神经末梢），水管既可以将桶里的水输出去（抑制性），也可以将其他水桶的水输进来（兴奋性）。当桶里的水达到一个高度时，就会通过另一条管子（轴突）将水输送出去。由于水管的粗细不同，对桶里的水的影响程度（权重）也不同。水管对水桶里的水位的改变（膜电位）自然就是这些水管输水量的累加。当然，这样来理解并不是很完美，因为神经元中的信号是采用一个个脉冲串的离散形式，而这里的水则是连续的。\n关于权值的理解，还有人做出一个非常形象的比喻。比如现在我们要选一个餐厅吃饭，于是对于某一个餐厅，我们有好几种选择因素 e.g.口味、位置、装潢、价格等等，这些选择因素就是输入，而每一个因素占的比重往往不同，比如我们往往会把口味和价格放在更重要的位置，装潢和位置则稍微不那么重要。很多个候选餐厅的选择结果最终汇总之后，就可以得到最后的决策。\n按照生物神经元，我们建立M-P模型。下图，展示的就是 M-P 模型的示意图：\n{:width=\u0026ldquo;800px\u0026rdquo;}\n那么接下来就好类比理解了。我们将这个模型和生物神经元的特性列表来比较：\n{:width=\u0026ldquo;800px\u0026rdquo;}\n结合M-P模型示意图来看，对于某一个神经元j，它可能接受同时接受了许多个输入信号，用χi表示。\n由于生物神经元具有不同的突触性质和突触强度，所以对神经元的影响不同，我们用权值 $w_{ij}$ 来表示，其大小则代表了突出的不同连接强度。\n$T_j$ 表示为一个阈值，或称为偏置，超过阈值为兴奋，低于是抑制。\n由于累加性，我们对全部输入信号进行累加整合，相当于生物神经元中的膜电位（水的变化总量），其值就为：\n$$ net\\_j\u0026rsquo;(t) = \\sum_{i=1}^{n}{w_{ij}X_i(t)} - T_j $$\nMLP：多层感知机（Multilayer Perceptron， 我们常说的神经网络 ） # 简单地将神经网络描述为将给定输入映射到所需输出的数学函数更容易理解一下。\n神经网络包含下面几个组件：\n一个输入层（input layer）: x 一个任意数量的隐藏层（hidden layers） 一个输出层（output layer）: $ \\hat{y} $ 每一个层之间有一个权重（weight）集和偏置（bias）集: W 和 b 需要为每一个隐藏层选择一个激活函数（activation function）， $\\sigma$。这里，我们都是用 Sigmod 作为激活函数。 下面显示2层神经网络的架构（注意：当计算层数的时候，一般都会把输入层忽略掉）。\n一个2层神经网络的架构 {:width=\u0026ldquo;800px\u0026rdquo;}\n开始构建 # 使用 Python 创建一个神经网络的类：\nclass NeuralNetwork: def __init__(self, x, y): self.input = x self.weights1 = np.random.rand(self.input.shape[1],4) self.weights2 = np.random.rand(4,1) self.y = y self.output = np.zeros(y.shape) 神经网络的训练 # 一个 2-layer 神经网络的输出 $\\hat{y}$ 是：\n$$ \\hat{y} = \\sigma(W_2\\sigma(W_1x+b_1)+b_2) $$\n你可能注意到上一一个等式，权重 W 和偏置 b 是影响 $\\hat{y}$。\n很自然，右边值的权重和偏置决定预测的强弱。使用输入数据对权重和偏置的微调过程，就是我们所说的神经网络的训练。\n训练过程的每一次迭代包含下面的步骤：\n计算预测输出 $\\hat{y}$ ，称之为前馈（feedforward） 更新权重和偏置，称之为反向反馈（backpropagation） 下面的序列图说明了该过程：\n{:width=\u0026ldquo;800px\u0026rdquo;}\n前馈（feedforward） # 正如我们所看到的，前馈就是简单的计算，现在看看代码时间\nclass NeuralNetwork: ... def feedforward(self): \u0026#34;\u0026#34;\u0026#34;前馈\u0026#34;\u0026#34;\u0026#34; self.layer1 = sigmoid(np.dot(self.input, self.weights1)) self.output = sigmoid(np.dot(self.layer1, self.weights2)) 然而，我们还需要一种评估预测好坏的方式。损失函数（Loss Function）就是这个用途。\n损失函数（Loss Function） # 损失函数有很多种，选择应该由我们问题的性质决定。在这里，我们将 SSE（sum of sqares error，和方差）作为损失函数：\n$$ SSE = \\sum_{i=1}^n{(y - \\hat{y})^2} $$ SSE 就是预测值和实际值的差异的和。差异是平方的，所以我们可以使用差的绝对值来衡量。\n我们训练的目标实际找出最佳权重和偏置组合使得损失函数最小化。\n反向反馈（Backpropagation） # 现在，我们已经测出了预测的误差（损失），我们需要寻找一种方式将误差反馈回去，来更新我们的权重和偏置。\n为了知道调整权重和偏置的适当数量，我们需要知道损失函数相对于权重和偏置的导数（derivative）。\n回想一下微积分函数的导数就是函数的斜率。\n梯度下降算法 {:width=\u0026ldquo;800px\u0026rdquo;}\n如果我们有导数，我们就能通过加减它来更新权重和偏置。这就是所谓的梯度下降。\n然而，我们不能直接计算损失函数的导数，因为损失函数不包含权重和偏置。因此，我们需要链规则（chain rule）来帮助我们计算它\n$$ Loss(y, \\hat{y}) = \\sum_{i=1}^{n}{(y-\\hat{y})^2} $$\n对 W 进行求导：\n$$ \\begin{split} \\frac{\\partial Loss(y, \\hat{y})}{\\partial W} \u0026amp;= \\frac{\\partial Loss(y, \\hat{y}}{\\partial \\hat{y}} * \\frac{\\partial \\hat{y}}{\\partial z} * \\frac{\\partial z}{ \\partial W} \\\\ \u0026amp;= 2(y-\\hat{y}) * sigmoid 函数的导数 * x \\\\ \u0026amp;= 2(y-\\hat{y}) * z(1-z) * x \\end{split} $$\n计算损失函数对权重的导数的推导。为了简化，我们只展示了一层神经网络的导数。\nPhew！虽然很丑，但是这就是我们需要的——损失函数对权重的导数，来帮助我们调整我们的权重。\n现在，我们把反向反馈函数添加到代码中：\nclass NeuralNetwork: ... def backprop(self): \u0026#34;\u0026#34;\u0026#34;反向反馈\u0026#34;\u0026#34;\u0026#34; # 计算出各层的导数 d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output))) d_weights1 = np.dot(self.input.T, (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1))) # 使用导数来更新参数 self.weights1 += d_weights1 self.weights2 += d_weights2 整合在一起 # 现在，我们有了完整的神经网络的代码实现。让我们来应用到例子中，看看效果\nX1 X2 X3 y 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 我们的神经网络应该学习到立项的权重集来展示这个函数。请注意，仅仅通过检查来计算权重对我们来说并不是微不足道的。\n让我们迭代 1500 次，看看会是什么结果。看看每一次迭代的损失，我们可以明显的看出损失单调递减到一个最小值。这与我们之前讨论过的梯度下降算法一致。\n{:width=\u0026ldquo;800px\u0026rdquo;}\n让我们看一下 1500 次迭代后神经网络最后的预测（输出）：\n预测 Y（实际） 0.023 0 0.979 1 0.975 1 0.025 0 我们做到了！我们前馈和反向反馈算法成功的训练了一个神经网络，他的预测收敛于真正的价值观。\n注意到了预测值和实际值存在略微差别。这个是合理的，为了避免过拟合，让神经网络对未知数据有更好的泛化能力。\n接下来呢？ # 后续，我们将会深入讲解激活函数，学习率等等，如果有时间的话^_^。\n最后的思考 # 我是从0开始学习，甚至我还用不用的语言构造自己的神经网络，过程是很有趣的。\n虽然像 TensorFlow 和 Keras 这样的深度学习库，让我们不需要对神经网络内部机制有足够的了解，也能构建深度网络，但是能够更深入的了解，对后续的研究更有益。\n","date":"2018-12-06","permalink":"/n3xtchen/algorithm/2018/12/06/neural-network/","section":"时间线","summary":"为了更好的理解深度学习，我决定从零开始构建一个神经网络（Neural Network）。","title":"入门: 神经网络（Neural Network）及 Python 实现"},{"content":"当你在 Ubuntu 使用 apt-get update 的时候，我获取如下信息：\nE: Repository 'http://dl.google.com/linux/chrome/deb stable Release' changed its 'Origin' value from 'Google, Inc.' to 'Google LLC' N: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details. 分析 # 这个错误信息确定: Google Chrome 从名为 Google LLC 新实体获取的更新和你系统信任的 Google, Inc 一样。所以我们需要手动确认这个源的可信度。\n解决方法 # ichexw in ~ $ sudo apt update 注意：这里是 apt 不是 apt-get。\n然后，输入 y 接受这个变更：\n... E: Repository 'http://dl.google.com/linux/chrome/deb stable Release' changed its 'Origin' value from 'Google, Inc.' to 'Google LLC' N: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details. Do you want to accept these changes and continue updating from this repository? [y/N] y ... 然后，就不会出现上面这个问题。\n引用： Stackoverflow-E: Repository \u0026lsquo;http://dl.google.com/linux/chrome-remote-desktop/deb stable Release\u0026rsquo; changed its \u0026lsquo;Origin\u0026rsquo; value from \u0026lsquo;Google, Inc.\u0026rsquo; to \u0026lsquo;Google LLC\u0026rsquo;\n","date":"2018-11-12","permalink":"/n3xtchen/2018/11/12/apt-e-repository-change-its-origin-vale/","section":"时间线","summary":"当你在 Ubuntu 使用 apt-get update 的时候，我获取如下信息：","title":"E: Repository 'http://url stable Release' changed its 'Origin' value from 'XX' to 'YY'"},{"content":"","date":"2018-11-12","permalink":"/n3xtchen/categories/linux/","section":"分类页","summary":"","title":"Linux"},{"content":"","date":"2018-11-12","permalink":"/n3xtchen/tags/ubuntu/","section":"标签","summary":"","title":"ubuntu"},{"content":"","date":"2018-10-30","permalink":"/n3xtchen/tags/go/","section":"标签","summary":"","title":"go"},{"content":"","date":"2018-10-30","permalink":"/n3xtchen/categories/go/","section":"分类页","summary":"","title":"Go"},{"content":"作为伪用户，Go 可谓是当代最反现代的语言（是一种退化），就是有个好爹，吐槽几点：\n函数不支持默认值 有限制的范型 不支持多肽 不支持重载 不支持关键字拓展 没得用，感觉大家都在滥用 struct 和 intereface，因为只有它能用，论坛上，简直就是万能的解决方案。\nGo 程序员们，你们是带着镣铐在编程！\n解决依赖上，让我头痛不已。最开始，我是使用 dep。至于为什么用？因为大家都在用。\n老实说，还是简化了不少东西，它会自动识别代码中的依赖并提取，帮你安装。是的，作为解决外部依赖的方案，它是个合格的方案。\n如果内部库的依赖怎么办？ # 不得不在吐槽下 Go 语言，你丫的，连相对路径依赖都没有，所以代码都要放在 GOPATH 下，文件的层级结构只能写死。 也就是说你只能在 GOPATH 下开发，感觉我的 \u0026ldquo;人身自由\u0026rdquo; 都被限制了。整个人都不好了。\n我可是一个崇尚自由的码农。\n怎么办呢？\nGO module 横空出事（VGO） # Go 在 1.11 之后，就集成在 go tool 工具链中。如果仅仅是解决外部依赖上，VGO 除了不需要安装，和 DEP 没有太大得区别（其实我更喜欢 dep 这个名字，vgo 这名字太路人）。\ndep 的两个文件：\nGopkg.lock Gopkg.toml vgo 的两个文件：\ngo.mod go.sum 但是，但是 VGO 它支持导入内部库，也就是说，你的可以不必在 GOPATH 开发程序，在运行程序的时候，它帮你注入内部库。\n先来看看怎么用！\n首先，请确认的 Golang 语言的版本是 1.11 之后\n我的版本是 go1.11.2\nichexw in ~/Dev/go/learning → go version go version go1.11.2 darwin/amd64 ichexw in ~/Dev/go/learning → cho \\$GOPATH /Users/ichexw/Dev/go ichexw in ~/Dev/go/learning → echo \\$GOROT /usr/local/Cellar/go/1.11.2/libexec/src/vgo-demo/lib 看看 go mod 的命令：\nichexw in ~/Dev/go/learning → go help mod Go mod provides access to operations on modules. Note that support for modules is built into all the go commands, not just 'go mod'. For example, day-to-day adding, removing, upgrading, and downgrading of dependencies should be done using 'go get'. See 'go help modules' for an overview of module functionality. Usage: go mod \u0026lt;command\u0026gt; [arguments] The commands are: download download modules to local cache edit edit go.mod from tools or scripts graph print module requirement graph init initialize new module in current directory tidy add missing and remove unused modules vendor make vendored copy of dependencies verify verify dependencies have expected content why explain why packages or modules are needed Use \u0026quot;go help mod \u0026lt;command\u0026gt;\u0026quot; for more information about a command. Go Mod 初始化 # ichexw in ~/Dev/go/learning → mkdir vgo-demo ichexw in ~/Dev/go/learning → cd vgo-dem 一、创建自己的库： # ichexw in ~/Dev/go/learning/vgo-demo → cat lib/hello.go package lib import \u0026quot;fmt\u0026quot; func Hello() { fmt.Print(\u0026quot;Hello, Go!\u0026quot;) } 二、创建 main.go 文件 # ichexw in ~/Dev/go/learning/vgo-demo → cat main.go package main import \u0026quot;vgo-demo/lib\u0026quot; func main() { lib.Hello() } 三、执行程序 # ichexw in ~/Dev/go/learning/vgo-demo \u0026gt; go run main.go main.go:3:8: cannot find package \u0026quot;vgo-demo/lib\u0026quot; in any of: /usr/local/Cellar/go/1.11.2/libexec/src/vgo-demo/lib (from \\$GOROOT) /Users/ichexw/Dev/go/src/vgo-demo/lib (from \\$GOPATH) 报错了，我们来分析下，错误信息告诉我们：main.go 文件的第三行出现错误，无法导入 \u0026ldquo;vgo-demo/lib\u0026rdquo;；我们分析下：\n# main.go import \u0026quot;vgo-demo/lib\u0026quot; Go 编译器包查找的路径（优先级从下到上）是:\n$GOROOT/src: 标准库所在的位置 $GOPATH/src: 工作目录 但是，我的项目并不在这两者之一；常规的做法，我把我的项目移到工作目录下：\nichexw in ~/Dev/go/learning → cp -r vgo-de \\$GOPATH/src/ ichexw in ~/Dev/go/learning → cd \\$GOPATH/src/vgo-demo ichexw in ~/Dev/go/src/vgo-demo → go run main.go Hello, Go! 这不是我想要，开发程序更多是分项目，而不是语种\npath/to/dev ├── proj1 │ ├── go │ └── python | └── ... ├── proj2 │ ├── scala │ └── go | └── ... └── .. 这个时候，GOPATH 该怎么设置呢？是不是很为难，而且内部依赖只被所在的项目使用，不存在跨项目调用，我可以这么做：\n开发某个项目的时候，手动设置 GOPATH 变量，但是这个也太挫了 把 path/to/dev 设置成 GOPATH，这个我就。。。 个人认为的最佳实践：\npath/to/go/path/: 设置成 GOPATH，存取通用外部依赖的类库，以及自己开发的跨项目使用的通用类库 path/to/dev/projN/go: 某个项目，自用的类库封装，通过 VGO 自行引用 项目依赖的外部类库：vender 下 目录格式如下：\npath/to/gopath # 跨项目依赖类库/工具 path/to/projgo/ # 我的 go 项目 ├── main.go ├── locallib1 # 当前类库1 | └── ... ├── locallib2 # 当前类库2 | └── ... ├── ... # 其他文件/本地类库 └── vender # 当前项目依赖的类库 ","date":"2018-10-30","permalink":"/n3xtchen/2018/10/30/go-mod-local-pacakge/","section":"时间线","summary":"作为伪用户，Go 可谓是当代最反现代的语言（是一种退化），就是有个好爹，吐槽几点：","title":"Go module 解决本地库依赖问题(更新中。。。)"},{"content":" 译自： Moving to three-person engineering teams\n三人工程团队能工作得更高效。三人团队中的每一个人都会更加主动，因为他们有更强的归属感，他们都觉得团队是属于他们的。他们花销更小而且更合理。而且，三人团队也不会太小。他们有足够的人处理假期或者解决问题。\n但是三人团队也不想他们所期待的那么敏捷，因为他们很难脱身。他们可能很难形成足够的团队领导。他们也趋于招更多的人来完成更大的任务。在 Q 管理的团队中，当小于十人工程师（在公司创建初期）时，三人小团队是很普遍。但是我们已经扩大到 25个人，我们的团队也膨胀到 5 或 6 个人。每个人都变得不主动。接着就是，每一件事情都被拖慢。\n几个月前，我们决定把大的团队分拆成 3 人小团队。正如我们希望的，三人小团队更加主动，交付的更快更多。在我的后重组文件调查中，团队称赞迅速和更强的归属感：\n小型 = 快速的开发。能够拥有我们所处理的代码的完整心智模型。在理解上，没有人能够被排除在外或落后。\n更小的团队使得计划和连续工作变成更容易。这让我们可以在一些项目更快速的推进。\n更多的拥有权，领导的机会和灵活\n更容易让我们为工作感到自豪，因为更容易问责，每一个人都可以对团队的成功产生更直接的影响。\n但是转到三人小团队迫使我们做了一些改变。\n我们需要学会更容易信任 # 在一个老员工告诉我因为没有给予新人领导职责，从而阻碍了我们，所以我们决定转到三人小团队。其他工程师也认为只有少数受宠的人才能领导任何事情。\n这个工程师迫使我面对阻碍我的偏见。我倾向于相信重复证明自己成功的人。但是，团队中的每一个人都想要证明他们可以完成他们之前没有完成的任务。如果不能学会更多的信任，就不能让我的团队运转得更好。\n更大的组织架构也不适用于我们雇佣的员工。我们为我们招聘来的工程师感到自豪，因为他们能够很好地处理分歧，专注于解决问题，而不仅仅是简单地完成任务。\n为了变得更加信任，我们重新定义负责任的职责。我们新的领导者将只需要协调两个人的工作，而不是四个或五个。我们的领导者不需要管理别人。他们的角色也只是临时的。某些情况下，我们的领导者也更容易返回到非领导者的角色中。\n我们需要训练我们领导者 # 当我们增加几个领导岗位，我们决定在新的角色中训练他们。约翰 卢卡斯，我们的一个经理，为领导岗开发了一个培训课程。他要求新的领导者在几个方面评估自己，包括设定目标，识别问题，管理项目，管理风险，给予反馈以及创建心理防线。\n约翰的培训要求每一个领导根据这些维度编写自我评估报告。他也为每一个领导编写自己的评估；然后一对一地探讨差异。我们的新领导者在自我评估报告中都很反省和坦白。例如：\n我经常感觉反馈很伪善。我寻找和渴望我身边人给我的反馈，但是总是不可得。金姆 斯科特以毁灭性的同情来诊断我。我真心和关心和我共事的人，同时极力避免对抗和伤害他们的感情。如果我很直接和坦诚，尽管我也知道这对大家都好，但是还是这么做了。这是我最想要提升的能力。\n评估最有用地方就是他帮我我们的领导了解他们每天都要面临的新职责。当我们中很多有经验的领导者步入领导岗位，他们会很吃惊他们突然间要肩负的东西。开放性地讨论领导岗位的挑战将帮助我们领导者知道所预期的东西，以及很自然地去寻求帮助。\n管理者角色的变化 # 由于领导者肩负更多的职责，我们的管理自然就减少了。一些管理者转变到其他角色。原先的角色也会有新的职责。\n我们的管理演变成流浪者。每一个管理者都有多个三人团队向其汇报和大量的直接报告。管理者不在负责直接的项目管理。他们必须学会如何与每个三人团队的详细情况保持联系，并在需要时提供管理或技术支持。\n技术归属感不稳定 # 虽然我们很想说每一个三人团队可以完全接管一套小系统，它能完全和其他团队解偶，但是现在仍然有很多共享的系统。多个三人团队进行随时随叫的轮岗。作为亚马逊两个披萨原则的门徒，每个团队独家拥有授权和提供授权的系统，我非常不爽共享拥有权的主意。我担心团队会因此缺乏主人翁意识，不愿意投入更多来完善他们的系统。\n实际上，我们从没有更好地进行技术改进的透支。我认为主要原因是主人翁意识在心理上是很复杂看不透的。虽然共享系统会降低主人翁意识，但是三人团队的更窄和更深的授权很大提升了工程师感觉到的主人翁意识。\n我把这些归功于克里斯 盖尔和科伦 埃利奥特 - 麦克雷，他们鼓励我想一想设计团队责任和所有权的不同方法。\n工程师之间的工作更少的连接 # 虽然我们很满意三人团队的主意，他们开始汇报想要知道更多关于同事正在进行的项目。我们团队有时会发现重叠和互补的工作。我们不能保证百分百解决这个问题，但是正在尝试让每个团队每周发送一个非常简短（段落长度）的摘要来减少这种情况。\n我们为产品管理和设计创造更多的工作 # 在某种意义上，我们的三人团队并不是真的只有三个人。每一个团队也有一个相关的产品经理和设计师。他们不得不兼顾更多的团队。我们的产品经理团队也配齐了人员，可以处理更多的团队。增加产出和增加并行性有时也会给我们的设计团队带来压力。\n我们在三人团队的实验上，整体是正面的。我们交付得更快，得到更多的乐趣，并且我们在培养人才方面有所进步。随着三人团队的增加，同样也带来了挑战。我们也识别和训练更多项目领导者，我们必须扩大工程管理，设计和产品管理，以跟上团队的数量。\n","date":"2018-10-24","permalink":"/n3xtchen/2018/10/24/moving-to-three-person-engineering-teams/","section":"时间线","summary":"译自： Moving to three-person engineering teams","title":"三个工程小团队"},{"content":"是人都想要变得更好，做的更出色，不仅仅是在工作，还有在生活。\n如何知道比上个月做的更好 自我感觉在特定领域有所改善，但是改善了多少？ 到底花费了多少时间在偏离目标的事情上？ 我的行为是否遵循我的优先级？ 为了回答上面的问题，我从 2017 年开始我的自我追踪和持续改进之旅。\n我知道手动跟踪所有内容会非常困难，而且生成对比实际变好的指标所花费的时间与我更相关。考虑到这一点，我寻找各种 App 和工具来帮助精简 “自我量化” 的任务。\n结果是一系列追踪 APP 在我的生活背后中大量运转，收集数据来全面了解我的时间实际使用情况，而不是考虑怎么利用时间。\n这篇文章将为你展示我所有用来追踪指标的应用，涵盖你如何使用它们来增加实际帮助优化自己和改善自己的洞见。这些实验都尝试与您的生活方式，小工具和目标产生共鸣。\n一个重要的编注：我的标准之一就是仅仅使用免费应用，但是你应该牢记党你使用免费一些东西时，你本身也是个他们的产品（虽然这种情况经常发生在你付钱的时候）。注册这些应用之前，确保你不介意数据分享和隐私政策。我很乐意和这些公司分享数据，但是这是个人的选择。\n全文通读或者跳到你感兴趣的章节 # 你的阅读量 # 书籍 文章 其他媒体的消费情况 # 音乐 Podcast TV 如何利用时间？ # 自动时间追踪 手动时间追踪 把时间花在哪里？ # 地点 你完成哪些任务以及完成情况？ # 任务 写作 程序 是否关注你的身体健康？ # 步数 睡眠 饮食 如何花钱？ # 收入和支出 还有哪些你比较关心的指标？ # 自定义 如何开始追踪和使用你的个人数据？ # 手机，聚合和分析你的数据 你从这些追踪数据中收获了什么？ 阅读书籍 # 在过去的几年中，我发现音频书籍极大地提高了每年的阅读量。我使用 Goodreads 来追踪我读过的书以及什么时候读的。在阅读的时候，它会鼓励我多做笔记，以至于在读完的时候，我可以写一些正式的书评。它也是一种找新书的工具。\n通过使用 Goodreads，我可以生成这样的报告\n这个报告可以看出我实际的阅读量，它改变了我的阅读习惯。（我强力推荐你使用它）Goodreads 也提供按年分类。\n年度报告帮助你理解你是否挑对书了。如果你的评分都不高，你就需要改变挑书的方式了。\n我也可以通过阅读 review 来温习我看过的好内容。如果你不知道自己学了什么，那么阅读的意义在哪里呢？\n阅读文章 # 如果你查询过我的 Goodreader 年报，你会发现我在 2016 年之前读书量很少。我为什么没有改善？但我知道我看内容的不少，只不过不是书籍，所以我决定我每个月阅读的文章量。\n我的想法就是使用 todoist ——我任务管理的选择——追踪我的阅读清单。我尝试像这样：\n它也可奏效，但是这个方法优三个缺陷：\n数据提取过程他过于手动化 我不能把读一篇文章作为一个任务，我仅仅喜欢使用 Todoist 来做任务管理 一些站点内容很好，但是糟糕的设计/阅读体验 我一个朋友推荐我使用 Pocket。它跨平台，可以作为稍后阅读的应用，解决的第二个问题。\n我也可以从网页中提取内容，使用新的界面阅读，你就可以根据你自己的需求进行定制化，解决了第三个问题。\n但是仍然有一个问题没有解决。我没有找到一种好的应用从 Pocket 中提取数据，所以我只能自己来。\n开发者方案 # 我是一个码农，所以我可以根据自己的需求从 Pocket 到处的 HTML 文件中提取自己想要的结果。我只是下载他们的文件，然后执行下面的代码：\nvar pocket = {\u0026#34;unread\u0026#34;:1420,\u0026#34;unread_pct\u0026#34;:4.79,\u0026#34;read\u0026#34;:1020,\u0026#34;read_pct\u0026#34;:1.18} var uls=document.body.getElementsByTagName(\u0026#34;ul\u0026#34;);unread=uls[0].children.length,pocket.unread_pct=+(100*(1-pocket.unread/unread)).toFixed(2),pocket.unread=unread,read=uls[1].children.length,pocket.read_pct=+(100*(1-pocket.read/read)).toFixed(2),pocket.read=uls[1].children.length,console.log(JSON.stringify(pocket)); 第一行就是三个月的结果：\n{\u0026#34;unread\u0026#34;:1420,\u0026#34;unread_pct\u0026#34;:4.79,\u0026#34;read\u0026#34;:1020,\u0026#34;read_pct\u0026#34;:1.18} 这里，我有 1446 篇文章未读以及读了 1050 文章。增加的百分比是未读 +1.8% 和 读了 + 2.86%。这一个月，我读了 30 篇，就是从这个月已读的总数减去上个月的已读综述\n非开发者方案 # 我在这篇文章的时候，这个方法对很多人来说不可实现。于是，我想了另一个非开发的方案。多亏了我们的编辑 Becky，我构建了一个 IFTTT + Pocket 的原型。\n我创建一个把 Pocket 管来呢到 Google Spreadsheets 的小应用。不幸的是，Pocket 没有提供完成时间的字段，因此，我使用标签来追踪我读文章所在的月份。这个展示结果如下：\nD 列就是这个文章加入到 Pocket 的时间。我决定保留它，以及我可以看到我收藏和阅读的时间。我每次读一篇文章的时候都要添加标签，这样做很不优雅，但是它有效。\n现在，我们有一个表格，可以很方便地获取我们想要的数据。A 列就是我们月份标签，可以给文章排序和统计。我们还没有为这个数据添加图标，但是他们已经足够方便来跟踪我们的趋势。\n音乐 # 我一般系统边听音乐，边工作。有时候，我使用它鼓励自己，有时让他帮助我集中精神。我使用 last.fm；它可以提供一个报表来展示我听过的音乐，以及听的时间。如果你想，你可以把歌和你高产时间进行关联。你可以手动，也可以使用像 Exist.io 这样的工具来进行自由数据聚合。\nLast.fm 也会给你发送年报，而且很有用：\nPodcasts # 我经常在不需要高度集中精神的时候（如洗盘子，遛狗等等），听 Podcasat（或者 音频书）。我已经使用 Podcast Addict 这款 Android 应用很多年了。它是免费的，但是我通过捐赠来去除广告，顺便贡献使他能成为更好的应用。如果你没有 Android 设备，你可以尝试 Pocket Cast，但是你需要为此花费 9 刀。\n这两个应用都有提供统计功能。比如 Podcast Addict，你可以在 setting \u0026gt; stats 看到如下信息：\n不幸的是，这些数据不好提取。我要做年复盘的时候，我必须通过手工方式。通常情况，了解你的内容消费量以及在你的个人发展中贡献量，这个已经足够了。\nTV # 我尽量避免随意看电视（随意意味着无目的：坐下来，不断转台来找），但是我还是会追一些剧。我会事先找好我喜欢的剧目，找时间和家人一起看。它同样要花费一些时间，所以我想要追踪它（在这里，你也可以看出一些模式）。\n我使用 TV Time 来追踪我看过的电视节目。他仍然是需要手工处理——我必须记住打开应用，把它标记为看过——但是很快就形成习惯了。每当我看一集精彩的剧集时，我会在应用程序中看到有趣的梗，立即获得奖励。\n当然，可以看到我的阅读/整体统计来看看我的长期奖励\n追你喜欢的剧没问题，但是要记住，既然投入你的时间，要从中收获什么。问自己，值得吗？这取决于你的决定。妻子和我非常享受这些时间；当然所有的后期剧集分析和预测都很有趣。\n从 TV Time 中唯一缺失的特征是电影追踪。出于这个目的，我决定使用 trakt.tv 来追踪。\n如果能有一个应用能整合 trait.tv 和 tvtime，就好了，但是实际并没有。TV Time 是一款很棒的 Android 应用，但是不知电影。Trakt.tv 支持电影，但是你必须使用其他社区开发的应用来追踪和发送你的统计。所以目前为止，我同时使用他们。\n时间追踪：自动化 # 使用自动追踪应用，你只要设置他，就可以不用管他们了。这个应用运行在后台，持续追踪你做的事情，使用了多少时间。这里，我使用 Rescue Time 来根据你的需求配置。\n你可以定制，让应用记录你的电脑使用情况/或者它要监控哪些应用。例如，你可以高数 RescueTime 制追踪你使用 Excel 的时间，忽略所有你选择不记录的其他应用。\n我使用它来追踪很多应用。我什么都不用做就可以自动获取我一天的时间使用情况：\n我经常从这些报告中获取有价值的内容来帮助自己更好地利用时间。例如，我可以查看应用的分类，问自己是否合理地使用了这些时间。（我看过一本书《When: The Scientific Secrets of Perfect Timing》，书中让我懂得活动的时间那么重要）。随着时间的推移，我可以使用这些数据来优化我的日程，在我精力充沛的时候，安排尽可能多的事情。\n时间追踪：手动 # 不幸的是，自动追踪只能到现在这个程度了。知道花了多少时间在邮件已经很好了，但是用了哪一个客户端？是在工作邮件还是个人邮件？同样问题也出现在其他应用中。手动时间追踪要求你操作，比如开始计时，选择分类——但是一旦你养成习惯，将会使你最小化时间。\n对于手动时间追踪，我是 Toggl 的超级粉丝。他拥有友好的交互界面，易于使用，可以和很多应用整合（包括支持 Todoist），而且跨平台（包括 Linux）。\n我为客户端，类别和项目创建了一个简化的 Toggl 系统，以便轻松提取我感兴趣的报告，同时最大限度地减少添加到我日常活动中的官僚作风。\n我使用客户端的一些例子和他们的项目：\nDoist Shallow work Deep work Meeting Myself Health Finance PotHix Presentation 我根据日常工作、专业生活以及业余项目把报告分成 3 个大类。我追踪开会的时间，以便我后续评估是否值得。如果我发现时间没有很好的利用，我会和我的团队讨论，有规划得使每一个会议更加有用。\n我决定在 Doist 下面维护一些项目，减少选择我正在进行的项目所需的认知负荷。在想要精准数据和避免让自我追踪占用太多时间之间需要做一个权衡。\n当我只关注一个任务的时候，我进入深度工作状态，他不会让我分心——把手机、社交软件等等关掉。（拿编程做个例子）和我不需要很集中精神使使用浅度工作（相关的软件就是 Twist，我们的团队异步通行应用，做项目管理等等）。\n（如果你不知道深度工作和浅度工作的概念，我推荐你读一读 Cal Newport - 《Deep Work》。）\n我 99.99% 的时间在远程工作。Toggl 追踪时间帮助我了解我是否停滞了或者有更多工作要做。我尝试 Doist 在工作日使用 8 个小时。追踪那些时间使用更有意识地计划我的一天，聪明地工作来确保我可以把每一个事情都完成。\n记住：手动追踪很费时费力，你不可能追踪每一个时间。尽可能追踪必要的事情！\n接下来 # 我现在有时候会记录我的位置。我是 Google Latitude 的用户，直到他下线，现在我使用 Google timeline。我不经常在社交网络或者其他地方分享我的位置坐标，但是记录我的位置坐标还是有用的。\n每个月，Google 会发送一条我地理位置的简报：\n在我还在上班的时候，在交通工具上花费的时间用来理解我的通勤时间也是极好的。远程工作之后，看到我在交通工具上使用时间也是不错的。\n我不能提取很多信息来关联我的地理数据，但是能可以看到我过去访问过哪些地方，也是挺好的，尤其当我在旅游的时候。\n任务 # 虽然可能看起来有点偏颇，但在我的印象中，我几乎从一开始就是Todoist用户——准确说 2007年——10年后，我到 doist 工作。\n我申请 API 和整合开发工程的职务的原因，很多出于我喜欢这个产品，认同公司的价值。在过去十年中，我使用了很多任务列表应用，最终我决定回到 Todoist，因为它有我所有想要的功能。\nTodoist 有一个有效的视角，在那里， 你能看到你在某个时间点完成某些事情的高级统计报表。我不是坚持日常任务目标的超级粉丝。（虽然如果你愿意，你可以改变它们，让它更容易，或者如果你愿意，可以更多挑战自己），但是我真心喜欢在每个月看到周视角的大图：\n这个报告展示了我如何按找拆解我的任务。红色条状是我的私人任务，蓝色的是工作任务。在我的日常中，时常有很多细小的个人任务，我的工作任务则是很大块，需要深度工作。为了使这个视图更有用，你得为你要追踪的工作分类选择不同颜色，便于查看。\n这样的报道加上 Toggl 和 RescueTime 提供的报告，让我想知道我是否花了太多时间阅读我的 Twist 线程。\n我是一个早上的人，我本应该花更多时间在做深度工作的时候，我却常常在读 Twist（一种团队协作工具）。计划是在早上浏览我的 twisted 帖子并将我需要跟进的对话直接从应用程序转换为Todoist任务，以便我可以在下午处理它们。（让 twist 的未读数量为0 会让我好受点，我可以在Todoist中更好地开展后续工作）。\n有了这些数据，我可以根据以前没有注意到的趋势得出一些结论。\n另外值得称道的是 Todoist 的年报（在每年的一月份发出，这么做不仅仅为了娱乐，其中也暗含一些有用的个人洞见）：\n身体和健康 # 这个目录有很多可用的追踪器。，他们中大部分都可以附带可穿戴设别，当然需要额外的付费，他们追踪各种各样的东西。你可以购买他们，当然你也可以保持简单，使用免费或者最少花费的部分。\n步数 # 当然，我知道步数不是衡量健康状况的好指标，但是走路确实有益健康。如果你，比方我，关心每天走了多少步，Google Fit 将会是不错的选择。\nGoogle Fit 追踪步数，距离以及猜测你正在进行体育运动类型（最后一项，现在看来不是很准确）。其实很简单；你只要安装它，然后随身携带你的手机即可（这不是废话吗？）。\n以我为例，我使用 30 刀的腕带来帮助我追踪我的步数，它还可以追踪我的睡眠。\n睡眠 # 现在手机应用都可以用来追踪你的睡眠，但是，个人不喜欢在睡觉的时候，离手机太近了。出于这个原因，我决定买一个腕带来追踪我的睡眠模式，而且还能让步数统计更准确。\n你可以买到各种各样的智能腕带，价格从几十刀到几百刀不等。我选择了一个不是很贵，而且可以满足的需求——小米手环2——但是可能大部分腕带可能都能满足我的需求。至于要不要全天带在手腕，这就是你自己的选择。\n下面的图就是我从 MiFit 获取的睡眠报告：\n这里可以看到按天、周、月维度看我的睡眠时间。上面的截图还是相当好用的，因为我不仅可以看到我的睡眠时间，还可以看出我的睡眠质量。深紫色就是展示我的深度睡眠。\n从这些报告中，我可以分析我每次的数据，来评估是否足够好了，还是我应该改变一下自己的日常来改善我的睡眠时间/质量。截屏上的一个好例子就是我的平均睡眠时间：\u0026lt; 7 小时就是不好的，因此我将提出一个计划来帮助我在下个月改善。\n饮食 # 我发现用来追踪饮食和锻炼的最好应用是 MyFitnessPal。他们的饮食数据是无比的。我追踪我的日常摄入的食物和锻炼，但有时没有坚持做。我在节食过程中做过这种跟踪，体验非常出色。\n追踪你摄入的食物是很难的，因为它一个纯手工的过程，但是如果你遵循某种饮食习惯，结果是很有价值的。但是，可以偶尔追踪下一周，提供一个数据点，看啊看你是否进行正确的饮食习惯，从种类上和量上。\n财务状况 # 钱是我要追踪的最重要内容中的一种。我已经坚持好多年了。请记住财务追踪和控制支出不是同一个东西。你可以不改变生活的习惯，就能追踪你的财务情况。最棒的是，你可以查看你的收支，以及他们是否使你的生活更有价值。你也查看自己的以往的花费数据，来决定你能否支付一项大额支出——比如房产或者长途旅游。\n我使用本地应用，连接我的银行来获取我的支出。他们通过连接到你信用卡公司，来使用你的数据。如果你可以接受，它将是一个简洁的工具，把你在追踪花费分类下节省大量的时间，但是它是用一个国家，他们不是以英语作为本地语言。\n在这个应用中，我可以生成下面这张图，展示每一个类的占比（如娱乐，家庭，以及旅游等等。）：\n与其推荐一款大家都不能用应用，我建议大家使用 YNAB（你需要一个预算的简称）。它们有自己的一套独特财务管理哲学和系统，但是也包含了我要使用的特征：\nYNAB 的最大问题是需要付费，因此，它是否值得你投资。Mint 和 Clarity Money 也是一个不错的免费备选方案，但是没有相关的使用经验。\n写作 # 你知道你每个月写了多少字吗？当你用英文写作时，你犯了多少错吗？Grammarly 可以帮助你解答这些问题。\n如果你非英文母语，我强烈推荐它。它能识别不被注意常见的错误（比如介词）。\nGrammarly 每个月会发送一个简报，告诉你上一个月的统计数据：\n不幸的是，Grammarly 只提供的这个简报，于是我做了一些手动提取工作来创建一个表格：\n能看出写作字数的增长，是不是很棒？\n当然，你可以挑战每个月的字数目标。\n代码追踪 # 这个章节适用于平时有写代码需求的人。如果你想要追踪你每天在写代码的时间，使用的语言/编辑器，我强烈推荐使用 Wakatime。\n我在我的 now 页面（基于 Derek Siver now page）使用它们内置的图表。我最喜欢的图表之一就是我的语言突破：\n为了让他奏效，你必须在你的 IDE/文本编辑器中安装它，但是这个没什么难度。\n你自定义的指标 # 有时你想要追踪一些特定的指标，但是没有 apps 提供这个功能。在这种场景下，你需要回到 Google Spreadsheets 来构建你的自己指标。举个例子，我设置了一个目标，一年做 20 次公共演讲，然后按月来追踪进度。\n下面的图就是我去年最终图表：\n我能够知道哪些地方和我的计划相关，将会是很棒的事情。你可以看到事情的转变，在四月的时候，我看到我在我计划内。\n你可以为了任何一个指标，创建一个 “goal vs actual\u0026quot; 曲线——读书，发布的文章，跑的里程以及收入等等。Lucid Chart 有一个很棒的新手引导，帮助快速学会使用它。\n如果你使用 Google Sheet 追踪你想关注的一个或两个目标，可视化进度的额外冬季将值得你手动操作。（向 Todoist 添加每周或每月定期任务可能会有所帮助，以提醒您时刻跟进它。）\n如果你不想提供数据给第三方公司，电子表格软件将会是很棒的工具，追踪任何我想要覆盖到的事情。你可以使用 Google Spreadsheets，Excel 或者 OpenOffice Calc 来创建你自己报表系统。但是要记住创建和维护这样的追踪系统需要花一些时间和努力，所以如果你不介意安装写软件来追踪的话，是一个不错的选择。\n如何开展数据收集和分析？ # 我趋于最便宜，最强定制化的追踪配置，但是有时花点钱让一些事情更好的落地。让我们评估下免费和付费方式来汇总你的数据。\n免费 # 你不用同时使用所有的这些应用或策略。如果你想要开始自己收集数据，你可以挑选一个应用/策略，应用到你生活的一个特定领域。\n例如，你应该开始使用 RescueTime。它不需要日常手动输入（只需要在开始的时候，进行一些分类配置），你就可以在几周之后看到一些有趣的模式。\n付费 # 如果你开始使用应用开始追踪的你数据，为一些有用的服务付费，我将会推荐尝试下 Exist.io。它是一个付费服务，但是你可以免费试用 30 天，如果你是被推荐的，就可以试用 60 天。如果你想要试用 2 个月，我可以提供给你我的推荐链接。\n你可以把它但做一种个人仪表盘，整合所有的数据，容易使用和分析。这款应用通过链接你的日常使用来引导你：\n你很快就能看到像下面这样的报表：\n为了以你的数据为中心，应用允许你为每一天添加自动标签，它会自动分拆出有趣的洞见——比如生产和锻炼的相关性，甚至体重和位置相关性。它也能根据现在的平均水平来设置个人目标，甚至内置情绪追踪（这个指引没有涵盖这个内容）。\n它一个很棒的服务，很棒的公司。\n从这些追踪数据中收获了什么？ # 一句话，可以持续改善。好的习惯很难形成。坏的习惯也很难打破。如果没有意识的提升到生活中，就很难做到这两件事。\n追踪你的数据可以真实的展示你平时时间使用情况（而不是你考虑如何利用时间）。不是对逝去的时间的责备或内疚。它是一种通知模式，并为你下个月改善作出有意识的决定。它给你变得更好的机会——而不是你看起来像什么。\n以有形，可衡量的方式将您的生活与您的价值观和优先事项保持一致，这也感觉很好。\n这里是我常规检查我的指标之后归纳我所能改善的点：\n检查我是否符合四月预期的前提下，改善我演示文档的外观 改变我选书的方式，因为我读过书的评分普遍比较低 我需要改变的工作方式，因为我不能聚焦我想做的事情 停止在日常时间阅读我的 newsfeed，因为已经加了很多文件到 Pocket 中，都没有读 减少听播客的时间。使用有声读物来替代，因为它对我的生活注入更多的价值 增加一个预警提醒我是否每天使用 Telegram 超过 30 分钟 追踪每天我花在工作上的时间。我通过加入硬性指标，来聚焦我完成的事情和更智能的工作。通过这些，来平衡我的工作和生活 这些改变已经为我的生活注入的大量的价值。你可能得到同样的结论。就像大家说的，从来都没有全栈解决的方案。你可以从这里读到所有生活和工作的建议，但是最终，每个人都是不一样的。汇总你自己的数据让它根据你的想法来优化你的生活。你可能永远都不能达到你的理想状态（这是多么遗憾的一件事情），但是你可以沿着这条路需有改善它。\n","date":"2018-10-09","permalink":"/n3xtchen/2018/10/09/quantify-yourself/","section":"时间线","summary":"是人都想要变得更好，做的更出色，不仅仅是在工作，还有在生活。","title":"量化你自己: （大部分）免费的工具和策略来追踪你生活中的（几乎）每一个角落"},{"content":"","date":"2018-08-17","permalink":"/n3xtchen/categories/data-science/","section":"分类页","summary":"","title":"DATA-Science"},{"content":"","date":"2018-08-17","permalink":"/n3xtchen/tags/ml/","section":"标签","summary":"","title":"ml"},{"content":"很久以前，有一个男孩，他名字叫数据。穷其一生，他一直在尝试理解生命的意义。我的价值是什么？我能对这个世界找成什么影响？我来自哪里？这些问题一直萦绕在他的脑海中；幸运的是，纯粹运气，数据最后找到了一个方法，并通过了一次伟大的变革。\n这一切都始于数据在他遇到一个奇怪但有趣的管道（Pipe）时走下行。管道的一端是入口，另一个端是出口。这个管道也可以使用 5 个字符来标记：“O.S.E.M.N”。好奇心使然，他决定进入管道。长故事短说……进入的是数据，出来的是洞见（Insight）。\n数据科学就是 OSEMN # 作为有抱负的数据科学家，你有机会磨练你的巫师和侦探的力量。在巫师的一面，我的意思是指拥有预测的能力。在侦探的一面，你可以找到数据的未知模式和趋势。\n理解数据科学管道的标准工作流是通往业务理解和问题解决的关键步骤。我发现一个非常简单的首字母缩写来概括数据科学管道。那就是 O.S.E.M.N。\n请记住：本文将简要介绍对典型数据科学管道中的预期的高级概述。从构建业务问题来创建可操作的见解。不要当心，很好理解的！\nOSEMN 管道 # O(Obtain): 获取数据 S(Scrub): 清洗你的数据 E(Explore): 探索/可视化数据，帮助我们寻找模式和趋势 M(Model): 为你的数据构建模型，将赋予我们预测的巫师魔力 N(iNterpret): 解释我们的数据 业务问题 # 开始 OSEMN 管道之前，最至关重要的一步，以及我们必须考虑了的一点，理解我们尝试解决的问题。\n问你自己：\n我们如何把数据转化成现金？ 我可以使用这些数据产生多大的影响？ 我们的模型带来怎么样的商业价值？ 什么能帮助我们节省成本？ 有哪些可以事情让我们的业务更有效地开展？ 理解这个基础概念将让你走的更远，让你向数据科学家（我并不是，但是坚信能）迈出一大步。尽管如此，不管你预测的模型效果有多好，不管有多少数据，不管你的管道有多 OSEMN…你的方案和可操作洞见只能很好地适用于你要解决的问题。\n\u0026ldquo;出色的数据科学有赖于你对数据提出的问题而不仅仅是数据清洗和分析\u0026rdquo; —— Riley Newman\n获取数据 # 作为数据科学家，没有数据寸步难行。根据经验，在你获取数据时候，有些事情你必须考虑。你必须鉴定你所拥有的所有数据（可能来源于网络、外部或内部数据库）。你必须把数据抽取（extract）成可用的数据结构（.csv，json，xml 等等）\n技能要求 # 数据库管理: MySQL/PostgreSQL/MongDB 关系数据库查询技术 提取无结构的数据：文本，视频，音频以及文档 分布式存储：Hadoop，Apache Spark/Flink 清洗数据 # 管道的这个阶段通常是最耗费时间和精力的。因为结果和机器学习的输出只和你的输出强相关。俗话说，龙生龙凤生凤，老鼠生的孩子会打洞。同样也适用于数据科学，输入模型的数据质量越好，噪音越小，输出模型的性能指标通常就越好。\n目标 # 检查数据：了解您正在使用的每一个特镇，识别错误，缺失值和损坏记录 清洗数据：丢弃、替换以及填补缺失值 技能要求 # 脚本语言：Python/R/SAS 数据清洗工具：Python Panda/R 分布式处理：Hadoop/MapReduce/Spark \u0026ldquo;准备充分的人已经把战打了一半\u0026rdquo; —— Miguel de Cervantes\n探索（探索性数据分析） # 现在进入探索阶段，我们尝试了解数据的模式和价值。我们将使用不同类型的可视化和统计测试来支撑我们的发现。\n技能要求 # Python: Numpy/Matplotlib/Pandas/Scipy R: GGplot2/Dplyr 推论统计：在统计学中，研究如何根据样本数据去推断总体数量特征的方法。 它是在对样本数据进行描述的基础上，对统计总体的未知数量特征做出以概率形式表述的推断。 实验设计 数据可视化 提示：在分析的过程中，会产生 蜘蛛感官 带来的刺痛；即有意识（故意）地去发现奇怪的模式和趋势。\n蜘蛛感官：在《蜘蛛侠》中，男主角Peter是因為被一只轉基因的蜘蛛咬傷後，擁有非凡超能力，特異功能，並且也擁有了“蜘蛛感官”，他能感受到周圍存在的潛在危險，即使蜘蛛俠在行俠仗義時，在車輛快速行駛過程馬路上，任然可以躲避被車撞到的危險 。\n设计考虑：大部分情况就是直接进行可视化。 这都是关于最终用户的解释。 专注于您的受众。\n模型（机器学习） # 现在进入大家感兴趣的环节。模型在统计学意义上是通用的规则。你可以把机器学习作为你工具集中的一个工具。你可以使用各种各样的算法来达到不同商业目的。你的特征越好，你的预测能力就越强。在清洗数据和寻找重要特征之后，使用模型作为预测工具将强化你的业务决策力。\n预测力的一个例子：其中一个很棒例子就在在沃尔玛供应链中。沃尔玛可以预测飓风季节，在他们某个商场将会卖光他们的草莓流行挞。通过数据挖掘，他们的数据显示：飓风来临前的大部分畅销单品都是流行挞。听起来很疯狂，这是一个真实的故事，并提出了不低估预测分析能力的观点。\n目标 # 深度分析：使用预测模型/算法 评估和优化模型 技能要求 # 机器学习：监督/无监督算法 评估方法 机器学习算法库：Python(Sci-kit)/R(CARET) 线性代数和多元微积分 解释（讲述数据故事） # 这是故事时间！管道最重要的步骤是理解和学习如何解释你的发现。讲故事是关键，不要低估它的作用。它将余人对接，说服他们和帮助他们。\n在故事讲述过程中，情感是关键的驱动力。人们不会奇迹般地马上理解的的发现。产生影响的最佳方式就是通过情感讲故事。作为人类，很大程度受情绪影响。如果你能让你受众感同身受，那一切将在你的掌握中。当你在呈现数据的时候，牢记心理学的重要。了解你的受众，并和他们产生链接的艺术就是数据故事的最佳中的一份。\n强化说故事能力的最佳实践就是不断排练。\n目标 # 验证你的业务洞见：回顾到业务问题 可视化你的发现：保持简单和优先级驱动 讲述一个清洗可行的故事：与非技术受众进行有效地沟通 技能要求 # 业务领域知识 数据可视化工具：Tableau/D3JS/Matplotlib/GGplot/Seaborn 沟通：展示/讲述 和 报告/写作 更新模型 # 不要担心的故事无止境。线上的模型，需要进行周期行的更新，频率依赖于你获取新数据的频率。假设你是亚马逊的数据科学家，你为客户推出了一项新功能，购买“鞋功能”。 你是旧模型没有这个，现在你必须更新包含此功能的模型。 如果没有，您的模型会随着时间的推移而降级，并且性能不会很好，从而使您的业务也会降级。 新功能的引入将通过不同的变化或可能与其他功能的相关性来改变模型性能。\n结语 # 实际上，你遇到的大部分问题都是工程问题。即便你拥有机器学习大神的所有资源，产生的最大影响都来源于伟大的特征，而不是牛逼的机器学习算法。因此，基本方法论：\n确保你的管道是扎实地端对端 从一个合理的目标出发 直观理解你的数据（可视化） 确保你的管道持续坚固（通过更新） 希望这些方法能够创造更多的价值（造福于企业），或者让很多人在很长的一段时间内感到 Happy（造福于用户）。\n因此，下一次如果有人问你什么是数据科学。大胆地告诉他们：\n“数据科学就是 OSEMN”\n","date":"2018-08-17","permalink":"/n3xtchen/2018/08/17/data-science-is-osemn/","section":"时间线","summary":"很久以前，有一个男孩，他名字叫数据。穷其一生，他一直在尝试理解生命的意义。我的价值是什么？我能对这个世界找成什么影响？我来自哪里？这些问题一直萦绕在他的脑海中；幸运的是，纯粹运气，数据最后找到了一个方法，并通过了一次伟大的变革。","title":"数据的一生: OSEMN 数据科学流水线方法"},{"content":"我是用一个称之为 生产日志（productivity journals） 来管理我的长期目标。\n生产日志 有两种类型：\n短期日志（下文简称 STJs）：按月维护 长期日志（下文简称 LTJs）：保持 5 年 指定这个时间的依据是，有足够长的时间处理短期可记忆的事务，并且足够短来作出合理可靠的长期决策。（这个时间窗口可以任意定制；重要的是长短期日志需要区分开）。\nLTJ 作为你的基石，需要定期回访来确保你的短期行为和你的长期目标和原则一致。\nSTJ 每日使用，需要设计每周和每月需要冲击计划来贴合短期的目标进展。通过记录你的进展，你可以做出适当的调整，让你更高效地追逐你的长期目标。\nLTJ 和 STJ 一起是一个强大的协同组合：LTJs 允许自上而下方法来实现目标。STJs 是自下而上的，\n","date":"2018-03-18","permalink":"/n3xtchen/2018/03/18/journaling-to-align-lt-goals-and-st-goals/","section":"时间线","summary":"我是用一个称之为 生产日志（productivity journals） 来管理我的长期目标。","title":"管理自己的长期和短期目标：使用日志（Journals）"},{"content":"Spark ML Pipeline 有各种各样的算法同时，你可能发现自己也需要不离开 Pipeline 模型也可以使用额外的函数；在 Spark MLlib 中，就不是个事 —— 你可以使用 RDD 转换器实现自己的算法。至于 Spark ML Pipeline ，同样的方法也可用，但是我们失去了 Pipeline 优雅的整合方式，包括自动运行元算法（meta-algorithms，对其他算法进行组合的方式），例如参数交叉验证（cross-validation parameter search）。在这篇文章中，将使用 word count 示例作为入门课程，来演示 Spark ML Pipeline Model 类编写（做算法的哥么永远无法摆脱 word count 示例，^_^）。\n如果你想把你自实现的算法添加到 Spark Pipeline 中，则需要实现 Estimator 或 Transformer（它们都是 PipelineStage 接口的实现）。对于不要求训练的算法，你只要实现 Transformer 接口；至于需要训练的算法，同时还需要实现 Estimator 接口，你的实现都需要 org.apache.spark.ml 包下。你还需要注意训练不仅限于复杂的机器学习模型；MinMaxScaler 也需要训练来决定范围。如果他们需要训练，那么他们必须实现 Estimator 而不是 Transform。\n注意 # 直接使用 PipelineStage 将无法奏效，因为 Pipeline 内部采用的是反射，而它假定所有的阶段（Stages）不是 Estimator 就是 Transformer。\n除了典型的 transform 和 fit 方法外，所有的 Pipeline Stage(管道阶段) 还需要提供 transformSchema 和 copy 方法或实现一个类——copy 用于给当前阶段制作副本，它会将新指定参数合并进来，简单的调用 defaultCopy 方法，除非你的类有自定义的构造函数 ）\n在 Pipeline Stage 的开始或者副本委派阶段，transformSchema 必须根据任何参数集和输入模式产生你的 Pipeline Stage 所期待的输出。大部分 Pipeline Stage 简单的加入新的字段；除非需要，一般不会丢弃之前阶段的字段，但是有时会导致比下游需要的包含更多数据，这样会导致性能问题。如果你发现在你的 Pipeline 这是个问题，你可以创建你自己的 Stage 来去除不必要的字段。\nclass HardCodedWordCountStage(override val uid: String) extends Transformer { def this() = this(Identifiable.randomUID(\u0026#34;hardcodedwordcount\u0026#34;)) def copy(extra: ParamMap): HardCodedWordCountStage = { defaultCopy(extra) } } 除了生成输出的模式（schema），transformSchema 函数应该验证输入模式（schema）是否符合当前 Stage（例如，输入的字段是否是预期类型）。\n这里同样也是你执行 Stage 参数验证的地方。\n一个简单的 transformSchema ，输入字符串，输出向量，如下所示（字段名都是硬编码，大家不要太 care）：\noverride def transformSchema(schema: StructType): StructType = { // Check that the input type is a string val idx = schema.fieldIndex(\u0026#34;happy_pandas\u0026#34;) val field = schema.fields(idx) if (field.dataType != StringType) { throw new Exception(s\u0026#34;Input type ${field.dataType} did not match input type StringType\u0026#34;) } // Add the return field schema.add(StructField(\u0026#34;happy_panda_counts\u0026#34;, IntegerType, false)) } 不需要训练的算法只要使用 Transformer 接口就可以轻易实现了。由于是一个简单的 Pipeline Stage，我们可以实现一个简单的转换器（transformer），输入字符串，返回字数。\ndef transform(df: Dataset[_]): DataFrame = { val wordcount = udf { in: String =\u0026gt; in.split(\u0026#34; \u0026#34;).size } df.select(col(\u0026#34;*\u0026#34;), wordcount(df.col(\u0026#34;happy_pandas\u0026#34;)).as(\u0026#34;happy_panda_counts\u0026#34;)) } 为了充分利用 Pipeline 接口，您需要使用 params（参数） 接口使您的 Pipeline Stage 可配置。\nparams 接口都是公有（public）的，然而不幸的是，Spark 内部常用的默认参数是私有（private）的，因此你会得到一份很多重复的代码。除了允许用户指定值之外，参数也可以包含一些基本的验证逻辑（e.g. 正则化参数必须是非负数）。最常用的两个常数就是输入字段和输出字段。\n字符参数外，其他的类型也可以使用，包括停用词的字符串列表。\nclass ConfigurableWordCount(override val uid: String) extends Transformer { final val inputCol= new Param[String](this, \u0026#34;inputCol\u0026#34;, \u0026#34;The input column\u0026#34;) final val outputCol = new Param[String](this, \u0026#34;outputCol\u0026#34;, \u0026#34;The output column\u0026#34;) def setInputCol(value: String): this.type = set(inputCol, value) def setOutputCol(value: String): this.type = set(outputCol, value) def this() = this(Identifiable.randomUID(\u0026#34;configurablewordcount\u0026#34;)) def copy(extra: ParamMap): HardCodedWordCountStage = { defaultCopy(extra) } override def transformSchema(schema: StructType): StructType = { // Check that the input type is a string val idx = schema.fieldIndex($(inputCol)) val field = schema.fields(idx) if (field.dataType != StringType) { throw new Exception(s\u0026#34;Input type ${field.dataType} did not match input type StringType\u0026#34;) } // Add the return field schema.add(StructField($(outputCol), IntegerType, false)) } def transform(df: Dataset[_]): DataFrame = { val wordcount = udf { in: String =\u0026gt; in.split(\u0026#34; \u0026#34;).size } df.select(col(\u0026#34;*\u0026#34;), wordcount(df.col($(inputCol))).as($(outputCol))) } } 需要训练的算法则需要实现 Estimator 接口 —— 尽管对于许多算法，org.apache.spark.ml.Predictor 或 org.apache.spark.ml.classificationClassifier 助手类更容易实现。Estimator 和 Transformer 最本质的不同：除了直接表达对输入的转换之外，还要有一个训练步骤（ train 函数的形式）。一个字符索引（String Indexer）是一个你可以实现的最简单评估器（Estimator），虽然他已经在 Spark 中实现了，但是仍然不影响它成为一个演示 Estimator 接口使用的好例子。\ntrait SimpleIndexerParams extends Params { final val inputCol= new Param[String](this, \u0026#34;inputCol\u0026#34;, \u0026#34;The input column\u0026#34;) final val outputCol = new Param[String](this, \u0026#34;outputCol\u0026#34;, \u0026#34;The output column\u0026#34;) } class SimpleIndexer(override val uid: String) extends Estimator[SimpleIndexerModel] with SimpleIndexerParams { def setInputCol(value: String) = set(inputCol, value) def setOutputCol(value: String) = set(outputCol, value) def this() = this(Identifiable.randomUID(\u0026#34;simpleindexer\u0026#34;)) override def copy(extra: ParamMap): SimpleIndexer = { defaultCopy(extra) } override def transformSchema(schema: StructType): StructType = { // Check that the input type is a string val idx = schema.fieldIndex($(inputCol)) val field = schema.fields(idx) if (field.dataType != StringType) { throw new Exception(s\u0026#34;Input type ${field.dataType} did not match input type StringType\u0026#34;) } // Add the return field schema.add(StructField($(outputCol), IntegerType, false)) } override def fit(dataset: Dataset[_]): SimpleIndexerModel = { import dataset.sparkSession.implicits._ val words = dataset.select(dataset($(inputCol)).as[String]).distinct .collect() new SimpleIndexerModel(uid, words) ; } } class SimpleIndexerModel( override val uid: String, words: Array[String]) extends Model[SimpleIndexerModel] with SimpleIndexerParams { override def copy(extra: ParamMap): SimpleIndexerModel = { defaultCopy(extra) } private val labelToIndex: Map[String, Double] = words.zipWithIndex. map{case (x, y) =\u0026gt; (x, y.toDouble)}.toMap override def transformSchema(schema: StructType): StructType = { // Check that the input type is a string val idx = schema.fieldIndex($(inputCol)) val field = schema.fields(idx) if (field.dataType != StringType) { throw new Exception(s\u0026#34;Input type ${field.dataType} did not match input type StringType\u0026#34;) } // Add the return field schema.add(StructField($(outputCol), IntegerType, false)) } override def transform(dataset: Dataset[_]): DataFrame = { val indexer = udf { label: String =\u0026gt; labelToIndex(label) } dataset.select(col(\u0026#34;*\u0026#34;), indexer(dataset($(inputCol)).cast(StringType)).as($(outputCol))) } } 如果你实现一个 迭代算法（iterative algorithm），你可能需要考虑自动缓存输入数据（如果它没有被缓存），或者允许用户指定持久化等级。\nPredictor 接口添加三个最常用的参数（输入和输出字段）： 标签字段（Label）、 **特征字段（Featuire）**和预测字段——为我们提供自动处理模式的转换器（schema transformation）。\nClassifier 接口也做了同样的事情，除了他还添加了一个 rawPredictionColumn 字段和提供工具来侦测分类的个数，并将输入的 DataFrame 转化成 LabeledPoint 的 RDD（这使得封装遗留 MLlib 分类算法更加容易）。\n如果你要实现一个 回归（regression） 或者 聚类（clustering） 接口，没有公有基础接口使用，因此你需要使用普通的 Estimator 接口。\n// Simple Bernouli Naive Bayes classifier - no sanity checks for brevity // Example only - not for production use. class SimpleNaiveBayes(val uid: String) extends Classifier[Vector, SimpleNaiveBayes, SimpleNaiveBayesModel] { def this() = this(Identifiable.randomUID(\u0026#34;simple-naive-bayes\u0026#34;)) override def train(ds: Dataset[_]): SimpleNaiveBayesModel = { import ds.sparkSession.implicits._ ds.cache() // Note: you can use getNumClasses and extractLabeledPoints to get an RDD instead // Using the RDD approach is common when integrating with legacy machine learning code // or iterative algorithms which can create large query plans. // Here we use Datasets since neither of those apply. // Compute the number of documents val numDocs = ds.count // Get the number of classes. // Note this estimator assumes they start at 0 and go to numClasses val numClasses = getNumClasses(ds) // Get the number of features by peaking at the first row val numFeatures: Integer = ds.select(col($(featuresCol))).head .get(0).asInstanceOf[Vector].size // Determine the number of records for each class val groupedByLabel = ds.select(col($(labelCol)).as[Double]).groupByKey(x =\u0026gt; x) val classCounts = groupedByLabel.agg(count(\u0026#34;*\u0026#34;).as[Long]) .sort(col(\u0026#34;value\u0026#34;)).collect().toMap // Select the labels and features so we can more easily map over them. // Note: we do this as a DataFrame using the untyped API because the Vector // UDT is no longer public. val df = ds.select(col($(labelCol)).cast(DoubleType), col($(featuresCol))) // Figure out the non-zero frequency of each feature for each label and // output label index pairs using a case clas to make it easier to work with. val labelCounts: Dataset[LabeledToken] = df.flatMap { case Row(label: Double, features: Vector) =\u0026gt; features.toArray.zip(Stream from 1) .filter{vIdx =\u0026gt; vIdx._2 == 1.0} .map{case (v, idx) =\u0026gt; LabeledToken(label, idx)} } // Use the typed Dataset aggregation API to count the number of non-zero // features for each label-feature index. val aggregatedCounts: Array[((Double, Integer), Long)] = labelCounts .groupByKey(x =\u0026gt; (x.label, x.index)) .agg(count(\u0026#34;*\u0026#34;).as[Long]).collect() val theta = Array.fill(numClasses)(new Array[Double](numFeatures)) // Compute the denominator for the general prioirs val piLogDenom = math.log(numDocs + numClasses) // Compute the priors for each class val pi = classCounts.map{case(_, cc) =\u0026gt; math.log(cc.toDouble) - piLogDenom }.toArray // For each label/feature update the probabilities aggregatedCounts.foreach{case ((label, featureIndex), count) =\u0026gt; // log of number of documents for this label + 2.0 (smoothing) val thetaLogDenom = math.log( classCounts.get(label).map(_.toDouble).getOrElse(0.0) + 2.0) theta(label.toInt)(featureIndex) = math.log(count + 1.0) - thetaLogDenom } // Unpersist now that we are done computing everything ds.unpersist() // Construct a model new SimpleNaiveBayesModel(uid, numClasses, numFeatures, Vectors.dense(pi), new DenseMatrix(numClasses, theta(0).length, theta.flatten, true)) } override def copy(extra: ParamMap) = { defaultCopy(extra) } } // Simplified Naive Bayes Model case class SimpleNaiveBayesModel( override val uid: String, override val numClasses: Int, override val numFeatures: Int, val pi: Vector, val theta: DenseMatrix) extends ClassificationModel[Vector, SimpleNaiveBayesModel] { override def copy(extra: ParamMap) = { defaultCopy(extra) } // We have to do some tricks here because we are using Spark\u0026#39;s // Vector/DenseMatrix calculations - but for your own model don\u0026#39;t feel // limited to Spark\u0026#39;s native ones. val negThetaArray = theta.values.map(v =\u0026gt; math.log(1.0 - math.exp(v))) val negTheta = new DenseMatrix(numClasses, numFeatures, negThetaArray, true) val thetaMinusNegThetaArray = theta.values.zip(negThetaArray) .map{case (v, nv) =\u0026gt; v - nv} val thetaMinusNegTheta = new DenseMatrix( numClasses, numFeatures, thetaMinusNegThetaArray, true) val onesVec = Vectors.dense(Array.fill(theta.numCols)(1.0)) val negThetaSum: Array[Double] = negTheta.multiply(onesVec).toArray // Here is the prediciton functionality you need to implement - for ClassificationModels // transform automatically wraps this - but if you might benefit from broadcasting your model or // other optimizations you can also override transform. def predictRaw(features: Vector): Vector = { // Toy implementation - use BLAS or similar instead // the summing of the three vectors but the functionality isn\u0026#39;t exposed. Vectors.dense(thetaMinusNegTheta.multiply(features).toArray.zip(pi.toArray) .map{case (x, y) =\u0026gt; x + y}.zip(negThetaSum).map{case (x, y) =\u0026gt; x + y} ) } } 注意 # 如果你知识简单的修改现有的算法，你可以拓展它（伪装成 org.apache.spark 项目 ）。\n现在你已经学会如何拓展 Spark ML Pipeline 的 API。如果你忘记了，有一个好的参考就是 Spark 的算法实现。——虽然它有时使用内部的 API，但是大部分地方他们都是实现公有接口（如你想要的方式）。\nExtend Spark ML for your own model/transformer types ","date":"2018-03-06","permalink":"/n3xtchen/2018/03/06/spark-ml-customer-modeltransformer-type/","section":"时间线","summary":"Spark ML Pipeline 有各种各样的算法同时，你可能发现自己也需要不离开 Pipeline 模型也可以使用额外的函数；在 Spark MLlib 中，就不是个事 —— 你可以使用 RDD 转换器实现自己的算法。至于 Spark ML Pipeline ，同样的方法也可用，但是我们失去了 Pipeline 优雅的整合方式，包括自动运行元算法（meta-algorithms，对其他算法进行组合的方式），例如参数交叉验证（cross-validation parameter search）。在这篇文章中，将使用 word count 示例作为入门课程，来演示 Spark ML Pipeline Model 类编写（做算法的哥么永远无法摆脱 word count 示例，^_^）。","title":"Spark ML: 创建你自己的算法管道"},{"content":"今天遇到个简单的错误，在这里与大家分享下。\n测试脚本如下：\nimport org.apache.spark.sql.{SparkSession, DataFrame} val spark = sparkSesssion.builder.getOrCreate() import spark.implicits._ val foo = Seq((1, 2, 3), (3, 4, 5)).toDF(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) val selectFields = Seq(\u0026#34;a\u0026#34;, \u0026#34;c\u0026#34;) foo.select(selectFields: _*).show 本来很简单的东西，结果报错了！\n[error] /path/to/your.scala:XX: overloaded method value select with alternatives: [error][U1](c1: org.apache.spark.sql.TypedColumn[org.apache.spark.sql.Row,U1])org.apache.spark.sql.Dataset[U1] \u0026lt;and\u0026gt; [error] (col: String,cols: String*)org.apache.spark.sql.DataFrame \u0026lt;and\u0026gt; [error] (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame [error] cannot be applied to (Array[String]) [error] df.select(colNames).toDF(fieldNames: _*) [error] ^ [error] one error found [error] (compile:compileIncremental) Compilation failed 好死不死，没有认真看报错信息或者看接口文档，调到我怀疑人生。\n其实很简单的， 是 DataFrame.select 的封装如此，在认真看看报错信息：\n[error] (col: String,cols: String*)org.apache.spark.sql.DataFrame \u0026lt;and\u0026gt; [error] (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.DataFrame 为什么会分装成这样。。。针对传入的是字符，必须是 (col=\u0026quot;字符\u0026quot;, cols=Seq(\u0026quot;字符列表\u0026quot;): _*)\n所以正确的代码是\nimport org.apache.spark.sql.{SparkSession, DataFrame} val spark = sparkSesssion.builder.getOrCreate() import spark.implicits._ val foo = Seq((1, 2, 3), (3, 4, 5)).toDF(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) val selectFields = Seq(\u0026#34;a\u0026#34;, \u0026#34;c\u0026#34;) // 如果传入的是字符串序列 foo.select(selectFields.head, selectFields.tail: _*).show // 如果字符序列转化成 DataFrame.col，就可以直接传可变参数 foo.select(selectFields.map(fool.col(_)): _*).show 再次吐槽下，为什么对字符串序列和列序列不用同一种封装。。。\n","date":"2018-01-22","permalink":"/n3xtchen/2018/01/22/scala-spark-dataframe-dataframeselect-multiple-columns-given-a-sequence-of-column-names/","section":"时间线","summary":"今天遇到个简单的错误，在这里与大家分享下。","title":"Scala Spark DataFrame: DataFrame.select 传入可变参数的方法"},{"content":"","date":"2018-01-09","permalink":"/n3xtchen/categories/ml/","section":"分类页","summary":"","title":"ml"},{"content":" 介绍 # Money makes the world go round（金钱驱动整个世界运转），无论你同不同意，有些时候你无法忽视它。在数字革命的时代，Data makes the world go round（数据驱动整个世界运转） 这句话更准确。事实上，无论数据的规模大小，数据已经成为企业、公司和组织的首要（first class）资产。一个智能系统，无论其复杂性，都需要强大的数据支撑。智能系统的核心，我拥有一个多个基于机器学习、深度学习或者统计方法的算法来消费数据，收集知识，并在一段时间内提供智能洞见（Insight）。算法本身很幼稚，不能适用于原始数据。因此，从原始数据中提取有意义的特征是非常重要的，特闷可以被算法理解并消费。\n标准机器学习任务管道 # 任何一个智能系统基本上都包含一个点对点的管道，从摄取原始数据，利用数据处理技术来清洗（wrangle），处理和工程化从这些数据中提取有意义的特征和属性。然后，我们时常利用如统计模型或者机器学习模型来模型化这些特征，然后根据需要解决的问题，部署该模型。如下展示的是是基于 CRISP-DM 工业处理模型标准的机器学习管道\n直接在原始数据之上建立模型是很蛮干的，因为我们不可能得到想要的结果和指标，算法也不会智能到自动从原始特征中提取有意义的特征（也存在一些自动特征提取技术，从深度学习方法论角度看，在一定程度上是可行的，但是更多应用在特征工程之后）。\n我们主要关注的领域属于 数据准备（data preparation ） 方面，如上图所示，我们使用各种方法论，以便在经过必要的 清洗（wrangling） 和 预处理（pre-processing） 之后从原始数据中提取有意义的属性或特征。\n动机 # 特征工程 是构建任何智能系统的重要组成部分。尽管你有很多更新的方法学，如 深度学习 和 元启发式（meta-heuristics） 有助于自动机器学习，但每个问题都是特定领域的，而更好的特性（适合问题）往往是系统性能的决定因素。特征工程 是一门艺术同样也是一门科学，这就是数据科学家在建模之前通常将 70％ 的时间用于数据准备阶段的原因。 我们来看看几位来自数据科学领域知名人士的特征工程相关评价。\n\u0026ldquo;Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.\u0026rdquo;\n\u0026ldquo;\u0026ldquo;解决特征问题是极其困难、耗时以及要求专业知识。应用机器学习（Applied machine learning） 基本上是特征工程。\u0026rdquo;\n—— 吴恩达\n他基本上强调了我们之前所提的 —— 数据科学家需要花接近 80% 的时间在特征工程上（很困难、耗时的过程，同时要求 **领域知识（Domain Knowledge）**和数学计算）。\n“Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.”\n“特征工程 是将 原始数据 转换成 特征 的处理过程，这些 特征 能更好讲 潜在问题 映射成 预测模型 ，提高 模型 在 看不见的数据 的预测精度”\n—— Dr. Jason Brownlee\n特征工程 就是将数据转换为特征的过程，作为机器学习模型的输入；高质量的特征有助于提高整体模型性能。特征非常依赖于潜在的问题。因此，尽管机器学习任务在不同场景下是相同的，像垃圾邮件分类器或者字迹识别，不同的场景下的特征提取将会是不同。\n来自华盛顿大学的 Pedro Domingos 教授（《终极算法》的作者）的标题为 “A Few Useful Things to Know about Machine Learning” 的 Paper 中提到：\n“At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.”\n“在最后，一些机器学习项目成功，而另一些失败了。什么导致他们之间的不同？最重要的因子是使用的特征”\n— Prof. Pedro Domingos\n让你对特征工程有所启发的最后引用来自著名的 Kaggler，Xavier Conort。你们大多数人已经知道，经常在Kaggle上发布艰难的现实世界中的机器学习问题，而这些问题通常对所有人开放。\n“The algorithms we used are very standard for Kagglers. …We spent most of our efforts in feature engineering. … We were also very careful to discard features likely to expose us to the risk of over-fitting our model.”\n“真的 Kaggler 使用的算法是非常标准的。……我们花了很大的努力在特征工程上……我们也很小心地丢弃容易造成模型过拟合的特征”\n— Xavier Conort\n理解特征 # 特征通常是在原始数据之上的特定表示，他是一个相对独立，可衡量的属性，通常由数据集的列来描述。考虑一个通用的二维数据集，每一个观察结果都由一行（Row）描述，每个特征由一列（Column）描述，这些列将具有特定的观测值。\n正如上述例子的表格，每一个行指定一个特征向量，而所有观测值的特征向量构成一个 2 维特征矩阵，被称为特征集。这个类似于呈现2维数据的数据框（data frame）或者电子表格（Spreadsheets）。典型的机器学习算法可用于数值矩阵或者 张量（tensor），因此大部分特征工程技术都是将原始数据转化成一些数值形式以便容易被算法理解。\n特征大体上可以分为两个大类：\n原始特征：直接从数据集中获取，而不用额外的数据操作或工程。 派生特征（Derived feature）：通常是通过特征工程获取，我们从已有的数据数据中提取出特征。 一个简单的例子就是从雇员数据集中包含生日的，通过和当前时间相减，创建出岁数的特征。\n数据的类型和格式也是各种各样，包括结构化和非结构化的。在这篇文章中，我们将集中讨论处理处理结构化数值数据的特诊工程策略。\n数值类型数据的特征工程 # 数字数据一般以标量（scalar）的形式描述观察，记录或者尺寸。这里，我们讲的数值类型数据就是**连续型数据（Continuous data）**而不是离散数据（它一般以分类形式来描述的）。数值型数据也可以被呈现为向量（Vector，向量中的每一个值或实体都表示一个特定的特征）。整型和浮点型是连续型数据最常见广泛使用的的数值类型。虽然数值可以直接喂给机器学习模型，但是你仍然在构建模型前，根据相关的场景、问题和领域进行特征工程。因此仍然有特征工程的需求。\n","date":"2018-01-09","permalink":"/n3xtchen/2018/01/09/understanding-feature-engineering-continuous/","section":"时间线","summary":"介绍 # Money makes the world go round（金钱驱动整个世界运转），无论你同不同意，有些时候你无法忽视它。在数字革命的时代，Data makes the world go round（数据驱动整个世界运转） 这句话更准确。事实上，无论数据的规模大小，数据已经成为企业、公司和组织的首要（first class）资产。一个智能系统，无论其复杂性，都需要强大的数据支撑。智能系统的核心，我拥有一个多个基于机器学习、深度学习或者统计方法的算法来消费数据，收集知识，并在一段时间内提供智能洞见（Insight）。算法本身很幼稚，不能适用于原始数据。因此，从原始数据中提取有意义的特征是非常重要的，特闷可以被算法理解并消费。","title":"理解特征工程 - 连续型数值特征"},{"content":"","date":"2017-10-22","permalink":"/n3xtchen/tags/blockchain/","section":"标签","summary":"","title":"blockchain"},{"content":"","date":"2017-10-22","permalink":"/n3xtchen/categories/new-tech/","section":"分类页","summary":"","title":"New-Tech"},{"content":"Ah，区块链。\n这可是一个很重要的东西。如果你完全不知道，那就要检讨下自己。\n她时常过于复杂，过于神秘，超奇异。（我不知道用什么词来正确的描述它，但是很多人都叫她 区块链（Blockchains）她们到底是什么？加入我们，在这里，我将进行简单粗暴讲解一番；我将尽力确保你离开这里之前，知道下面这个问题的答案：\n当人们谈论区块链时，他们都在说些什么？\n这可能会覆盖很多内容，本文将分成两个部分来一一讲解。\n第一个部分，了解下区块链的数据结构，有哪些属性，以及其他你需要理解的细节。\n第二个部分，应用你所学到的实现一个实际广泛的区块链应用，像强大的分布式账本(distributed ledgers)，加密算法（比如，比特币、莱特币或者像以太坊那样的智能合约(Smart-Contact)）。\n哈希(Hashes) # 讲区块链之前，我们需要理解一个概念。对于了解他们的工作原理来说，这个概念是至关重要的，绝对不是在浪费时间。\n这个概念就是哈希(Hash)。相关的操作就是哈希化(Hashing)，包含一个哈希(Hash)，Hash 函数和对应的算法。\n这里不会讲解哈希算法的工作原理，因为它们有很多，足够写好几本书了，而且具体实现对于了解区块链，帮助不大；我们只需要理解他的概念和作用就好。\n一个 Hash 算法是用来帮助把你的数据，无论长的还是短的，转化成相同长度的输出。具体的长度依赖于特定的算法，然而同一种算法都将永远产生相同长度的 Hash。\n来看一个具体的例子，SHA256，一个流行通用的哈希算法，看个例子：\nn3xtchen 生成 SHA256：\nfce22051b0d4aafd 4b1372bbdf014ea6 bb9627a48a0b20dd 1b30114db0b68736 小提示：SHA256 中的 256 代表实际的输出是 256 位（一个英文字符，一个字节，8位），我们一般习惯用十六进制（由 0-9 和 a-f 构成）来表示 Hash - 接下来我们都用它。\n一旦我们使用 SHA256 来执行 Hash 操作，“n3xthen” 永远都只会产生相同的输出；任何字符串都只会产生 64 个字符长度的 Hash。让我们一起看一下，超过 64 个字符的 SHA256 哈希后的结果：\nThis is a string that\u0026#39;s longer than 64 characters. The hash is still exactly 64. 生成 SHA256：\n8dee9e39a59cf0b7 0f10e8725792837a ac7dcf8c8729e4a6 bf38af8cb9725b09 结果仍然是 64个字符（256位）。\n对于任何 Hash 算法来说，原则都是一样。然而，对于好的 Hash 函数，还要满足以下原则：\n均匀分布（Uniformity）：我们希望任何我们输出的 Hash 在感官上是随机等概率的（或者接近）。也就是说，如果我们使用这个函数计算成千上万个 Hash，我们不应该从输出中观察到任何模式。\n由于它最小化了哈希任意两个输入产生同一个输出的风险，均匀分布是非常重要的一个特征。假定这种类型的意外冲突不可能发生的前提下，所有通过 Hash 交互的软件之间才能进行有效的沟通。\n不可逆（Non-invertibility）：根据算法的预期用途，是另一个重要原则。这应该是不可能，或者说非常低概率的。理想状态下，很容易把数据哈希化，但是反过来的话就几乎不可能了。\n不连续（discontinuity） ：要求算法对于类似的输入，应该产生差异很大的 Hash。当然也存在特定领域（例如，搜索相关的）的 Hash 算法要求连续的（与该原则相对立），但是大部分的 Hash 函数要求你尽可能不联系。‘abcdefh’ 和 ‘abcdefg’ 的 Hash 之间应该是完全不同，而不是仅仅一个或者几个字符不同。\n我的第一个区块链 # 区块链是一种数据结构。\n稍微具体，是一种有序排列。再具体点，使用一种很难被篡改方式来存储有序排列。\n最后一句话是最重要的——按顺序存储的数据结构有好多种 —— 数组（Array），链表（Linked List），双链表（Double-Linked List），等等。区块链就是在它们的基础上添加了一些特征，来确保如果列表的任一部分被篡改，整条链都会被判定成无效。它们基本上是个健壮的列表——没有任何魔法。\n一条区块链，就和其他基于列表的数据结构一样，有一个单元（一个区块）构成，它可以存储一个数据包，以及一些把区块合并在一起的机制。正如其名。我们把我们的数据打包到整齐的区块中，然后生成一条区块链。\n让我们一起造一个！首先，我们需要数据\n\u0026ldquo;我是一个区块!\u0026rdquo; \u0026ldquo;我是另一个区块！\u0026rdquo; “我是第三个” 。。。 实际放在区块链中的内容都是有特定价值的内容。它可以是一串字符，就和我们的例子一样；也可以是数千个交易记录的捆绑包，类似场景就是像比特币这样的密码币（cryptocurrency）。至于现在，你只要记住，我们不需要关心存储哪一种数据类型，只需要把它作为一种通用格式来存储。\n我们还需要一些关于区块的元数据。它的格式取决于特定区块链的目的，至于我们的例子，在每一个区块中，再包含一个区块数字。第一个区块是数字 1，第二个是 2，以此类推。\n{:.table-bordered}\n序号 内容 1 我是一个区块 2 我是另一个区块 3 我是第三个 \u0026hellip; \u0026hellip; 在这一点上，一个典型的动态列表结构将会添加一些信息，将多个区块的逻辑连接成一个序列。通常采用指针的形式，指向存储器中前一项和下一个项的位置。 我们会继续借鉴这个想法：\n{:.table-bordered}\n序号 内容 上一个序号 下一个序号 1 我是一个区块 - 2 2 我是另一个区块 1 3 3 我是第三个 2 4 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 我们现在有了完整功能的 双链表，它将很好地服务于很多应用。\n然而，区块链：每一个区块还存储前一个区块的 Hash。不连续和随机离散的 Hash 非常适合检查数据的完整性，因为如果输入数据哪怕一位变化，它产生的 Hash 也将明显不同。现在填充我们的 Hash，完成一个简单但功能完备的区块链。\n{:.table-bordered}\n序号 上一个区块的 Hash 内容 上一个序号 下一个序号 1 000000000000000\n000000000000000\n000000000000000\n000000000000000 我是一个区块 - 2 2 54ffe690a43c1950\n5ee598266fb9c7b4\nceca09d8ba19f4fc\na8b3ae4003d7b3ba 我是另一个区块 1 3 3 1cd6ec23adbf38de\n034058d7fa10d8a5\n4f0f01f2f791dbea\nd24d4ee2df854c10 我是第三个 2 4 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 这里还有一些东西需要指出。首先，第一个区块链很特别。它的**“上一个区块” ** Hash 全部都是 0。那是因为不存在上一个区块，因此我们没有东西可以验证。第一个区块时长被叫做创世纪块（Genesis Block ）—— 如果你感兴趣的话，你可以看一下比特币的 《创世纪区块》。那里有很多可能对你来说可能不会有实际意义，但是你可以看到**“上一个区块”** Hash 全部为 0，就像我们的一样！\n其次，要清楚的知道我们所哈希的数据。任一给定区块的**“上一个区块 Hash“** 都不仅仅只是上一个区块的字符数据。他实际上是上一个区块的全部数据的 哈希 —— 元数据，上一个 Hash 以及全部。\n为了证明这一点（我说过他很重要），看一下第二个区块。他的上一个区块 Hash 是 48b48…e76cb。花一点时间在之前的 区块链 寻找。\n然后，SHA256(”我是一个区块\u0026quot;)不等于 54ffe\u0026hellip;7b3ba\na9c3e4fd6b4b7a11 5137a56d76cf6ed6 b23559b2c1bc28a6 b7b9e649919a77be 为了生成 48b48…e76cb 这个 哈希，不仅需要字符数据本身，还需要按顺序组和所有的区块。 也就是说，我们把字符串 “1” + “00000000 \u0026hellip; 00000000” + “我是一个区块”，在这样做的时候，我们得到了你可以在链中看到的结果\n10000000000000000000000000000000000000000000000000000000000000000我是一个区块 54ffe690a43c1950 5ee598266fb9c7b4 ceca09d8ba19f4fc a8b3ae4003d7b3ba 作为简单数据建构进行操作一样，所有这些构成了一个 区块链 ！链接的区块都包含自己的数据，以及 上一个区块 的 哈希。\n区块链 的不同实现是通过使用大量的其他元数据，它们把所有数据都打包到它们区块中，这个是最基本的。在下一篇文章中，我们将进一步探讨如何通过在大型分布式网络中共享这些简单结构来解锁巨大的能力。\n目前，很多内容被引入的，因此，我们需要巩固下知识，让我们更多的思考一下我们把整合在一起的简单 区块链。 具体来说，正如我之前提到，我想要说明下如何正确地在链中包含的 哈希 值使得它能够防止篡改。\n区块链安全 # 让我们角色扮演下。我们在一个银行里工作，银行的作用就是把所有的消费记录都存储在 区块链 中。之所以这么决定是因为有人告诉他们 区块链 被验证出来，没有人不会因为淘气，可以篡改任意一条记录。人们有任何理由做这些事情 —— 把钱转到他们自己的账户上，犯罪目的欺诈别人。如果很容易办到，那就很糟糕了，\n我想要讲解清楚我们引入的 区块链 的数据结构如何帮助避免此类事情的发生。这里是我们之前使用的区块链，还是拿这个作为例子（从现在开始，为了节省空间，提升展示效率的，我把链表的指针去掉）：\n{:.table-bordered}\n序号 上一个区块的 Hash 内容 上一个序号 下一个序号 1 000000000000000\n000000000000000\n000000000000000\n000000000000000 小明支付小红 ¥10 - 2 2 804ba5fc063af7c5\n6aa6905352d0958a\na982bec4d8ff3541\n6669beef18af65b3 Evil 支付李雷 ¥7.5 1 3 3 ade381abcec55140\n1dade0dd4de0ccc2\n13b4c0108e35a9b6\n69e3056f4a44ee8e n3xtchen 支付 punk ¥1111 2 4 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 这些区块可能以某种分布式方式存储着 —— 一个区块一个文件，也就是说，你可以自信的认为一个潜在攻击者只能篡改链中的一个或者几个区块。具体实现没那么重要（如果这种自信有那么一点误导你，你就保持这种想法 —— 你迟早会适应密码币群体的）。\n你在银行中的工作就是偶尔验证这个交易记录日志，来确保它不会混乱。（或者写一个程序完成这个工作，如果你不想用笔和纸计算很多 哈希 的）现在看看具体的过程？\n从链条的开端开始。对于加入每一个区块，你需要计算前一个区块包含所有数据的 哈希，和当前存储的 上一个哈希 做对比。如果他们匹配，很好！哈希 计算很迅速的，因此对于你来说没有什么成本的。\n现在让我们幻想一种场景。我在银行中的工作就是罪犯，尝试盗窃金钱。我碰巧知道 Evil 是一个土匪，因为我们更新第二个区块，并据为己有。然而，我不能获取其他所有的区块，因此，我只能让他们保持原样。现在的链条看起来像：\n{:.table-bordered}\n序号 上一个区块的 Hash 内容 上一个序号 下一个序号 1 000000000000000\n000000000000000\n000000000000000\n000000000000000 小明支付小红 ¥10 - 2 2 804ba5fc063af7c5\n6aa6905352d0958a\na982bec4d8ff3541\n6669beef18af65b3 Evil 支付 Craker ¥70000000 1 3 3 ade381abcec55140\n1dade0dd4de0ccc2\n13b4c0108e35a9b6\n69e3056f4a44ee8e n3xtchen 支付 punk ¥1111 2 4 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 当你的下一个验证链的时候，我怎么寻找？区块 1检出，每次都是这么做的。区块链 2 检出 —— 它对于区块 1 来说，是正确的。这很奇怪，对吧？坏的的区块自己可以验证。然而，区块 3 就不能坚持了。你算出了，区块链 2：\n2804ba5fc063af7c56aa6905352d0958aa982bec4d8ff35416669beef18af65b3Evil 支付 Craker ¥70000000 586e4ccd12a406b7 a72383d401f51557 0fa1ad6448a8b9bc 9173479d2cb51593 Woah！区块 3 声称那不是他的 上一个哈希！实际上，这根本不行 —— 正常的哈希函数式不连续的。你现在知道链被篡改了。你需要，当你不知道，银行在这种情况下会怎么做 —— 从上一个备份还原链？\n想想都很可怕。\n对于我来说这是非常重要的，因为你明白每个块的 “前一个哈希” 是整个上一个块的哈希，而不仅仅是它的数据？为了证明这一点，只要进一步验证区块 4 就好了。如果你是好的，重新计算 区块 3 的 “前一个哈希” 来考虑 区块 2 的变化，你会发现 区块 4 的 “前一个哈希” 不能和 区块 3 匹配了！这似乎因为 区块 3 被改变了 —— “前一个哈希” 指的是 区块 2\n看看区块链的哈希机制如何让单一损害或者篡改的区块会导致整个链条失效的。很酷，对吧？如果每一个只有钱一个区块内容的 哈希, 也不是它的元数据，然后在我们的例子中，区块 3 将告诉我们有东西出错了，但是区块 3 将被检测出是好的。我希望你同意这是更弱的状态。\n现在……\n来做个总结吧！ # 在这个部分，我们已经学习了哈希（Hash）和区块链的基本结构；还展示了识别仅被修改一比特（only-a-bit-contrived ）区块链的例子，解释了区块链利用散列算法的完整性检查能力，拥有常规列表结构所不具备的安全性。\n下一次，我们将看看区块链如何在现实世界上产生巨大的效应（巨大的财务价值）。从加密货币始祖，比特币开始，我们将了解工作证明（proof-of-work）和分布式分类帐，以了解如何使用块链来实现功能分散的货币。接着，也是最后，我将介绍即将推出的更为未来的智能合约的基本概念，将概念概括为不仅仅是货币，如以太币。\n感谢你的阅读。\n","date":"2017-10-22","permalink":"/n3xtchen/2017/10/22/knowing-blockchain/","section":"时间线","summary":"Ah，区块链。","title":"这里有你想知道的区块链（Blockchains）- 第一部分"},{"content":"","date":"2017-10-21","permalink":"/n3xtchen/tags/scala/","section":"标签","summary":"","title":"scala"},{"content":"这一两年非常火的主题就是机器学习 - 与计算统计密切相关的跨学科领域，让我们的计算机学会不被明确编程下进行自动化工作。\n研究表明，在数据分析领域，机器学习被广泛的使用 - 从贷款风险评估到自动驾驶技术。\n在接下来的文章中，我将把 MLlib（Spark 的机器学习）介绍给大家。\n在阅读之前，还有一点很重要 - 本文旨在介绍库的使用，而不是机器学和统计学背后的概念和理论。因此，需要读者对这些主题有基本的概念；同时还需要 Spark 的基础知识。\n本文基于 Apache Spark 2.X 的 API，它引入了新的 DataFrame （和旧的 ** RDD** 有较大区别）。使用 DataFrame 的好处之一就是使用起来比 RDD 更简单，更友好。诚然， RDD 将仍然可以使用，但是已经处在维护模式（它将不再进行新功能开发；当 DataFrame 成熟足以取代 RDD，它将被弃用）。\nMLlib 介绍 # MLlib （Machine Learning Library 的缩写）是 Apache Spark 下的机器学习库，在解决机器学习问题方面提供极好的拓展性和易用性。目前，MLlib 使用 Breeze 解决线性代数问题。\n该库包含了很多的特性，我现在做个简短介绍。每一个特征都会在后续的章节中进行深入探讨。\n功能 # 算法 # 回归（Regression） 线性回归（Linear） 广义线性回归（Generalized Linear） 决策树（Decision Tree） 随机森林（Random Forest） 梯度提升树（Gradient-boosted Tree） Survival Isotonic 分类（Classification） 逻辑回归（Logistic，二分类和多酚类） 决策树（Decision Tree） 随机森林（Random Forest） 梯度提升树（Gradient-boosted Tree） 多层反馈（Multilayer Perceptron） 支持向量机（Linear support vector machine） One-vs-All 朴素贝叶斯（Naive Bayes） 聚类（Clustering） K-means 隐含狄利克雷分布（LDA） Bisecting K-means 高斯混合模型（Gaussian Mixture Model） 协同过滤（Collaborative Filtering） 特征工程（Featurization） # 特征提取 转换 降维（Dimensionality reduction） 筛选（Selection） 管道（Pipelines） # 组合管道（Composing Pipelines） 构建、评估和调优（Tuning）机器学习管道 持久化（Persistence） # 保存算法，模型和管道到持久化存储器，以备后续使用 从持久化存储器载入算法、模型和管道 实用工具（Utilities） # 线性代数（Linear algebra） 统计 数据处理 其他 DataFrame # 正如前面提到的，DataFrame 是 Spark 2.X 新引入的特性，用来取代旧的 RDD。DataFrame 是 Spark 的一种数据集（简单的说，一个分布式，强类型的数据集合，在 Spark 1.6 的时候接口初次被引入），由字段（Column，以变量形式呈现）组成。\n它的概念和 关系数据库 中的 表 或者 R/Python 的 DataFrame 一样，但是进行了一系列的优化。\n独特性 # 那么相较于 RDD， DataFrame 的主要卖点和优势是什么呢？\n​ 译自 https://blog.scalac.io/scala-spark-ml.html\n","date":"2017-10-21","permalink":"/n3xtchen/2017/10/21/use-spark-ml/","section":"时间线","summary":"这一两年非常火的主题就是机器学习 - 与计算统计密切相关的跨学科领域，让我们的计算机学会不被明确编程下进行自动化工作。","title":"Spark - 机器学习入门"},{"content":"","date":"2017-10-09","permalink":"/n3xtchen/tags/capybara/","section":"标签","summary":"","title":"capybara"},{"content":"","date":"2017-10-09","permalink":"/n3xtchen/tags/cucumber/","section":"标签","summary":"","title":"cucumber"},{"content":"","date":"2017-10-09","permalink":"/n3xtchen/categories/ruby/","section":"分类页","summary":"","title":"Ruby"},{"content":" 前言 # 我一般使用 Selenium Webdriver 进行 WEB 自动化测试。她是自动化测试领域知名的框架，拥有成熟的社区；当你遇到困难的的时候，你很容易找到解决方法。另一方面，我的一些朋友是 Ruby For Test 的大粉丝。我也同时也参加了一些 Ruby 自动化测试沙龙，了解 Ruby 的一些特性和能力。作为我的观点，Ruby 拥有简洁的语法，并且易于上手；你可以使用 Capybara 驱动 WEB 应用，RestClient 进行接口测试和 SitePrism 进行 POM (Page Object Model) UI 测试；掌握了这些库，你就可以轻轻松松开始自动化测试了。\n本文，我将讲解如何安装和设置 Ruby，Capybara 和 Cucumber。这是针对 Ruby 测试新手的入门教程。首先，我们先从定义开始。\n什么是 Capybara # 下面是 官方 的描述的非常清楚了：\nCapybara 是由 Ruby 编写的一个库，目的是为了简化模拟用户在应用中的交互。\nCapybara 提供统一简洁的接口来操作不同的驱动来执行你的测试。你可以无差别地选择 Selenium, Webkit 或 纯 Ruby drivers。\n用 Capybara 强大的同步功能来处理异步网页。Capybara 会自动等待你的内容出现在页面上，而不用进行手动的睡眠。\n什么是 Cucumber # Cucumber 是一个用于编写和执行软件功能描述。她支持 行为驱动开发（BDD）。它提供一种编写测试的方式：不受限于他们的技术背景，任何人都可以读懂。Cucumber（中文名：黄瓜） 理解的语言叫做 Gherkin（中文名：嫩黄瓜）。Cucumber 自己本身是使用 Ruby 实现的，但是她允许使用 Ruby 或者其他语言（不仅仅限于 Java、C# 和 Python）来编写测试。\n下面是 Gherkin 脚本：\nScenario: Filter the television list Given some different televisions in the TV listing page When I visit the TV listing page And I search for \u0026quot;Samsung\u0026quot; TVs Then I only see titles matching with Samsung TVs When I erase the search term Then I see all television brands again 安装和配置 # 前置条件： # Window （我的系统是 Win7） FireFox（由于新手，不建议折腾 IE） 安装和配置 Ruby 环境 # 对于 Windows 来说，安装 Ruby 最好的地方是 http://rubyinstaller.org/downloads/。打开网站，下载最新的 32 位 Ruby。\n在这里，我们将使用 Ruby 2.4.2 版本。下面是我勾选的安装选项，大家可以参考下。\n如果你习惯使用 Window 自带的命令行工具（cmd.exe 或 PowerShell），大家务必把 Add Ruby executables to your PATH. 这个选项选中；\n看到红色方框了没有？像 Capybara 和 Cucumber 这样的类库会依赖原生的 C 拓展，所以必须安装。\n关于 DevKit # 对于老用户来说，使用安装 DevKit 来解决安装原生拓展的问题，现在该项目已经不在维护了。从 Ruby 2.4 开始，我们使用 MYSYS2 编译 Ruby 来取代 Devkit；\n具体内容详见 Meet MSYS and the DevKit\n安装成功后，直接弹出命令行工具，内容如如下；你应该先选择 2，安装并更新 MinGW，因为 RubyInstaller 中的 MinGW 版本比较旧，不更新会导致后续安装失败。另外，你还可以一堆的 Gnome 工具（Linux 下常见的工具，如 Bash，sed 等等）\n_____ _ _____ _ _ _ ___ | __ \\ | | |_ _| | | | | | |__ \\ | |__) | _| |__ _ _ | | _ __ ___| |_ __ _| | | ___ _ __ ) | | _ / | | | '_ \\| | | | | | | '_ \\/ __| __/ _` | | |/ _ \\ '__/ / | | \\ \\ |_| | |_) | |_| |_| |_| | | \\__ \\ || (_| | | | __/ | / /_ |_| \\_\\__,_|_.__/ \\__, |_____|_| |_|___/\\__\\__,_|_|_|\\___|_||____| __/ | _ |___/ _|_ _ __ | | o __ _| _ _ | (_) | |^| | | |(_|(_)\\^/_\u0026gt; 1 - MSYS2 base installation 2 - MSYS2 system update 3 - MSYS2 and MINGW development toolchain Which components shall be installed? If unsure press ENTER [1,2,3] 安装过程中会弹出 MSYS2 安装向导，要你选择安装路径；这个路径要记住，环境变量的配置文件就存放在这里。在这里，我把 MSYS2 安装在 C://msys2 目录下面\n好久没倒腾 Windows，发现 mingw 居然自带了 ArchLinux 的包管理工具 pacman，好吃惊！\n安装完，会重新回到上述界面。这时，你选择 3，来安装开发工具链，比如 make、autoconf、automake 这些常用编译工具。\n最后，回车退出。\n目前为止，你已经安装你的 Ruby 开发环境。\nNote: 后续的步骤都在 msys2 中执行\n接下来我们验证下环境是否可用 # 执行 ruby -v 来验证 Ruby 安装。命令输出如下：\nTest@t-w7sp1eng-ie9 MINGW32 ~ $ ruby -v bash: ruby: command not found 尴尬的发现 MinGW 没把 Ruby 程序加到路径中。怎么办？打开 C:\\msys32\\home\\Test.bash_rc 文件，在文件末尾追加如下命令：\nexport PATH=/c/Ruby24/bin:$PATH 小常识：/c/Ruby24/bin，Posix Path，这是一种文件路径规范，Unix/Linux 都遵循这个规范，MinGW 会把这个格式转化成 Windows 的路径格式；如果希望深入了解，详见 MinGW-Posix Path Conversion。为了便于学习，我整理一个规律：\n如果遇到第一个 ／，你将第一个 ／ 去掉，并把第二个改成 : 其他的 ／ 一律转化成 \\ 再次执行命令验证下，将打印出你所安装详细版本：\nTest@t-w7sp1eng-ie9 MINGW32 ~ $ ruby -v ruby 2.4.2p198 (2017-09-14 revision 59899) [i386-mingw32] 执行 gem -v 来验证 RubyGems 安装，输出相应的版本号。RubyGems 是 Ruby 的包管理工具，应该包含在 Ruby 的标准安装。\nTest@t-w7sp1eng-ie9 MINGW32 ~ $ gem -v 2.6.13 验证 MSYS2-DEVKIT 是否安装正确\nTest@t-w7sp1eng-ie9 MINGW32 ~ $ gem install json --platform ruby \u0026lt;\u0026lt;\u0026lt; 命令在这里 Temporarily enhancing PATH for MSYS/MINGW... # 这里插句话，下面就是说明安装 JSON 库我时，我们编译了原生拓展（native extensions） Building native extensions. This could take a while... ... 安装自动化测试所需要的库 # 要安装的库如下:\nCucumber：技术白痴都看得懂的测试代码，用来指挥整个测试流程 Capybara：利用 webdriver 操纵浏览器，人性化的 DSL，使用起来更便捷 Selenium Webdriver：它才是真正操纵浏览器的工具 RSpec：结果验证工具，如断言和判定 复习完前面的内容后，开始安装。。。\n在你的 MSYS2 命令行中，执行如下命令\nTest@t-w7sp1eng-ie9 MINGW32 ~ $ gem install cucumber capybara selenium-webdriver rspec 为了安装这 4 个 gems（相当于软件包），我们总共安装了 29 个 gems(包括依赖)。\n验证 Cucumber 是否安装成功。如果出现如下信息，说明安装成功：\nTest@t-w7sp1eng-ie9 MINGW32 ~ $ cucumber help \u0026lt;\u0026lt;\u0026lt; 命令在这里 *** WARNING: You must use ANSICON 1.31 or higher (https://github.com/adoxa/ansicon/) to get coloured output on Windows No such file or directory - help. You can use `cucumber --init` to get started. 开始使用 Ruby、Capybara 和 Cucumber 开始编写自动化测试脚本 # 首先，我们需要创建一个 feature 目录，用于存放我们的所有的测试用例。\nFeature 文件应该使用 Gherkin（Given-When-Then，同样支持 i18n：） 语法来编写。Gherkin 是一门面向领域语言（DSL），允许你不用考虑具体的实现来描述你的软件行为。\n每一个场景（Scenario）有三个部分构成：Given，When 和 Then\nGiven：描述预设的条件 When：描述你发起的行为 Then：描述预期的结果 还可以使用 And 来衔接多个行为。例如：\nGiven I am on the product page And I am logged in Then I should see \u0026quot;Welcome!\u0026quot; And I should see \u0026quot;Personal Details\u0026quot; 这个例子中，第一个 And 扮演 Given，而第二个扮演 Then。\n进入 features 目录，创建一个 test.feature 文件。我们的测试用例如下：\n进入 www.google.com.hk 搜索 N3xt-Tech 博客 看到 N3xt-Tech 博客 的搜索内容 点击 N3xt-Tech 博客 链接 等待 10 秒 现在，我们使用 notepad++（或者其他文字编辑系统） 打开 test.feature 文件（注意：一定要使用 UTF-8 字符编码保存），使用 Gherkin 语法编写上面的测试用例。\nFeature: Find the Yahoo Website Scenario: Search a website in google Given I am on the Google homepage When I will search for \u0026quot;N3xt-Tech 博客\u0026quot; Then I should see \u0026quot;N3xt-Tech 博客\u0026quot; Then I will click the “N3xt-Tech 博客” link Then I will wait for 10 seconds 编写完，我们尝试运行下我们的测试：\ncucumber features\\test.feature 我们还没有定义好测试步骤。因此，我们运行测试后会得到上述结果\n这个就是 Cucumber 人性化的一个体现，你直接测试说明，她会帮助你生成测试代码模版，这是你只需要填充的测试逻辑就好了。\n首先在 features 目录中创建一个名为 step_definitions 文件。然后，创建一个名为 test_steps.rb 的脚本文件，把刚才测试结果的代码片段（上图中红色方框中的内容）粘贴进去。\n说明：Ruby 程序文件都是使用 .rb 后缀保存，所以后续看到 .rb 文件，里面就是 Ruby 程序了，\n在执行一遍命令，查看下输出：\n看红色方框的部分，对比上一个输出，之前场景（scenario）和测试步骤（steps）时都是未定义（undefined），现在都是 pending（待定）和 skipped 状态。\n代码中 pending 的部分就是我们后续要填充的测试逻辑。\n现在，我们可以编写测试步骤了。我现在将要教会你们如何一步步实现自动化测试的。\n开始前，现在先看看最终目录结构（标准的 Cucumber 目录结构）：\n. # C:\\\\msys2\\home\\test └── web_test # 项目目录 └── features # 测试用例目录 ├── step_definitions # 测试步骤具体实现 │ └── test_steps.rb ├── support │ └── env.rb └── test.feature # Gherkin 4 directories, 3 files 由于作为初学者教程，我将隐藏一些复杂的代码逻辑。\n我们在 features 目录中创建一个 support 目录（如何之前的目录结构中），然后创建 env.rb 文件，来初始化环境。env.rb 代码如下：\nrequire 'rubygems' require 'capybara' require 'capybara/dsl' require 'rspec' Capybara.run_server = false #设置默认的 Driver Capybara.default_driver = :selenium #设置默认的选择器 Capybara.default_selector = :css #同步相关设置 module Helpers def without_resynchronize page.driver.options[:resynchronize] = false yield page.driver.options[:resynchronize] = true end end World(Capybara::DSL, Helpers) 它主要用来配置后续我们可以在 Cucumber 使用 Capybara 方法，这里暂时不需要了解代码具体实现，在后续博客中将会一一解释。\n首先，我们需要访问 google.com.hk 站点。Capybara 提供 visit 方法来实现这个目的。在 Selenium ，我们可以使用 driver.get() 或 driver.navigate().to() 来完成这个动作。因此，我们应该添加如下代码来访问 google.com.hk:\nvisit 'http://www.google.com.uk' 经过上面的操作，我们已经到 google.com.hk 的页面上，我们要在搜索框中输入我们要查询的文本。如下所述，查询框的 id 是 lst-ib。\nCapybara 提供一个方法叫 fill_in，用于文本填充操作。我们可以使用如下代码实现这个操作。在 Selenium 中，我们可以使用 textElement.sendKeys(String) 方法:\nfill_in 'lst-ib', :with =\u0026gt; searchText click_on 'Google 搜索'\t# 点击搜索按钮 接着，我们需要在当前页面检索期待的查询结果。我们可以使用 page.should have_content 方法。在 Selenium 中，我们可以使用 JUnit，TestNG 或者 Hamcrest 断言:\npage.should have_content(expectedText) 现在，该点击 Next-Tech 博客 链接了。如下图所示，链接文本就是 Next-Tech 博客：\n在 Capybara 中，我们可以使用 click_link 来执行点击操作。在 Selenium 中，我们可以使用 driver.findElement(By.linkText(\u0026quot;Next-Tech 博客\u0026quot;)):\nclick_link('Next-Tech 博客') 最后一步了，我们将在 Next-Tech 博客 的站点上停留 10 秒，使用 sleep(10) 来实现。在 Selenium 中，我们使用 Thread.sleep(10);\n现在，我们把之前的代码都整合在一起。我们的 test_steps.rb 代码如下：\n# encoding: UTF-8 # 访问 google.com.hk Given(/^I am on the Google homepage$/) do visit 'https://www.google.co.uk/' end # 输入 \u0026quot;Next-Tech 博客\u0026quot; 到搜索框中 When(/^I will search for \u0026quot;([^\u0026quot;]*)\u0026quot;$/) do |searchText| fill_in 'lst-ib', :with =\u0026gt; searchText click_on 'Google 搜索' end # 在当前页面将看到 “Next-Tech 博客” Then(/^I should see \u0026quot;([^\u0026quot;]*)\u0026quot;$/) do |expectedText| page.should have_content(expectedText) end # 点击链接 Then(/^I will click the “([^\u0026quot;]*)” link$/) do |link| click_link(link) end # 等待 10 s Then(/^I will wait for (\\d+) seconds$/) do |waitTime| sleep (waitTime.to_i) end 我们已经可以开始我们完整的测试用例了。首先，进入你的项目目录，它包含如下所示的 features 目录，开始运行 Cucumber：\ncd ~/web_test cucumber feature\\test.feature 在运行时候，发现如下错误：\n这个错误（红色方框内）的主要意思就是 调用 FireFox 进行自动化测试时，Selenium 没找到相应的驱动来操纵浏览器，所以我们就按照错误中的提示，进行操作。\n下载 geckodriver（地址就是图中绿色下划线的部分，当前最新版本是 0.19.0），并解压到指定位置（我把它放到 C://geckodriver-v0.19.0-win32 中）：\n把刚刚下载的程序添加到 PATH 中。于是，我们再次打开 C:\\msys32\\home\\Test.bash_rc 文件，在该文件末尾追加如下命令：\nexport PATH=/c/geckodriver-v0.19.0-win32:$PATH 接着，重新运行 Cucumber，看看整个测试执行过程，^_^\n结语 # 用 Ruby 好多年了，写起来真心爽，主要用来做自动化测试和页面监控。不认真看，还以为你是在写作文，看看 Capybara 的封装方法，明显就是主谓宾结构嘛 ^_^，这样编程语言你不觉得酷吗？Cucumber 真的做到了 只要你识字，肯定看得懂用例，产品 🐶 也能过来对着代码指指点点了（终于可以更好融入了产品迭代中），自豪不自豪？这就是使用 Ruby 开发自动化测试的魅力所在。\n我常说：“如果当初可以选择，我希望我的第一门语言是 Ruby”。现在后生的语言多少都能看到 Ruby 的影子。\nHappy Programming！ Happy Testing！\n参考文献：\nRuby for Newbies: Testing Web Apps with Capybara and Cucumber Hassle-Free Start Guide for Ruby, Cucumber, and Capybara on Windows ","date":"2017-10-09","permalink":"/n3xtchen/2017/10/09/ruby-cucumber-and-capybara-on-window/","section":"时间线","summary":"前言 # 我一般使用 Selenium Webdriver 进行 WEB 自动化测试。她是自动化测试领域知名的框架，拥有成熟的社区；当你遇到困难的的时候，你很容易找到解决方法。另一方面，我的一些朋友是 Ruby For Test 的大粉丝。我也同时也参加了一些 Ruby 自动化测试沙龙，了解 Ruby 的一些特性和能力。作为我的观点，Ruby 拥有简洁的语法，并且易于上手；你可以使用 Capybara 驱动 WEB 应用，RestClient 进行接口测试和 SitePrism 进行 POM (Page Object Model) UI 测试；掌握了这些库，你就可以轻轻松松开始自动化测试了。","title":"Window 下 Cucumber 新人手册：使用 Ruby 和 Capybara 进行自动化测试"},{"content":"","date":"2017-07-14","permalink":"/n3xtchen/tags/bash/","section":"标签","summary":"","title":"bash"},{"content":"","date":"2017-07-14","permalink":"/n3xtchen/categories/bash/","section":"分类页","summary":"","title":"BASH"},{"content":"多年来，我发现肯多人使用下面的模型进行过滤映射（filter-map）:\nichexw $ [生成数据] | grep something | awk '{print $2}' 但是，可以缩写成：\nichexw $ [生成数据] | awk '/something/' 它隐含着打印匹配的正则表达式。\n为什么我喜欢这么做呢？\n有如下四个原因：\n更少的输入 更少的进程 awk 默认使用现代的正则表式，就像 grep -E 配合其他 awk 命令使用 “grep -v” 也是可以的 # awk 可以模拟 grep 的反向选择的，但是不是个好主意：\nichexw $ [生成数据] | awk '/something/ {next} 1' 这样子很丑 比 grep 命令长 这到底是干什么用的，完全看不懂 \u0026ndash; 这需要对 awk 命令更深入的了解. 当然，你还可以这么做：\nichexw $ [生成数据] | awk '! /something/' 这样看起来就好多了。\n引用自： SKIP grep, use AWK\n","date":"2017-07-14","permalink":"/n3xtchen/2017/07/14/skip-grep-use-awk/","section":"时间线","summary":"多年来，我发现肯多人使用下面的模型进行过滤映射（filter-map）:","title":"跳过 GREP，使用 AWK"},{"content":"为了演示不同类型的 ElasticSearch 的查询，我们将使用书文档信息的集合（有以下字段：title（标题）, authors（作者）, summary（摘要）, publish_date（发布日期）和 num_reviews（浏览数））。\n在这之前，首先我们应该先创建一个新的索引（index），并批量导入一些文档：\n创建索引：\nPUT /bookdb_index { \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 1 }} 批量上传文档：\nPOST /bookdb_index/book/_bulk { \u0026quot;index\u0026quot;: { \u0026quot;_id\u0026quot;: 1 }} { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;authors\u0026quot;: [\u0026quot;clinton gormley\u0026quot;, \u0026quot;zachary tong\u0026quot;], \u0026quot;summary\u0026quot; : \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;publish_date\u0026quot; : \u0026quot;2015-02-07\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;publisher\u0026quot;: \u0026quot;oreilly\u0026quot; } { \u0026quot;index\u0026quot;: { \u0026quot;_id\u0026quot;: 2 }} { \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;authors\u0026quot;: [\u0026quot;grant ingersoll\u0026quot;, \u0026quot;thomas morton\u0026quot;, \u0026quot;drew farris\u0026quot;], \u0026quot;summary\u0026quot; : \u0026quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization\u0026quot;, \u0026quot;publish_date\u0026quot; : \u0026quot;2013-01-24\u0026quot;, \u0026quot;num_reviews\u0026quot;: 12, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } { \u0026quot;index\u0026quot;: { \u0026quot;_id\u0026quot;: 3 }} { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;authors\u0026quot;: [\u0026quot;radu gheorge\u0026quot;, \u0026quot;matthew lee hinman\u0026quot;, \u0026quot;roy russo\u0026quot;], \u0026quot;summary\u0026quot; : \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;publish_date\u0026quot; : \u0026quot;2015-12-03\u0026quot;, \u0026quot;num_reviews\u0026quot;: 18, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } { \u0026quot;index\u0026quot;: { \u0026quot;_id\u0026quot;: 4 }} { \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;authors\u0026quot;: [\u0026quot;trey grainger\u0026quot;, \u0026quot;timothy potter\u0026quot;], \u0026quot;summary\u0026quot; : \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;publish_date\u0026quot; : \u0026quot;2014-04-05\u0026quot;, \u0026quot;num_reviews\u0026quot;: 23, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } 栗子： # 1. 基本的匹配（Query）查询 # 有两种方式来执行一个全文匹配查询：\n使用 Search Lite API，它从 url 中读取所有的查询参数 使用完整 JSON 作为请求体，这样你可以使用完整的 Elasticsearch DSL 下面是一个基本的匹配查询，查询任一字段包含 Guide 的记录\nGET /bookdb_index/book/_search?q=guide [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.28168046, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;authors\u0026quot;: [\u0026quot;clinton gormley\u0026quot;, \u0026quot;zachary tong\u0026quot;], \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.24144039, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;authors\u0026quot;: [\u0026quot;trey grainger\u0026quot;, \u0026quot;timothy potter\u0026quot;], \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot;, \u0026quot;num_reviews\u0026quot;: 23, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } } ] 下面是完整 Body 版本的查询，生成相同的内容：\n{ \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;guide\u0026quot;, \u0026quot;fields\u0026quot; : [\u0026quot;_all\u0026quot;] } } } multi_match 是 match 的作为在多个字段运行相同操作的一个速记法。fields 属性用来指定查询针对的字段，在这个例子中，我们想要对文档的所有字段进行匹配。两个 API 都允许你指定要查询的字段。例如，查询 title 字段中包含 in Action 的书：\nGET /bookdb_index/book/_search?q=title:in action [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.6259885, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;trey grainger\u0026quot;, \u0026quot;timothy potter\u0026quot; ], \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot;, \u0026quot;num_reviews\u0026quot;: 23, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.5975345, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;radu gheorge\u0026quot;, \u0026quot;matthew lee hinman\u0026quot;, \u0026quot;roy russo\u0026quot; ], \u0026quot;summary\u0026quot;: \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot;, \u0026quot;num_reviews\u0026quot;: 18, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } } ] 然而， 完整的 DSL 给予你灵活创建更复杂查询和指定返回结果的能力（后面，我们会一一阐述）。在下面例子中，我们指定 size 限定返回的结果条数，from 指定起始位子，_source 指定要返回的字段，以及语法高亮\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot; : { \u0026quot;title\u0026quot; : \u0026quot;in action\u0026quot; } }, \u0026quot;size\u0026quot;: 2, \u0026quot;from\u0026quot;: 0, \u0026quot;_source\u0026quot;: [ \u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot; ], \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot; : { \u0026quot;title\u0026quot; : {} } } } [Results] \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;max_score\u0026quot;: 0.9105287, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.9105287, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; }, \u0026quot;highlight\u0026quot;: { \u0026quot;title\u0026quot;: [ \u0026quot;Elasticsearch \u0026lt;em\u0026gt;in\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;Action\u0026lt;/em\u0026gt;\u0026quot; ] } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.9105287, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; }, \u0026quot;highlight\u0026quot;: { \u0026quot;title\u0026quot;: [ \u0026quot;Solr \u0026lt;em\u0026gt;in\u0026lt;/em\u0026gt; \u0026lt;em\u0026gt;Action\u0026lt;/em\u0026gt;\u0026quot; ] } } ] } 注意：对于多个词查询，match 允许指定是否使用 and 操作符来取代默认的 or 操作符。你还可以指定 mininum_should_match 选项来调整返回结果的相关程度。具体看后面的例子。\n2. 多字段（Multi-filed）查询 # 正如我们已经看到来的，为了根据多个字段检索（e.g. 在 title 和 summary 字段都是相同的查询字符串的结果），你可以使用 multi_match 语句\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;elasticsearch guide\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;] } } } [Results] \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 3, \u0026quot;max_score\u0026quot;: 0.9448582, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.9448582, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;clinton gormley\u0026quot;, \u0026quot;zachary tong\u0026quot; ], \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.17312013, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;radu gheorge\u0026quot;, \u0026quot;matthew lee hinman\u0026quot;, \u0026quot;roy russo\u0026quot; ], \u0026quot;summary\u0026quot;: \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot;, \u0026quot;num_reviews\u0026quot;: 18, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.14965448, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;trey grainger\u0026quot;, \u0026quot;timothy potter\u0026quot; ], \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot;, \u0026quot;num_reviews\u0026quot;: 23, \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } } ] } 注：第三条被匹配，因为 guide 在 summary 字段中被找到。\n3. Boosting # 由于我们是多个字段查询，我们可能需要提高某一个字段的分值。在下面的例子中，我们把 summary 字段的分数提高三倍，为了提升 summary 字段的重要度；因此，我们把文档 4 的相关度提高了。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;elasticsearch guide\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary^3\u0026quot;] } }, \u0026quot;_source\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.31495273, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.14965448, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.13094766, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; } } ] 注：提升不是简简单单通过提升因子把计算分数加成。实际的 boost 值通过归一化和一些内部优化给出的。相关信息请见 Elasticsearch guide\n4. Bool 查询 # 为了提供更相关或者特定的结果，AND/OR/NOT 操作符可以用来调整我们的查询。它是以 布尔查询 的方式来实现的。布尔查询 接受如下参数：\nmust 等同于 AND must_not 等同于 NOT should 等同于 OR 打比方，如果我想要查询这样类型的书：书名包含 ElasticSearch 或者（OR） Solr，并且（AND）它的作者是 Clinton Gormley 不是（NOT）Radu Gheorge\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: { \u0026quot;bool\u0026quot; : { \u0026quot;should\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch\u0026quot; }}, { \u0026quot;match\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Solr\u0026quot; }} ] } }, \u0026quot;must\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;authors\u0026quot;: \u0026quot;clinton gormely\u0026quot; }}, \u0026quot;must_not\u0026quot;: { \u0026quot;match\u0026quot;: {\u0026quot;authors\u0026quot;: \u0026quot;radu gheorge\u0026quot; }} } } } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.3672021, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;clinton gormley\u0026quot;, \u0026quot;zachary tong\u0026quot; ], \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;publisher\u0026quot;: \u0026quot;oreilly\u0026quot; } } ] 注：正如你所看到的，布尔查询 可以包装任何其他查询类型，包括其他布尔查询，以创建任意复杂或深度嵌套的查询。\n5. 模糊（Fuzzy）查询 # 在进行匹配和多项匹配时，可以启用模糊匹配来捕捉拼写错误，模糊度是基于原始单词的编辑距离来指定的。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;comprihensiv guide\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;], \u0026quot;fuzziness\u0026quot;: \u0026quot;AUTO\u0026quot; } }, \u0026quot;_source\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot;], \u0026quot;size\u0026quot;: 1 } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.5961596, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } } ] 注：当术语长度大于 5 个字符时，AUTO 的模糊值等同于指定值 “2”。但是，80％ 拼写错误的编辑距离为 1，所以，将模糊值设置为 1 可能会提高您的整体搜索性能。更多详细信息，请参阅Elasticsearch指南中的“排版和拼写错误”（Typos and Misspellings）。\n6. 通配符（Wildcard）查询 # 通配符查询 允许你指定匹配的模式，而不是整个术语。\n？ 匹配任何字符 * 匹配零个或多个字符。 例如，要查找名称以字母\u0026rsquo;t\u0026rsquo;开头的所有作者的记录：\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;wildcard\u0026quot; : { \u0026quot;authors\u0026quot; : \u0026quot;t*\u0026quot; } }, \u0026quot;_source\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;authors\u0026quot;], \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot; : { \u0026quot;authors\u0026quot; : {} } } } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;clinton gormley\u0026quot;, \u0026quot;zachary tong\u0026quot; ] }, \u0026quot;highlight\u0026quot;: { \u0026quot;authors\u0026quot;: [ \u0026quot;zachary \u0026lt;em\u0026gt;tong\u0026lt;/em\u0026gt;\u0026quot; ] } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;grant ingersoll\u0026quot;, \u0026quot;thomas morton\u0026quot;, \u0026quot;drew farris\u0026quot; ] }, \u0026quot;highlight\u0026quot;: { \u0026quot;authors\u0026quot;: [ \u0026quot;\u0026lt;em\u0026gt;thomas\u0026lt;/em\u0026gt; morton\u0026quot; ] } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;trey grainger\u0026quot;, \u0026quot;timothy potter\u0026quot; ] }, \u0026quot;highlight\u0026quot;: { \u0026quot;authors\u0026quot;: [ \u0026quot;\u0026lt;em\u0026gt;trey\u0026lt;/em\u0026gt; grainger\u0026quot;, \u0026quot;\u0026lt;em\u0026gt;timothy\u0026lt;/em\u0026gt; potter\u0026quot; ] } } ] 7. 正则（Regexp）查询 # 正则查询 让你可以使用比 通配符查询 更复杂的模式进行查询：\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;regexp\u0026quot; : { \u0026quot;authors\u0026quot; : \u0026quot;t[a-z]*y\u0026quot; } }, \u0026quot;_source\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;authors\u0026quot;], \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot; : { \u0026quot;authors\u0026quot; : {} } } } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;trey grainger\u0026quot;, \u0026quot;timothy potter\u0026quot; ] }, \u0026quot;highlight\u0026quot;: { \u0026quot;authors\u0026quot;: [ \u0026quot;\u0026lt;em\u0026gt;trey\u0026lt;/em\u0026gt; grainger\u0026quot;, \u0026quot;\u0026lt;em\u0026gt;timothy\u0026lt;/em\u0026gt; potter\u0026quot; ] } } ] 8. 短语匹配(Match Phrase)查询 # 短语匹配查询 要求在请求字符串中的所有查询项必须都在文档中存在，文中顺序也得和请求字符串一致，且彼此相连。默认情况下，查询项之间必须紧密相连，但可以设置 slop 值来指定查询项之间可以分隔多远的距离，结果仍将被当作一次成功的匹配。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot;: \u0026quot;search engine\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;], \u0026quot;type\u0026quot;: \u0026quot;phrase\u0026quot;, \u0026quot;slop\u0026quot;: 3 } }, \u0026quot;_source\u0026quot;: [ \u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot; ] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.22327082, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.16113183, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } } ] 注：在上述例子中，对于非整句类型的查询，_id 为 1 的文档一般会比 _id 为 4 的文档得分高，结果位置也更靠前，因为它的字段长度较短，但是对于 短语匹配类型 查询，由于查询项之间的接近程度是一个计算因素，因此 _id 为 4 的文档得分更高。\n9. 短语前缀（Match Phrase Prefix）查询 # 短语前缀式查询 能够进行 即时搜索（search-as-you-type） 类型的匹配，或者说提供一个查询时的初级自动补全功能，无需以任何方式准备你的数据。和 match_phrase 查询类似，它接收slop 参数（用来调整单词顺序和不太严格的相对位置）和 max_expansions 参数（用来限制查询项的数量，降低对资源需求的强度）。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase_prefix\u0026quot; : { \u0026quot;summary\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;search en\u0026quot;, \u0026quot;slop\u0026quot;: 3, \u0026quot;max_expansions\u0026quot;: 10 } } }, \u0026quot;_source\u0026quot;: [ \u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot; ] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.5161346, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.37248808, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } } ] 注：采用 查询时即时搜索 具有较大的性能成本。更好的解决方案是采用 索引时即时搜索。更多信息，请查看 自动补齐接口（Completion Suggester API） 或 边缘分词器（Edge-Ngram filters）的用法。\n10. 查询字符串（Query String） # 查询字符串 类型（query_string）的查询提供了一个方法，用简洁的简写语法来执行 多匹配查询、 布尔查询 、 提权查询、 模糊查询、 通配符查询、 正则查询 和范围查询。下面的例子中，我们在那些作者是 “grant ingersoll” 或 “tom morton” 的某本书当中，使用查询项 “search algorithm” 进行一次模糊查询，搜索全部字段，但给 summary 的权重提升 2 倍。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;query_string\u0026quot; : { \u0026quot;query\u0026quot;: \u0026quot;(saerch~1 algorithm~1) AND (grant ingersoll) OR (tom morton)\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;_all\u0026quot;, \u0026quot;summary^2\u0026quot;] } }, \u0026quot;_source\u0026quot;: [ \u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;authors\u0026quot; ], \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot; : { \u0026quot;summary\u0026quot; : {} } } } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 0.14558059, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;grant ingersoll\u0026quot;, \u0026quot;thomas morton\u0026quot;, \u0026quot;drew farris\u0026quot; ] }, \u0026quot;highlight\u0026quot;: { \u0026quot;summary\u0026quot;: [ \u0026quot;organize text using approaches such as full-text \u0026lt;em\u0026gt;search\u0026lt;/em\u0026gt;, proper name recognition, clustering, tagging, information extraction, and summarization\u0026quot; ] } } ] 11. 简单查询字符串（Simple Query String） # 简单请求字符串 类型（simple_query_string）的查询是请求字符串类型（query_string）查询的一个版本，它更适合那种仅暴露给用户一个简单搜索框的场景；因为它用 +/\\|/- 分别替换了 AND/OR/NOT，并且自动丢弃了请求中无效的部分，不会在用户出错时，抛出异常。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;simple_query_string\u0026quot; : { \u0026quot;query\u0026quot;: \u0026quot;(saerch~1 algorithm~1) + (grant ingersoll) | (tom morton)\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;_all\u0026quot;, \u0026quot;summary^2\u0026quot;] } }, \u0026quot;_source\u0026quot;: [ \u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;authors\u0026quot; ], \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot; : { \u0026quot;summary\u0026quot; : {} } } } 12. 词条（Term）/多词条（Terms）查询 # 以上例子均为 full-text(全文检索) 的示例。有时我们对结构化查询更感兴趣，希望得到更准确的匹配并返回结果，词条查询 和 多词条查询 可帮我们实现。在下面的例子中，我们要在索引中找到所有由 Manning 出版的图书。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot; : { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } }, \u0026quot;_source\u0026quot; : [\u0026quot;title\u0026quot;,\u0026quot;publish_date\u0026quot;,\u0026quot;publisher\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 1.2231436, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2013-01-24\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 1.2231436, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 1.2231436, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } } ] 可使用词条关键字来指定多个词条，将搜索项用数组传入。\n{ \u0026quot;query\u0026quot;: { \u0026quot;terms\u0026quot; : { \u0026quot;publisher\u0026quot;: [\u0026quot;oreilly\u0026quot;, \u0026quot;packt\u0026quot;] } } } 13. 词条（Term）查询 - 排序（Sorted） # 词条查询 的结果（和其他查询结果一样）可以被轻易排序，多级排序也被允许：\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;term\u0026quot; : { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot; } }, \u0026quot;_source\u0026quot; : [\u0026quot;title\u0026quot;,\u0026quot;publish_date\u0026quot;,\u0026quot;publisher\u0026quot;], \u0026quot;sort\u0026quot;: [ { \u0026quot;publish_date\u0026quot;: {\u0026quot;order\u0026quot;:\u0026quot;desc\u0026quot;}}, { \u0026quot;title\u0026quot;: { \u0026quot;order\u0026quot;: \u0026quot;desc\u0026quot; }} ] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: null, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; }, \u0026quot;sort\u0026quot;: [ 1449100800000, \u0026quot;in\u0026quot; ] }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: null, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; }, \u0026quot;sort\u0026quot;: [ 1396656000000, \u0026quot;solr\u0026quot; ] }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: null, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2013-01-24\u0026quot; }, \u0026quot;sort\u0026quot;: [ 1358985600000, \u0026quot;to\u0026quot; ] } ] 14. 范围查询 # 另一个结构化查询的例子是 范围查询。在这个例子中，我们要查找 2015 年出版的书。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot; : { \u0026quot;publish_date\u0026quot;: { \u0026quot;gte\u0026quot;: \u0026quot;2015-01-01\u0026quot;, \u0026quot;lte\u0026quot;: \u0026quot;2015-12-31\u0026quot; } } }, \u0026quot;_source\u0026quot; : [\u0026quot;title\u0026quot;,\u0026quot;publish_date\u0026quot;,\u0026quot;publisher\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;oreilly\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 1, \u0026quot;_source\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;manning\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; } } ] 注：范围查询 用于日期、数字和字符串类型的字段。\n15. 过滤(Filtered)查询 # 过滤查询允许你可以过滤查询结果。对于我们的例子中，要在标题或摘要中检索一些书，查询项为 Elasticsearch，但我们又想筛出那些仅有 20 个以上评论的。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;filtered\u0026quot;: { \u0026quot;query\u0026quot; : { \u0026quot;multi_match\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;elasticsearch\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;,\u0026quot;summary\u0026quot;] } }, \u0026quot;filter\u0026quot;: { \u0026quot;range\u0026quot; : { \u0026quot;num_reviews\u0026quot;: { \u0026quot;gte\u0026quot;: 20 } } } } }, \u0026quot;_source\u0026quot; : [\u0026quot;title\u0026quot;,\u0026quot;summary\u0026quot;,\u0026quot;publisher\u0026quot;, \u0026quot;num_reviews\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.5955761, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;publisher\u0026quot;: \u0026quot;oreilly\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot; } } ] 注：过滤查询 并不强制它作用于其上的查询必须存在。如果未指定查询，match_all 基本上会返回索引内的全部文档。实际上，过滤只在第一次运行，以减少所需的查询面积，并且，在第一次使用后过滤会被缓存，大大提高了性能。\n更新：过滤查询 将在 ElasticSearch 5 中移除，使用 布尔查询 替代。 下面有个例子使用 布尔查询 重写上面的例子：\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot; : { \u0026quot;multi_match\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;elasticsearch\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;,\u0026quot;summary\u0026quot;] } }, \u0026quot;filter\u0026quot;: { \u0026quot;range\u0026quot; : { \u0026quot;num_reviews\u0026quot;: { \u0026quot;gte\u0026quot;: 20 } } } } }, \u0026quot;_source\u0026quot; : [\u0026quot;title\u0026quot;,\u0026quot;summary\u0026quot;,\u0026quot;publisher\u0026quot;, \u0026quot;num_reviews\u0026quot;] } 在后续的例子中，我们将会把它使用在 多重过滤 中。\n16. 多重过滤（Multiple Filters） # 多重过滤 可以结合 布尔查询 使用，下一个例子中，过滤查询决定只返回那些包含至少20条评论，且必须在 2015 年前出版，且由 O\u0026rsquo;Reilly 出版的结果。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;filtered\u0026quot;: { \u0026quot;query\u0026quot; : { \u0026quot;multi_match\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;elasticsearch\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;,\u0026quot;summary\u0026quot;] } }, \u0026quot;filter\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: { \u0026quot;range\u0026quot; : { \u0026quot;num_reviews\u0026quot;: { \u0026quot;gte\u0026quot;: 20 } } }, \u0026quot;must_not\u0026quot;: { \u0026quot;range\u0026quot; : { \u0026quot;publish_date\u0026quot;: { \u0026quot;lte\u0026quot;: \u0026quot;2014-12-31\u0026quot; } } }, \u0026quot;should\u0026quot;: { \u0026quot;term\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;oreilly\u0026quot; } } } } } }, \u0026quot;_source\u0026quot; : [\u0026quot;title\u0026quot;,\u0026quot;summary\u0026quot;,\u0026quot;publisher\u0026quot;, \u0026quot;num_reviews\u0026quot;, \u0026quot;publish_date\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.5955761, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;publisher\u0026quot;: \u0026quot;oreilly\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } } ] 17. 作用分值: 域值（Field Value）因子 # 也许在某种情况下，你想把文档中的某个特定域作为计算相关性分值的一个因素，比较典型的场景是你想根据普及程度来提高一个文档的相关性。在我们的示例中，我们想把最受欢迎的书（基于评论数判断）的权重进行提高，可使用 field_value_factor 用以影响分值。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;function_score\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;search engine\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;] } }, \u0026quot;field_value_factor\u0026quot;: { \u0026quot;field\u0026quot; : \u0026quot;num_reviews\u0026quot;, \u0026quot;modifier\u0026quot;: \u0026quot;log1p\u0026quot;, \u0026quot;factor\u0026quot; : 2 } } }, \u0026quot;_source\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot;, \u0026quot;num_reviews\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.44831306, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.3718407, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;num_reviews\u0026quot;: 23, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.046479136, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;num_reviews\u0026quot;: 18, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 0.041432835, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization\u0026quot;, \u0026quot;num_reviews\u0026quot;: 12, \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2013-01-24\u0026quot; } } ] 注1: 我们可能刚运行了一个常规的 multi_match (多匹配)查询，并对 num_reviews 域进行了排序，这让我们失去了评估相关性分值的好处。\n注2: 有大量的附加参数可用来调整提升原始相关性分值效果的程度，比如 modifier, factor, boost_mode 等等，至于细节可在 Elasticsearch 指南中探索。\n18. 作用分值: 衰变（Decay）函数 # 假设不想使用域值做递增提升，而你有一个理想目标值，并希望用这个加权因子来对这个离你较远的目标值进行衰减。有个典型的用途是基于经纬度、价格或日期等数值域的提升。在如下的例子中，我们查找在2014年6月左右出版的，查询项是 search engines 的书。\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;function_score\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;search engine\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;] } }, \u0026quot;functions\u0026quot;: [ { \u0026quot;exp\u0026quot;: { \u0026quot;publish_date\u0026quot; : { \u0026quot;origin\u0026quot;: \u0026quot;2014-06-15\u0026quot;, \u0026quot;offset\u0026quot;: \u0026quot;7d\u0026quot;, \u0026quot;scale\u0026quot; : \u0026quot;30d\u0026quot; } } } ], \u0026quot;boost_mode\u0026quot; : \u0026quot;replace\u0026quot; } }, \u0026quot;_source\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot;, \u0026quot;num_reviews\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.27420625, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;num_reviews\u0026quot;: 23, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.005920768, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 0.000011564, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization\u0026quot;, \u0026quot;num_reviews\u0026quot;: 12, \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2013-01-24\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.0000059171475, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;num_reviews\u0026quot;: 18, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; } } ] 19. 函数分值: 脚本评分 # 当内置的评分函数无法满足你的需求时，还可以用 Groovy 脚本。在我们的例子中，想要指定一个脚本，能在决定把 num_reviews 的因子计算多少之前，先将 publish_date 考虑在内。因为很新的书也许不会有评论，分值不应该被惩罚。\n评分脚本如下：\npublish_date = doc['publish_date'].value num_reviews = doc['num_reviews'].value if (publish_date \u0026gt; Date.parse('yyyy-MM-dd', threshold).getTime()) { my_score = Math.log(2.5 + num_reviews) } else { my_score = Math.log(1 + num_reviews) } return my_score 在 script_score 参数内动态调用评分脚本：\nPOST /bookdb_index/book/_search { \u0026quot;query\u0026quot;: { \u0026quot;function_score\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;search engine\u0026quot;, \u0026quot;fields\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;] } }, \u0026quot;functions\u0026quot;: [ { \u0026quot;script_score\u0026quot;: { \u0026quot;params\u0026quot; : { \u0026quot;threshold\u0026quot;: \u0026quot;2015-07-30\u0026quot; }, \u0026quot;script\u0026quot;: \u0026quot;publish_date = doc['publish_date'].value; num_reviews = doc['num_reviews'].value; if (publish_date \u0026gt; Date.parse('yyyy-MM-dd', threshold).getTime()) { return log(2.5 + num_reviews) }; return log(1 + num_reviews);\u0026quot; } } ] } }, \u0026quot;_source\u0026quot;: [\u0026quot;title\u0026quot;, \u0026quot;summary\u0026quot;, \u0026quot;publish_date\u0026quot;, \u0026quot;num_reviews\u0026quot;] } [Results] \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 4, \u0026quot;max_score\u0026quot;: 0.8463001, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.8463001, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;A distibuted real-time search and analytics engine\u0026quot;, \u0026quot;num_reviews\u0026quot;: 20, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch: The Definitive Guide\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-02-07\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.7067348, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;Comprehensive guide to implementing a scalable search engine using Apache Solr\u0026quot;, \u0026quot;num_reviews\u0026quot;: 23, \u0026quot;title\u0026quot;: \u0026quot;Solr in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2014-04-05\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.08952084, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms\u0026quot;, \u0026quot;num_reviews\u0026quot;: 18, \u0026quot;title\u0026quot;: \u0026quot;Elasticsearch in Action\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2015-12-03\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;bookdb_index\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;book\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 0.07602123, \u0026quot;_source\u0026quot;: { \u0026quot;summary\u0026quot;: \u0026quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization\u0026quot;, \u0026quot;num_reviews\u0026quot;: 12, \u0026quot;title\u0026quot;: \u0026quot;Taming Text: How to Find, Organize, and Manipulate It\u0026quot;, \u0026quot;publish_date\u0026quot;: \u0026quot;2013-01-24\u0026quot; } } ] } 注1: 要在 Elasticsearch 实例中使用动态脚本，必须在 config/elasticsearch.yaml 文件中启用它；也可以使用存储在 Elasticsearch 服务器上的脚本。建议看看 Elasticsearch 指南文档获取更多信息。\n注2: 因 JSON 不能包含嵌入式换行符，请使用分号来分割语句。\n引用自： 23 USEFUL ELASTICSEARCH EXAMPLE QUERIES\n","date":"2017-07-05","permalink":"/n3xtchen/elasticsearch/2017/07/05/elasticsearch-23-useful-query-example/","section":"时间线","summary":"为了演示不同类型的 ElasticSearch 的查询，我们将使用书文档信息的集合（有以下字段：title（标题）, authors（作者）, summary（摘要）, publish_date（发布日期）和 num_reviews（浏览数））。","title":"19 个很有用的 ElasticSearch 查询语句"},{"content":"","date":"2017-07-05","permalink":"/n3xtchen/tags/elasticsearch/","section":"标签","summary":"","title":"elasticsearch"},{"content":"","date":"2017-07-05","permalink":"/n3xtchen/categories/elasticsearch/","section":"分类页","summary":"","title":"elasticsearch"},{"content":"","date":"2017-06-07","permalink":"/n3xtchen/tags/linux/","section":"标签","summary":"","title":"linux"},{"content":"","date":"2017-06-07","permalink":"/n3xtchen/tags/vim/","section":"标签","summary":"","title":"vim"},{"content":"","date":"2017-06-07","permalink":"/n3xtchen/categories/vim/","section":"分类页","summary":"","title":"Vim"},{"content":"","date":"2017-06-07","permalink":"/n3xtchen/2017/06/07/vim-80-ubuntu-1704/","section":"时间线","summary":"","title":"vim 8.0 - ubuntu 17.04 踩坑记录"},{"content":"","date":"2017-06-05","permalink":"/n3xtchen/tags/git/","section":"标签","summary":"","title":"git"},{"content":"","date":"2017-06-05","permalink":"/n3xtchen/categories/git/","section":"分类页","summary":"","title":"Git"},{"content":" 如果你想从 github/bucket 迁移到私有的 gitlab，或者反过来 你的 git 地址域名变更了 你的项目名称变更了 并且你的代码库已经比较巨大，不想重新全量 pull 代码，那这个教程就适合你。\n查看现在代码中的远端仓库地址 # ichexw → git remote -v origin\thttp://old/git/repo.git (fetch) origin\thttp://old/git/repo.git (push) 但是，你的远端（Remote）地址变成成 http://new/git，怎么办？\n一条命令解决\n修改语段仓库地址 # ichexw → git remote set-url origin http://new/git/repo.git 验证下：\nichexw → git remote -v origin\thttp://new/git/repo.git (fetch) origin\thttp://new/git/repo.git (push) 你已经成功迁移到新的代码库了，可以愉快的 pull 和 push 了\n","date":"2017-06-05","permalink":"/n3xtchen/2017/06/05/git-update-remote/","section":"时间线","summary":"如果你想从 github/bucket 迁移到私有的 gitlab，或者反过来 你的 git 地址域名变更了 你的项目名称变更了 并且你的代码库已经比较巨大，不想重新全量 pull 代码，那这个教程就适合你。","title":"Git 修改远端仓库地址"},{"content":"","date":"2017-05-17","permalink":"/n3xtchen/categories/data_analytics/","section":"分类页","summary":"","title":"data_analytics"},{"content":"","date":"2017-05-17","permalink":"/n3xtchen/tags/java/","section":"标签","summary":"","title":"java"},{"content":"","date":"2017-05-17","permalink":"/n3xtchen/tags/python/","section":"标签","summary":"","title":"python"},{"content":" 目录结构 # ├── LICENSE ├── Makefile \u0026lt;- 流程工具 ├── README.md \u0026lt;- 开发者使用的顶级文档 │ ├── data │ ├── external \u0026lt;- 来自第三方的数据 │ ├── interim \u0026lt;- 被转化过的中间结果 │ ├── processed \u0026lt;- 最后用于模型的数据集 │ └── raw \u0026lt;- 原始不可变的数据输出 │ ├── docs \u0026lt;- 默认是 Sphinx 项目 │ ├── models \u0026lt;- 训练和序列化模型，模型预测或者模型总结 │ ├── notebooks \u0026lt;- Jupyter notebooks. │ 命名规则：数字，创建者首字母和描述，并以 `-` 隔开 │ e.g. `1.0-cw-initial-data-exploration`. │ ├── references \u0026lt;- 数据字典，手册以及所有其他探索资料 │ ├── reports \u0026lt;- Generated analysis as HTML, PDF, LaTeX, etc. │ └── figures \u0026lt;- 存放用于报表的图表文件 │ ├── requirements.txt \u0026lt;- 依赖包文件 ├── src \u0026lt;- 存储源码的目录 │ ├── __init__.py \u0026lt;- 使 src 编程一个 python 模块 │ ├── data \u0026lt;- 下载或生成数据的脚本 │ │ └── make_dataset.py │ │ │ ├── features \u0026lt;- 将原始数据转化成模型特征的脚本 │ │ └── build_features.py │ │ │ ├── models \u0026lt;- 训练模型的脚本 │ │ ├── predict_model.py │ │ └── train_model.py │ │ │ └── visualization \u0026lt;- 创建探索的结果可视化脚本 │ └── visualize.py │ └── tox.ini \u0026lt;- 用于运行 tox 的配置文件 数据是不可变的 # 不要编辑你的原始数据，不要手动，更不要存在 excel 中。不要覆盖原始数据。不要保存多个版本的原始数据。保持原始数据不可变。你写的代码应该通过管道把原始数据导向最终分析。你不用每次为了一些图标而运行所有步骤，当时任何人都可以只通过 src 和 data/raw 中的数据，重新生成最后的产品。\n然而，如果数据不可变，它同样不需要保存在版本控制中。因此，默认，数据目录应该包含一个 .gitignore 文件。如果你的数据量小，而且很少变化，你也许可以把它放到版本控制了。如果一个文件超过 50M，github 会警告，拒绝超过 100M 的文件提交。还有其他保存／同步数据的选择，例如 AWS S3（s3cmd）,GLFS,Git Annex 和 dat。目前，我们使用 S3 bucket 和 使用 aws cli 来从服务器同步数据。\n引用自 Cookiecutter Data Science\n","date":"2017-05-17","permalink":"/n3xtchen/2017/05/17/data-science-project-rule/","section":"时间线","summary":"目录结构 # ├── LICENSE ├── Makefile \u0026lt;- 流程工具 ├── README.","title":"数据挖掘-探讨工程项目结构"},{"content":"我尝试打开一个不是从 App Store 下载的应用，系统会报错：\n“App” is damaged and can’t be opened. You should move it to the Trash. 解决方法：\n打开终端：\n$ sudo spctl --master-disable ","date":"2017-04-24","permalink":"/n3xtchen/2017/04/24/os-x-app-is-damaged-and-cant-be-opened/","section":"时间线","summary":"我尝试打开一个不是从 App Store 下载的应用，系统会报错：","title":"OS X 下安装未验证第三方开发的APP 方法"},{"content":"","date":"2017-04-24","permalink":"/n3xtchen/tags/osx/","section":"标签","summary":"","title":"osX"},{"content":"","date":"2017-04-15","permalink":"/n3xtchen/tags/docker/","section":"标签","summary":"","title":"docker"},{"content":"","date":"2017-04-15","permalink":"/n3xtchen/categories/docker/","section":"分类页","summary":"","title":"docker"},{"content":"CMD 和 ENTRYPOINT 指令在运行容器时决定哪些命令将被执行。下面是它们几条共同合作的规则：\nDockerfile 应该至少指定一个 CMD 或者 ENTRYPOINT 命令。 ENTRYPOINT 应该在使用可执行容器时被指定。 CMD 应该作为 ENTRYPOINT 命令定义默认参数或者运行即席命令的一种方式。 CMD 在容器作为可替换参数时会被替换。 下面例子将会演示上述的规则：\n没有 ENTRYPOINT # 带方括号（[]）的是执行形式（exec form，推荐使用），如果不带方括号则是shell形式（就会在命令的前面加上 /bin/sh -c ）。\n命令 说明 No CMD 错误，不允许 CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd ENTRYPOINT exec_entry p1_entry（shell 形式） # 第三个命令就是使用 CMD 为 ENTRYPOINT 设置默认参数（CMD 中都只是参数而不是可执行程序）。\n命令 说明 No CMD /bin/sh -c exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] /bin/sh -c exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] /bin/sh -c exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd ENTRYPOINT [“exec_entry”, “p1_entry”]（执行形式） # 命令 说明 No CMD exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd Over！ # 引用自： What is the difference between CMD and ENTRYPOINT in a Dockerfile?\n","date":"2017-04-15","permalink":"/n3xtchen/2017/04/15/docker-entrypoint--cmd/","section":"时间线","summary":"CMD 和 ENTRYPOINT 指令在运行容器时决定哪些命令将被执行。下面是它们几条共同合作的规则：","title":"Docker 指令之 ENTRYPOINT 和 CMD"},{"content":"","date":"2017-04-05","permalink":"/n3xtchen/tags/metaclass/","section":"标签","summary":"","title":"metaclass"},{"content":"今天写代码，遇到了莫名其妙的错误：\nscala\u0026gt; def list2Arr[A](list: List[A]) = list.toArray \u0026lt;console\u0026gt;:21: error: No ClassTag available for A def list2Arr[A](list: List[A]) = list.toArray ^ 于是，我 Google 了下，大致的方案下面这样\nscala\u0026gt; import scala.reflect.ClassTag import scala.reflect.ClassTag\tscala\u0026gt; def list2Arr[A: ClassTag](list: List[A]) = list.toArray list2Arr: [A](list: List[A])(implicit evidence$1: scala.reflect.ClassTag[A])Array[A] ClassTag[T] 保存了被泛型擦除后的原始类型 T，在运行可以正确取到。\n简单分析了下错误：\nscala\u0026gt; List(1).toArray def toArray[B \u0026gt;: Int](implicit evidence$1: scala.reflect.ClassTag[B]): Array[B] toArray 定义的形变（Variance，[B \u0026gt;: Int]）运行时，类型在传递过程中丢失了自身的特性；第一段代码，在运行的使用范型是，A 就是传递的类型，而它运行被擦除了，导致后续的函数无法获取：\n[B \u0026gt;: A] 参考文献：\nScala型变 Scala中Manifest、ClassTag、TypeTag的学习 Overcoming type erasure in Scala scala的类与类型 TypeTags and Manifests Scala：什么是TypeTag,我如何使用它？ Scala中TypeTags和Manifests的用法 scala类型系统：19) Manifest与TypeTag Scala 的那些奇怪的符号 （一）：“\u0026lt;:” 和 “\u0026gt;:” 作用及用法 ","date":"2017-04-05","permalink":"/n3xtchen/2017/04/05/scala-typetag--classtag/","section":"时间线","summary":"今天写代码，遇到了莫名其妙的错误：","title":"Scala ERROR: No ClassTag available for A"},{"content":"","date":"2017-03-31","permalink":"/n3xtchen/tags/kafka/","section":"标签","summary":"","title":"kafka"},{"content":"","date":"2017-03-31","permalink":"/n3xtchen/tags/pgsql/","section":"标签","summary":"","title":"pgsql"},{"content":" Apache Kafka 和 Postgres: 处理事务和报表能力 # Apache Kafka 是目前主流的分布式流处理平台，用于数据处理和信息一致保证。她允许你集中数据流，完成多种目的。我突然对 Mozilla 的数据管道 实现感兴趣，尤其是其中展示了 Kafka 作为流的入口。\nPostgres Bottled Water 是另一种方式。在这个场景下，Postgres 实例作为生产者，Broker 接受流，并定向到其他平台。她的优势就是拥有结合先进的 SQL 特性 的 Postgres 的 ACID 能力。将它作为一个拓展，还可以和其他特性一起运行。\nPostgres 9.6 的 COPY 工具（ 详见文档）还可以执行命令行来操作数据IO，这样就可以消费和生产数据给 Broker。\n开始之前的准备 # 测试环境的相关参数：\n系统： Ubuntu 16.04.2 LTS JAVA 版本：openjdk version \u0026ldquo;1.8.0_121\u0026rdquo;, 下面提供 Ubuntu/Debian 下的安装方法: apt-get -y install default-jre default-jdk Kafka 版本：kafka 3.2（confluent 官方，scala：2.11） 安装教程详见 [基础的 Kafka 操作](#基础的 Kafka 操作) kafkacat 和 librdkafka # kafkacat 是由 librdkafka 库的作者开发的另一个工具，功能用一句话概括：像 cat 命令那样在 Kafka 的 Broker 中生产和消费数据。\nUbuntu/Debian 安装命令如下：\nichexw$ sudo apt-get install kafkacat 生成数据到 Kafka Broker # 造模拟数据发送给 Kafka 的 Broker；Bash 脚本如下：\n# 随机文本 function randtext() { cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1 } while (true); do # 每 10s 升成 50 个随机序列 for i in $(seq 1 50) do echo \u0026quot;$(uuidgen);$(randtext)\u0026quot; done | kafkacat -P -b localhost:9092 -qe -K ';' -t PGSHARD sleep 10 done 温馨小提示：如果 uuidgen 在你环境下找不到，你需要自行安装 uuid-runtime，这里提供 Ubuntu/Debian 下的安装方法:\nichexw$ apt-get install uuid-runtime\nkafkacat 使用的参数：\n-P: 生产者模式；对应的 -C，就是消费者模式 -b \u0026lt;brokers,..\u0026gt;: 这个就是 Broker 的地址 -qe: 两条命令合并 -q 静默状态 -e 最后一条发送成功后推出 -K \u0026lt;delim\u0026gt;: 定义了界定符 -t \u0026lt;topic\u0026gt;: 定义想要把数据发送的 Topic。 默认，这个 Topic 已经创建了 3 个分区（0-2），允许我们并行从不同的频道消费数据。\n生产数据给 Broker 时，Keys 不是强制要求的；并不是每一个场景都需要，你可以无视它。\n在Postgres实例中使用和生成 # 通常语法和下面差不多：\nkafka_db=# COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -o beginning -c100 -qeJ -t PGSHARD -p 0 | awk ''{print \u0026quot;P0\\t\u0026quot; $0 }'''; awk 不是被强制要求的，它只是为了展示该功能的灵活。\nkafkacat 使用的参数：\n-J: 输出将会被打印成 json 格式，包含所有的消息信息，包括分区，键值和信息。 -o \u0026lt;offset\u0026gt;: 提取数据的提取偏移量 常量：beginning 从头开始；end 从尾部开始；stored 后面会接受 整型: 绝对位置 -整型: 从尾部开始相对位置 -c \u0026lt;cnt\u0026gt;: 将会限制数据的行数。COPY 命令也是具有事务的，这意味着处理的数据行数越多，事务越巨大，提交的时间也会受影响。 Postgres 的 COPY 命令：\nCOPY table_name [ ( column_name [, ...] ) ] FROM { 'filename' | PROGRAM 'command' | STDIN } 命令解释：\nfilename: 要导入的文件名，直接从文件导入数据 command: 命令，从读取命令的输出定向给数据表 STDIN: 输入 增量消费主题 # 从头开始消费 Topic 分区，并设置 100 个文档的限制：\nichexw$ psql -Upostgres kafka_db \u0026lt;\u0026lt;EOF COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD -o beginning -p 0 | awk ''{print \u0026quot;P0\\t\u0026quot; \\$0 }'' '; COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD -o beginning -p 1 | awk ''{print \u0026quot;P1\\t\u0026quot; \\$0 }'' '; COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD -o beginning -p 2 | awk ''{print \u0026quot;P2\\t\u0026quot; \\$0 }'' '; EOF 然后使用 stored，为了每次只消费在所在分区未使用过的数据：\nichexw$ psql -Un3xtchen kafka_db \u0026lt;\u0026lt;EOF COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD -o stored -p 0 | awk ''{print \u0026quot;P0\\t\u0026quot; \\$0 }'' '; COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD -o stored -p 1 | awk ''{print \u0026quot;P1\\t\u0026quot; \\$0 }'' '; COPY main(group_id,payload) FROM PROGRAM 'kafkacat -C -b localhost:9092 -c100 -qeJ -t PGSHARD -o stored -p 2 | awk ''{print \u0026quot;P2\\t\u0026quot; \\$0 }'' '; EOF 每一个 COPY 命令都是并行执行的，使得这种方式足够灵活，易于拓展到集群。\n注意：并不是绝对的一致性，一旦偏移量被消费了，将在 Broker 被标记；如果在 Postgres 端事务失败会导致潜在的数据丢失。\n在 Postgres 实例中生成消息 # 同样的方式也可以消费数据，用法和生产数据给 Broker 一样。这使得通过从 Broker 消费原始数据进行微聚合时，超级有用。\n下面的例子展示了如何使用超简单的聚合查询，并以 json 格式回吐给 Broker：\nkafka_db=# COPY (select row_to_json(row(now() ,group_id , count(*))) from main group by group_id) TO PROGRAM 'kafkacat -P -b localhost:9092 -qe -t AGGREGATIONS'; COPY 3 如果你有一堆的服务器，想要通过 key 查询主体内容，你可以这么做：\nkafka_db=# COPY (select inet_server_addr() || ';', row_to_json(row(now() ,group_id , count(*))) from main group by group_id) TO PROGRAM 'kafkacat -P -K '';'' -b localhost:9092 -qe -t AGGREGATIONS'; 现在查看信息：\nichexw$ kafkacat -C -b localhost:9092 -qeJ -t AGGREGATIONS -o beginning 注意看输出的区别\n不带 key 的：\n{\u0026quot;topic\u0026quot;:\u0026quot;AGGREGATIONS\u0026quot;,\u0026quot;partition\u0026quot;:0,\u0026quot;offset\u0026quot;:0,\u0026quot;key\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;payload\u0026quot;:\u0026quot;{\\\u0026quot;f1\\\u0026quot;:\\\u0026quot;2017-04-05T07:50:52.148631+00:00\\\u0026quot;,\\\u0026quot;f2\\\u0026quot;:\\\u0026quot;P0\\\u0026quot;,\\\u0026quot;f3\\\u0026quot;:100}\u0026quot;} {\u0026quot;topic\u0026quot;:\u0026quot;AGGREGATIONS\u0026quot;,\u0026quot;partition\u0026quot;:0,\u0026quot;offset\u0026quot;:1,\u0026quot;key\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;payload\u0026quot;:\u0026quot;\\\\N\\t{\\\u0026quot;f1\\\u0026quot;:\\\u0026quot;2017-04-05T07:51:10.004637+00:00\\\u0026quot;,\\\u0026quot;f2\\\u0026quot;:\\\u0026quot;P0\\\u0026quot;,\\\u0026quot;f3\\\u0026quot;:100}\u0026quot;} 带 key 的：\n{\u0026quot;topic\u0026quot;:\u0026quot;AGGREGATIONS\u0026quot;,\u0026quot;partition\u0026quot;:0,\u0026quot;offset\u0026quot;:0,\u0026quot;key\u0026quot;:\u0026quot;127.0.0.1/32\u0026quot;,\u0026quot;payload\u0026quot;:\u0026quot;{\\\u0026quot;f1\\\u0026quot;:\\\u0026quot;2017-04-05T07:50:52.148631+00:00\\\u0026quot;,\\\u0026quot;f2\\\u0026quot;:\\\u0026quot;P0\\\u0026quot;,\\\u0026quot;f3\\\u0026quot;:100}\u0026quot;} {\u0026quot;topic\u0026quot;:\u0026quot;AGGREGATIONS\u0026quot;,\u0026quot;partition\u0026quot;:0,\u0026quot;offset\u0026quot;:1,\u0026quot;key\u0026quot;:\u0026quot;127.0.0.1/32\u0026quot;,\u0026quot;payload\u0026quot;:\u0026quot;\\\\N\\t{\\\u0026quot;f1\\\u0026quot;:\\\u0026quot;2017-04-05T07:51:10.004637+00:00\\\u0026quot;,\\\u0026quot;f2\\\u0026quot;:\\\u0026quot;P0\\\u0026quot;,\\\u0026quot;f3\\\u0026quot;:100}\u0026quot;} 基础的 Kafka 操作 # 如果你是 Kafka 新手，接下来讲的一些命令将会帮助你，快速上手。\n下载并解压包 kafka：\nichexw$ wget -P path/to/dowload/directory http://packages.confluent.io/archive/4.0/confluent-oss-4.0.0-2.11.tar.gz # -P 下载文件存储的地址，我使用 /usr/loca/src/ ichexw$ cd path/to/dowload/directory ichexw$ tar -xzvf confluent-oss-4.0.0-2.11.tar.gz ichexw$ mv confluent-4.0.0 /path/to/install/directory/ # /path/to/install/directory/: kafka 的安装目录，我使用的是 /usr/local/ 启动相关服务：\nichexw$ bin/zookeeper-server-start etc/kafka/zookeeper.properties 2\u0026gt; zookeper.log \u0026amp; ichexw$ bin/kafka-server-start etc/kafka/server.properties 2\u0026gt; kafka.log \u0026amp; 创建主题（Tipics）：\nichexw$ bin/kafka-topics --list --zookeeper localhost:2181 ichexw$ bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic PGSHARD # 创建名为 PGSHARD, 3 个分区, 一个副本的主题 ichexw$ bin/kafka-topics --delete --zookeeper localhost:2181 --topic PGSHARD # 删除分区 ichexw$ bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic AGGREGATIONS ichexw$ bin/kafka-topics --delete --zookeeper localhost:2181 --topic AGGREGATIONS **注意：**你需要在 server.properties（默认在 kafka 安装文件夹的 /etc/kafka 中） 文件中设置 delete.topic.enable=true 选项，来激活删除主题操作\n引用自： Playing with Postgres and Kafka.\n","date":"2017-03-31","permalink":"/n3xtchen/2017/03/31/playing-with-postgres-and-kafka/","section":"时间线","summary":"Apache Kafka 和 Postgres: 处理事务和报表能力 # Apache Kafka 是目前主流的分布式流处理平台，用于数据处理和信息一致保证。她允许你集中数据流，完成多种目的。我突然对 Mozilla 的数据管道 实现感兴趣，尤其是其中展示了 Kafka 作为流的入口。","title":"让 Postgres 和 Kafka 一起玩耍"},{"content":"TLDR;\n1. 一切都是表 # 这是最微不足道的技巧，甚至不算一个技巧，但是它却是你全面了解 SQL 的基础：一切都是表！但你看到下面一段语句：\nSELECT * FROM person \u0026hellip;你很快注意到表 person。很棒，他是一张表。但是你意识到这整个语句也是一张表吗？举个例子，你可以写：\nSELECT * FROM ( SELECT * FROM person ) t 现在，你已经创建一张派生表（derived table）- 一张嵌套在 FROM 子句的 SELECT 语句。\n虽然这功能微不足道，但是很优雅。你也可创建一张使用 VALUES() 的 ad-hoc 的内存表，在一些数据库中（如，PostgreSQL，SQL Server）\nSELECT * FROM (VALUES(1), (2), (3))) t(a) 以及她的简单输出：\na --- 1 2 3 如果这个语法不知大，你可以把他转化成派生表，如，在 Oracle 中：\nSELECT * FROM ( SELECT 1 AS a FROM DUAL UINION ALL SELECT 2 AS a FROM DUAL UINION ALL SELECT 3 AS a FROM DUAL ) 正如你所看到的， VALUES() 和派生表实际上都是一样的东西，从概念上讲。让我们重温下插入语句，两种方法：\n-- SQL Server, PostgreSQL, some others: INSERT INTO my_table(a) VALUES (1), (2), (3); -- Oracle, many others: INSERT INTO my_table(a) SELECT 1 AS a FROM DUAL UINION ALL SELECT 2 AS a FROM DUAL UINION ALL SELECT 3 AS a FROM DUAL SQL 中，一切都是表。当你插入数据到表，你并不是在插入独立的行。你实际上插入的是一张表。大部分人经常只在表冲插入一张一行的表，因此意识不到。\n一切都是表。在 PostgreSQL 中，甚至连函数都是表：\nichexw=# SELECT * FROM substring('abcde', 2, 3); substring ----------- bcd (1 row) 如果你是个 JAVA 程序员，你可以使用 JAVA8 Stream API 来类比它。考虑下，下面的相同的概念\nTABLE：\tStream\u0026lt;Tuple\u0026lt;..\u0026gt;\u0026gt; SELECT:\tmap() DISTINCT:\tdistinct() JOIN:\tflatMap() WHERE/HAVING:\tfilter() GROUP BY:\tcollect() ORDER BY:\tsorted() UNION ALL:\tconcat() 使用 JAVA8 的过程中，“一切都是流”（至少在开始使用 Streams 的时候）。不管你怎么转换流，比如使用 map() 和 filter()，结果的类型永远都是一个流\n2. 使用递归SQL来生成数据 # 公用表表达式（Common Table Express，也叫 CTE）是 SQL 中声明变量的唯一方法（有别于 PostgreSQL 和 Sybase SQL 中的 Window 语句）\n她是一个强大的概念。异常强大。看看下面的语句：\nichexw=# WITH t1(v1, v2) AS (SELECT 1, 2), t2(w1, w2) AS ( SELECT v1 * 2, v2 * 2 FROM t1 ) SELECT * FROM t1, t2; v1 | v2 | w1 | w2 ----+----+----+---- 1 | 2 | 2 | 4 (1 row) 使用一个简单的 WITH 语法，你可以指定一个 Table 对象列表（记住：一切都是表），她们可以相互调用。\n这个很容易理解。这使得 CTE 很有用，并且她们还允许递归这种逆天的特性！现在看看下面的 PostgreSQL 语句：\nichexw=# WITH RECURSIVE t(v) AS ( SELECT 1 -- Seed Row UNION ALL SELECT v + 1 -- Recursion FROM t ) SELECT v FROM t LIMIT 5; v --- 1 2 3 4 5 (5 rows) 是不是很不错？看看注解，还是挺简单易用的。你定义了一个 CTE 实际上就是两个 UNION ALL 子查询\n第一个 UNION ALL 子查询称之为 “种子行（SEED ROW）”。她初始化了递归。她可以在后续的递归中生成一个或多行。记住：一切都是表，因此我们的递归生成的也是表，而不是一个独立的行或值。\n第二个 UNION ALL 子查询进行递归。如果你认真看，你会发现她是从 t 中检索数据的；例如 第二个子查询允许从我们声明的 CTE 中查询。递归。因此，她也能访问 CTE 中定义的 v 字段。\n在我们的例子中，把 row(1) 作为递归的种子，然后进行 v+1 的递归。这个递归通过设置一个 LIMIT 5 来作为停止条件的（是不是很很像 JAVA8 STREAM 中的潜在无限递归）。\nSide note：图灵完备 # 递归 CTE 使 SQL:1999 具有图灵完备，她意味着任何其他语言都可以使用 SQL 来重写！（如果你足够疯狂的话）\n下面可能在很多博客经常看到的一个令人影响深刻的例子：曼德博集合（The Mandelbrot Set）, e.g. 比如在 on http://explainextended.com/2013/12/31/happy-new-year-5/\nichexw=# WITH RECURSIVE q(r, i, rx, ix, g) AS ( SELECT r::DOUBLE PRECISION * 0.02, i::DOUBLE PRECISION * 0.02, .0::DOUBLE PRECISION , .0::DOUBLE PRECISION, 0 FROM generate_series(-60, 20) r, generate_series(-50, 50) i UNION ALL SELECT r, i, CASE WHEN abs(rx * rx + ix * ix) \u0026lt;= 2 THEN rx * rx - ix * ix END + r, CASE WHEN abs(rx * rx + ix * ix) \u0026lt;= 2 THEN 2 * rx * ix END + i, g + 1 FROM q WHERE rx IS NOT NULL AND g \u0026lt; 99 ) SELECT array_to_string(array_agg(s ORDER BY r), '') FROM ( SELECT i, r, substring(' .:-=+*#%@', max(g) / 10 + 1, 1) s FROM q GROUP BY i, r ) q GROUP BY i ORDER BY i; finance=# WITH RECURSIVE q(r, i, rx, ix, g) AS ( SELECT r::DOUBLE PRECISION * 0.02, i::DOUBLE PRECISION * 0.02, .0::DOUBLE PRECISION , .0::DOUBLE PRECISION, 0 FROM generate_series(-60, 20) r, generate_series(-50, 50) i UNION ALL SELECT r, i, CASE WHEN abs(rx * rx + ix * ix) \u0026lt;= 2 THEN rx * rx - ix * ix END + r, CASE WHEN abs(rx * rx + ix * ix) \u0026lt;= 2 THEN 2 * rx * ix END + i, g + 1 FROM q WHERE rx IS NOT NULL AND g \u0026lt; 99 ) SELECT array_to_string(array_agg(s ORDER BY r), '') FROM ( SELECT i, r, substring(' .:-=+*#%@', max(g) / 10 + 1, 1) s FROM q GROUP BY i, r ) q GROUP BY i ORDER BY i; ---------------------------------------------------------------------------------- ..... ..@ ..:..:. ..... ..:.. ..:.. ..-:.. .....=@#+: ....:.=@@=..... :.-..+@*@*::..:. ..:-@@@@@@@@:.:-. ..@@@@@@@@@+%.. ..@@@@@@@@@@-.. :-*@@@@@@@@@:-: ..:@@@@@@@@@@@.. ...*@@@@@@@@@@:.. . ......-@@@@@@@@@.... . ..... ..:.......=@@@@@@@-........: .. .-.:-.......==..*.=.::-@@@@@:::.:.@..*-. =. ...=...=...::+%.@:@@@@@@@@@@@@@+*#=.=:+-. ..- .:.:=::*....@@@@@@@@@@@@@@@@@@@@@@@@=@@.....::...:. ...*@@@@=.@:@@@@@@@@@@@@@@@@@@@@@@@@@@=.=....:...::. .::@@@@@:-@@@@@@@@@@@@@@@@@@@@@@@@@@@@:@..-:@=*:::. .-@@@@@-@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@.=@@@@=..: ...@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:@@@@@:.. ....:-*@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:: .....@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@-.. .....@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@-:... .--:+.@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@... .==@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@-.. ..+@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@-#. ...=+@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@.. -.=-@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@..: .*%:@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:@- . ..:... ..-@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ .............. ....-@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@%@= .--.-.....-=.:..........::@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@.. ..=:-....=@+..=.........@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:. .:+@@::@==@-*:%:+.......:@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@. ::@@@-@@@@@@@@@-:=.....:@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@: .:@@@@@@@@@@@@@@@=:.....%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ .:@@@@@@@@@@@@@@@@@-...:@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@:- :@@@@@@@@@@@@@@@@@@@-..%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@. %@@@@@@@@@@@@@@@@@@@-..-@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@. @@@@@@@@@@@@@@@@@@@@@::+@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@+ @@@@@@@@@@@@@@@@@@@@@@:@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@.. @@@@@@@@@@@@@@@@@@@@@@-@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@- @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@.. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@. : 被 shock 到了吧？\n3. 计算运行总计 # 在 Microsoft Excel 中，你会简单地计算前两个（或后续）的值之和（或差），然后使用有用的十字光标在整个电子表格中拖动该公式。\n您通过电子表格计算运行总计。\n在 SQL 的世界中，最好的方法就是使用窗口函数（Window Function）。\n窗口函数是一个很强大的概念-首先，她表面上看不那么好理解，但实际上真的很简单：\n窗口函数是相对于由SELECT转换的当前行的行的子集上的聚合/排名\n就这么简单，^_^\n它本质上意味着窗口函数可以对当前行“之上”或“之下”的行执行计算。不像常规的聚合和分组，它们不会转换行，这使它们非常有用。\n语法可以归纳如下：\nfunction(...) OVER ( PARTITION BY ... ORDER BY ... ROWS BETWEEN ... AND ... ) 一次，我有各种排序函数（我们将在后面一一解释这些函数），仅接着是 OVER() 分句，她指定了窗口，定义如下：\nPARTITION: 只有与当前行在同一分区中的行才会被视为该窗口 ORDER: 我们可以给筛选出来的窗口进行排序 ROWS（或 RANGE）帧定义：窗口可以被限制为“之前”和“之后”的固定量的行。 这就是窗口函数的全部。\n现在，我们来看看，她如何为我们实现计算运行总计？下面是数据\nID VALUE_DATE AMOUNT BALANCE 9997 2014-03-18 99.17 19985.81 9981 2014-03-16 71.44 19886.64 9979 2014-03-16 -94.60 19815.20 9977 2014-03-16 -6.96 19909.80 9971 2014-03-15 -65.95 19916.76 我们假设 BALANCE 就是我们想要数值。\n直观地，我们可以立即看出规律（看加重符号,加上符号）\nID VALUE_DATE AMOUNT BALANCE 9997 2014-03-18 -(99.17) +19985.81 9981 2014-03-16 -(71.44) 19886.64 9979 2014-03-16 -(-94.60) 19815.20 9977 2014-03-16 -6.96 =19909.80 9971 2014-03-15 -65.95 19916.76 因此，我们可以使用下面伪 SQL 语句来表达任何的任何余额：\n当前的月 - SUM(进出款项金额) OVER ( \u0026quot;当前行之上的所有行\u0026quot; ) 实际的 SQL 可以写成这样：\nSUM(t.amount) OVER ( PARTITION BY t.account_id ORDER BY t.value_date DESC, t.id DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING ) 解释如下：\n分区可以计算每一个银行账户的汇总，而不是全部数据的 汇总之前，排序保证交易在分区内是有序的 ROWS 分句讲只考虑分区内之前的行（给定的排序）的汇总 所有这些都将发生在内存中的数据集已经由您在FROM .. WHERE等子句中选择，因此非常快。\n中场休息 # 在我们介绍其他技巧之前，大家思考下：我们已经看到了\n（递归）Common Table Expressions(公共表表达式,CTE) 窗口函数 她们都有共同的特征：\n很棒 异常强大 声明式 SQL 标准的一部分 在大部分关系型数据库可用(除了 MySQL) 非常重要构件 如果一定要从这个文章中得出什么结论的话，那就是你绝对应该知道现代的SQL这两个重要构件。为什么？可以从这个 站点中 中得到答案。\n4. 寻找连续无间隔的最长子序列 # 很多应用或者网站为了刺激用户活跃留存，对连续登录的用户进行奖励。比如，StackOverflow 的徽章：\nEnthusianst: 连续30天访问每天都访问的用户 Fanatic：连续100天访问每天都访问的用户 那我们如何计算这些徽章呢？这些徽章用来奖励给连续使用他们平台指定天数的用户。不管婚礼或者结婚纪念日，你也必须登录，否则计数就会归0。\n正如我们所使用的是声明式编程，我不需要当心维护任何状态和内存计数。我们想要使用在线分析 SQL 的形式表达她。例如，看看这些数据（测试数据生成方法见附录-1）：\nn3xt-test=# SELECT login_time FROM user_login WHERE id = :user_id; login_time --------------------- 2017-02-17 16:00:00 2017-02-16 20:00:00 2017-02-16 03:00:00 2017-02-15 21:00:00 2017-02-15 20:00:00 2017-02-14 01:00:00 2017-02-12 09:00:00 2017-02-11 00:00:00 2017-02-10 20:00:00 2017-02-10 10:00:00 2017-02-09 20:00:00 2017-02-09 05:00:00 2017-02-08 19:00:00 (13 rows) 一点帮助都没有。让我们从时间戳中去掉小时，并去重。这很简单：\nn3xt-test=# SELECT DISTINCT CAST(login_time AS DATE) login_date FROM user_login WHERE id = :user_id; login_date ------------ 2017-02-17 2017-02-16 2017-02-15 2017-02-14 2017-02-12 2017-02-11 2017-02-10 2017-02-09 2017-02-08 (9 rows) 就是现在，使用我们已经学过的窗口函数，让我们给每一个日期加上简单的行数：\nn3xt-test=# SELECT login_date, row_number() OVER (ORDER BY login_date) FROM login_date; login_date | row_number ------------+------------ 2017-02-08 | 1 2017-02-09 | 2 2017-02-10 | 3 2017-02-11 | 4 2017-02-12 | 5 2017-02-14 | 6 2017-02-15 | 7 2017-02-16 | 8 2017-02-17 | 9 (9 rows) 接下来仍然很简单。看看发生了什么，如果不单独选择这些值，我们减去它们？\nn3xt-test=# SELECT login_date, (row_number() OVER (ORDER BY login_date)), login_date - (row_number() OVER (ORDER BY login_date))::INT grp FROM login_date; login_date | row_number | grp ------------+------------+------------ 2017-02-08 | 1 | 2017-02-07 2017-02-09 | 2 | 2017-02-07 2017-02-10 | 3 | 2017-02-07 2017-02-11 | 4 | 2017-02-07 2017-02-12 | 5 | 2017-02-07 2017-02-14 | 6 | 2017-02-08 2017-02-15 | 7 | 2017-02-08 2017-02-16 | 8 | 2017-02-08 2017-02-17 | 9 | 2017-02-08 (9 rows) 上述这些简单例子来说明了：\nROW_NUMBER() 不言而喻，不会有间隔。 然而我们的数据有 因此，我们把不连续有间隔的时间序列减去一个连续的整数序列，得到的新的日期相同的时间处在同一个连续日期：\nn3xt-test=# SELECT min(login_date), max(login_date), max(login_date) - min(login_date) + 1 AS length FROM login_date_groups GROUP BY grp ORDER BY length DESC; min | max | length ------------+------------+-------- 2017-02-08 | 2017-02-12 | 5 2017-02-14 | 2017-02-17 | 4 (2 rows) 下面是完整的查询语句：\n1 WITH login_date AS ( 2 SELECT DISTINCT CAST(login_time AS DATE) login_date 3 FROM user_login 4 WHERE id = 1 5 ), login_date_groups AS ( 6 SELECT 7 login_date, 8 (row_number() OVER (ORDER BY login_date)), 9 login_date - (row_number() OVER (ORDER BY login_date))::INT grp 10 FROM login_date 11 ) 12 SELECT 13 min(login_date), max(login_date), 14 max(login_date) - 15 min(login_date) + 1 AS length 16 FROM login_date_groups 17 GROUP BY grp 18 ORDER BY length DESC; 5. 寻找序列长度 # 上一个例子，我们已经提取连续值的序列。很简单，我们几乎滥用了整数连续队列。倘若序列的定义不够直观，？看看接下来的数据，LENGTH 是我们想计算的每一个序列的长度：\nid amount length 20 13.97 3 19 21.13 3 18 84.72 3 17 -18.91 2 16 -65.99 2 15 18.07 1 14 -52.68 1 13 16.87 1 12 -56.76 2 11 -94.72 2 10 95.46 1 9 -52.45 1 是的，你的猜测是正确，这个是收支方向（SIGN(AMOUNT)）相同根据订单ID排序生成的连续序列，看下格式化后的数据：\nid amount length 20 +13.97 3 19 +21.13 3 18 +84.72 3 17 -18.91 2 16 -65.99 2 15 +18.07 1 14 -52.68 1 13 +16.87 1 12 -56.76 2 11 -94.72 2 10 +95.46 1 9 -52.45 1 那我们要怎么做？太简单，首先去除所有的噪音，加入行数\nn3xt-test=# SELECT id, amount, sign(amount) AS sign, row_number() OVER (ORDER BY id DESC) AS rn FROM orders; id | amount | sign | rn ----+--------+------+---- 20 | 13.97 | 1 | 1 19 | 21.13 | 1 | 2 18 | 84.72 | 1 | 3 17 | -18.91 | -1 | 4 16 | -65.99 | -1 | 5 15 | 18.07 | 1 | 6 14 | -52.68 | -1 | 7 13 | 16.87 | 1 | 8 12 | -56.76 | -1 | 9 下一个目标是生成下面这样的表：\nid amount sign rn lo hi 20 13.97 1 1 1 19 21.13 1 2 18 84.72 1 3 3 17 -18.91 -1 4 4 16 -65.99 -1 5 5 15 18.07 1 6 6 6 14 -52.68 -1 7 7 7 13 16.87 1 8 8 8 12 -56.76 -1 9 9 11 -94.72 -1 10 10 10 95.46 1 11 11 11 在这个表中，我想复制行数到一个子系列的起始行（下界）的 LO 字段，和结束行（上界）的 HI 字段中。为了这个，我们需要使用两个魔法函数 LEAD() 和 LAG()：\nLEAD()：当前行的下 n 行\nLAG()：当前行的上 n 行\nn3xt-test=# SELECT lag(v) OVER (ORDER BY v), v, lead(v) OVER (ORDER BY v) FROM ( VALUES (1), (2), (3), (4) ) t(v); lag | v | lead -----+---+------ | 1 | 2 1 | 2 | 3 2 | 3 | 4 3 | 4 | (4 rows) 很神奇有木有？记住，在窗口函数内，你可以对 和当前相关的行的子集 进行排行或者聚合。在 LEAD() 和 LAG() 的例子中，我们访问当前行相关的行，重要指定偏离位置，是很容易的。在很多场景中时很有用的。\n继续我的 LO 和 HGIH 例子：\nSELECT trx.*, CASE WHEN lag(sign) OVER (ORDER BY id DESC) != sign THEN rn END AS lo, CASE WHEN lead(sign) OVER (ORDER BY id DESC) != sign THEN rn END AS hi FROM trx; 通过与上一行（lag()）对比 sign 字段，如果他们符号相反，我们把当前的行数复制到 LO 字段，因为这是我们序列的下界。\n然后通过与下一行（lead()）对比 sign 字段，如果他们符号相反，我们把当前的行数复制到 LO 字段，因为这是我们序列的上界。\n最后，我们需要处理讨厌的空值（NULL）：\nSELECT -- With NULL handling... trx.*, CASE WHEN coalesce(lag(sign) OVER (ORDER BY id DESC), 0) != sign THEN rn END AS lo, CASE WHEN coalesce(lead(sign) OVER (ORDER BY id DESC), 0) != sign THEN rn END AS hi FROM trx; 下一步，我们想要 LO 和 HI 出现在我们的所有行中。\nid amount sign rn lo hi 20 13.97 1 1 1 3 19 21.13 1 2 1 3 18 84.72 1 3 1 3 17 -18.91 -1 4 4 5 16 -65.99 -1 5 4 5 15 18.07 1 6 6 6 14 -52.68 -1 7 7 7 13 16.87 1 8 8 8 12 -56.76 -1 9 9 10 11 -94.72 -1 10 9 10 10 95.46 1 11 11 11 我们所使用的特性至少在 Redshift，Sybase SQL，DB2 以及 Oracle 中都可用。我们使用 IGNORE NULLS 语句：\nSELECT trx.*, last_value (lo) IGNORE NULLS OVER ( ORDER BY id DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS lo, first_value(hi) IGNORE NULLS OVER ( ORDER BY id DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS hi FROM trx 很多关键字！但是本质往往是相同的。在任何给定的当前行，我们寻找之前的值（previous values，ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW），但是忽略所有的空值。从之前的之中，我们获取最后的值，和我们的新 LO 值。换句话说，我们获取向前最接近当前行（closest preceding）的 LO 值。\nHI 也是同理。在任何给定的当前行，我们寻找随后的值（subsequent values，ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW），但是忽略所有的空值。从之前的之中，我们获取最后的值，和我们的新 HI 值。换句话说，我们获取向后最接近当前行（closest following）的 HI 值。\nSELECT -- With NULL handling... trx.*, coalesce(last_value (lo) IGNORE NULLS OVER ( ORDER BY id DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW), rn) AS lo, coalesce(first_value(hi) IGNORE NULLS OVER ( ORDER BY id DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING), rn) AS hi FROM trx 最后，我们只是做一个微不足道的最后一步，记住处理 off-by-1 错误：\nSELECT trx.*, 1 + hi - lo AS length FROM trx 这个是我们最后的结果：\nid amount sign rn lo hi length 20 13.97 1 1 1 3 3 19 21.13 1 2 1 3 3 18 84.72 1 3 1 3 3 17 -18.91 -1 4 4 5 2 16 -65.99 -1 5 4 5 2 15 18.07 1 6 6 6 1 14 -52.68 -1 7 7 7 1 13 16.87 1 8 8 8 1 12 -56.76 -1 9 9 10 2 11 -94.72 -1 10 9 10 2 10 95.46 1 11 11 11 1 下面是完整版的查询：\nWITH trx1(id, amount, sign, rn) AS ( SELECT id, amount, sign(amount), row_number() OVER (ORDER BY id DESC) FROM trx ), trx2(id, amount, sign, rn, lo, hi) AS ( SELECT trx1.*, CASE WHEN coalesce(lag(sign) OVER (ORDER BY id DESC), 0) != sign THEN rn END, CASE WHEN coalesce(lead(sign) OVER (ORDER BY id DESC), 0) != sign THEN rn END FROM trx1 ) SELECT trx2.*, 1 - last_value (lo) IGNORE NULLS OVER (ORDER BY id DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) + first_value(hi) IGNORE NULLS OVER (ORDER BY id DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) FROM trx2 由于 PostgreSQL 没有 IGNORE NULLS 语句，所以我在给出一个实现：\nWITH trx AS ( SELECT id, amount, sign(amount) AS sign, row_number() OVER (ORDER BY id DESC) AS rn FROM orders ), trx1 AS ( SELECT trx.*, CASE WHEN coalesce(lag(sign) OVER (ORDER BY id DESC), 0) != sign THEN rn END AS lo, CASE WHEN coalesce(lead(sign) OVER (ORDER BY id DESC), 0) != sign THEN rn END AS hi FROM trx ), trx2 AS ( SELECT -- 数据对齐 trx1.*, sum(case when lo is null then 0 else 1 end) over (order by id desc) as lo_partition FROM trx1 ) SELECT trx2.id, trx2.amount, trx2.sign, trx2.rn, max(trx2.lo) OVER (PARTITION BY trx2.lo_partition) lo, max(trx2.hi) OVER (PARTITION BY trx2.lo_partition) hi, max(trx2.hi) OVER (PARTITION BY trx2.lo_partition) - max(trx2.lo) OVER (PARTITION BY trx2.lo_partition) + 1 length FROM trx2; 6. 子集和問題（The subset sum problem with SQL） # 什么是子集和问题？这里进行了有趣的解释：\nhttps://xkcd.com/287\n还是维基百科上乏味的解释：\n子集和问题\n本质上，对每一个的求和。。。\nID TOTAL 1 25150 2 19800 3 27511 我想要尽可能地从这些组合项中找到“最好”的和：\nID ITEM 1 7120 2 8150 3 8255 4 9051 5 1220 6 12515 7 13555 8 5221 9 812 10 6562 如果你心算够好的话，你可以直接得出最佳的和：\nTOTAL SUM CALC 25150 25133 7120 + 8150 + 9051 + 812 19800 19768 1220 + 12515 + 5221 + 812 27511 27488 8150 + 8255 + 9051 + 1220 + 812 使用 SQL 怎么处理呢？简单，只需要使用创建一个 CTE，枚举出 2的n次方种减1个组合，并找到最接近的一个：\n-- 枚举所有的组合，2**n - 1 组合 WITH sums(sum, max_id, calc) AS (...) -- 找出最接近 total 的那一条 SELECT totals.total, something_something(total - sum) AS best, something_something(total - sum) AS calc FROM draw_the_rest_of_the_*bleep*_owl 如果你读到这里，说明我们是真朋友，^_^\n不要担心，方法并没有想象中那么难。\n首先，我们需要枚举所有子集合(实现全排列的算法)，这个比较简单：\nWITH RECURSIVE assign(id, total) AS ( ... ), vals(id, item) AS (...), sums (start_id, max_id, sum, cacl) AS ( SELECT id, id, item, item::text FROM vals UNION ALL SELECT sums.start_id, t.id, sum+item, cacl|| '+' || item::text FROM sums JOIN vals t ON sums.max_id \u0026lt; t.id ) SELECT * FROM sums 然后，我们把需要结果从候选组合中找到最接近的组合：\nSELECT total, sum, cacl FROM assign a, LATERAL ( SELECT sum, cacl FROM sums ORDER BY ABS(a.total - sum) FETCH FIRST 1 ROW ONLY\t-- 等同于 limit 1，这个是 SQL 标准，limit 不是 ) b; 为 ASSIGN 的每个值通过和 SUMS 连表获取按照排序的的一行的值。我们需要使用 LATERAL，因为他允许我们访问左边的表的字段，否则正常的 JOIN 无法直接获取的。\n同样的功能在 SQL Server（它的关键字是 CROSS APPLY） 也支持。\n在连表的右侧的结果依赖于左边的时候，LATERAL 可能很有用。与普通连接不同，这意味着 JOIN 顺序将从左到右依次设置，优化器具有一组减少的连接算法选项。想现在这个场景（带着 ORDER BY 和 FETCH FRIST），或者连接非嵌套的表值函数。\n下面是完整的查询。\nn3xt-test=# WITH RECURSIVE assign(id, total) AS ( SELECT 1, 25150 UNION ALL SELECT 2, 19800 UNION ALL SELECT 3, 27511 ), vals (id, item) AS ( SELECT 1 , 7120 UNION ALL SELECT 2 , 8150 UNION ALL SELECT 3 , 8255 UNION ALL SELECT 4 , 9051 UNION ALL SELECT 5 , 1220 UNION ALL SELECT 6 , 12515 UNION ALL SELECT 7 , 13555 UNION ALL SELECT 8 , 5221 UNION ALL SELECT 9 , 812 UNION ALL SELECT 10, 6562 ), sums (start_id, max_id, sum, cacl) AS ( SELECT id, id, item, item::text FROM vals UNION ALL SELECT sums.start_id, t.id, sum+item, cacl|| '+' || item::text FROM sums JOIN vals t ON sums.max_id \u0026lt; t.id ) SELECT total, sum, cacl FROM assign a, LATERAL ( SELECT sum, cacl FROM sums ORDER BY ABS(a.total - sum) FETCH FIRST 1 ROW ONLY ) b; total | sum | cacl -------+-------+------------------------- 25150 | 25133 | 7120+8150+9051+812 19800 | 19768 | 1220+12515+5221+812 27511 | 27488 | 8150+8255+9051+1220+812 (3 rows) 7. 覆盖运行中的汇总 # 之前，我们已经知道怎么使用窗口函数计算“一般的”运行中汇总。很简单。现在，如果我们想要覆盖运行中的汇总，使得她永远大于0？基本上，我们星耀计算这个：\nDATE AMOUNT TOTAL 2012-01-01 800 800 2012-02-01 1900 2700 2012-03-01 1750 4450 2012-04-01 -20000 0 2012-05-01 900 900 2012-06-01 3900 4800 2012-07-01 -2600 2200 2012-08-01 -2600 0 2012-09-01 2100 2100 2012-10-01 -2400 0 2012-11-01 1100 1100 2012-12-01 1300 2400 当一笔很大支出 -20000 被剪去，我将其归0即可，而不是显示世纪的 -15550。看我的注释就明白了：\nDATE AMOUNT TOTAL Total 的公式 2012-01-01 800 800 GREATEST(0, 800) 2012-02-01 1900 2700 GREATEST(0, 2700) 2012-03-01 1750 4450 GREATEST(0, 4450) 2012-04-01 -20000 0 GREATEST(0, -15550) 2012-05-01 900 900 GREATEST(0, 900) 2012-06-01 3900 4800 GREATEST(0, 4800) 2012-07-01 -2600 2200 GREATEST(0, 2200) 2012-08-01 -2600 0 GREATEST(0, -400) 2012-09-01 2100 2100 GREATEST(0, 2100) 2012-10-01 -2400 0 GREATEST(0, -300) 2012-11-01 1100 1100 GREATEST(0, 1100) 2012-12-01 1300 2400 GREATEST(0, 2400) 我们怎么做呢？窗口函数和递归CTE都是可以实现，看到这里大家估计也已经视觉疲劳了，我们换个新法子？但是这个法子只有 Oracle，vendor-specific SQL。\n将会非常的惊艳，只需要在任何报表的后面加上 MODEL：\nSELECT ... FROM some_table -- 放在任何表后面\tMODEL ... 然后你就可以直接在 SQL 语句中实现电子表格的逻辑，和 Excel 一样。\n下面是接下来三个语句将非常实用和广泛的使用\nMODEL -- 维度 DIMENSION BY ... -- 报表字段 MEASURES ... -- 公司 RULES ... 稍微解释下：\nDIMENSION BY：指定电子表格的维度。不像 Excel，你可以在 Oracle 中指定任意数量的维度，而不是2个。 MEASURES：可用的值。不像 Excel ，在单元格中可以使用元祖，而不是单一的值。 RULES：每一个单元格的公式。不像 Excel，这个公式集中放在这里，而不是在每一个单元格中。 使得 MODEL 使用起来比 Excel 难一些，但是功能更强大，如果你敢用。下面给一个小 demo：\nSELECT * FROM ( SELECT date, amount, 0 AS total FROM amounts ) MODEL DIMENSION BY (row_number() OVER (ORDER BY date) AS rn) MEASURES (date, amount, total) RULES ( total[any] = greatest(0, coalesce(total[cv(rn) - 1], 0) + amount[cv(rn)]) ) 8. 时间序列模式识别（Time Series Pattern Recognition） # 如果你对诈骗识别或者其他运行实时大数据的领域感兴趣，时间模式识别这个名词对你来说将不会太陌生。\n如果我们重温 5. 寻找序列长度 的章节，将会想在我们时间序列的复杂事件（Event）上生成触发器（Trigger）：\nID VALUE_DATE AMOUNT LEN TRIGGER 9997 2014-03-18 + 99.17 1 9981 2014-03-16 - 71.44 4 9979 2014-03-16 - 94.60 4 x 9977 2014-03-16 - 6.96 4 9971 2014-03-15 - 65.95 4 9964 2014-03-15 + 15.13 3 9962 2014-03-15 + 17.47 3 9960 2014-03-15 + 3.55 3 9959 2014-03-14 - 32.00 1 触发器的规则是：\n如果某个事件连续发生3次，则触发该触发器（Trigger）。\n也和之前的 MODEL 语句类似，我们能做的就是使用 Oracle 12c 语法：\nSELECT ... FROM some_table\tMATCH_RECOGNIZE (...) MATCH_RECOGNIZE 的最简单的应用包括以下子句：\nSELECT * FROM series MATCH_RECOGNIZE ( -- 模式匹配在这个顺序下完成 ORDER BY ... -- 用来匹配的字段 MEASURES ... -- 每一次匹配后返回行的配置 ALL ROWS PER MATCH -- 匹配事件的正则表达式 PATTERN (...) -- 事件的定义 DEFINE ... ) 这个听起来太疯狂了。现在看一个实际的例子：\nSELECT * FROM series MATCH_RECOGNIZE ( ORDER BY id MEASURES classifier() AS trg ALL ROWS PER MATCH PATTERN (S (R X R+)?) DEFINE R AS sign(R.amount) = prev(sign(R.amount)), X AS sign(X.amount) = prev(sign(X.amount)) ) 我们做了什么？\n根据 ID 排序 然后我们指定我们想要的值作为结果。 我们需要 MEASURE 触发器（Trigger），它被定义为分类器，即我们将在模式中使用的文字。 此外，我们想要匹配的所有行。 我们指定类正则表达式模式。这个模式是一个 S 事件（Event）定义开始，接着 R 时间定义重复。如果全部模式匹配，我们会的得到 SRXR，SRXRR 或 SRXRRR，例如， X 将会在序列长度大于 4 的第三个位置北标记 最后，我们定义 R 和 X 成同一个事件（Event），即当前行和上一行的 SIGN(AMOUNT) 相同时触发。我们没有定义 S，他可以是任何的其他行。 这个查询会产生下面魔法般的输出：\nID VALUE_DATE AMOUNT TRG 9997 2014-03-18 + 99.17 S 9981 2014-03-16 - 71.44 R 9979 2014-03-16 - 94.60 X 9977 2014-03-16 - 6.96 R 9971 2014-03-15 - 65.95 S 9964 2014-03-15 + 15.13 S 9962 2014-03-15 + 17.47 S 9960 2014-03-15 + 3.55 S 9959 2014-03-14 - 32.00 S 我们可以看到一个 X 在我们的事件（Event）系统。这个就是实际上我们想要的。在一系列长度大于3的事件（相同符号）的第三次重复时触发。\nBoom！\n实际上，我们根本不 Care S 和 R 事件（Event），只需要像这样去掉就好：\nSELECT id, value_date, amount, CASE trg WHEN 'X' THEN 'X' END trg FROM series MATCH_RECOGNIZE ( ORDER BY id MEASURES classifier() AS trg ALL ROWS PER MATCH PATTERN (S (R X R+)?) DEFINE R AS sign(R.amount) = prev(sign(R.amount)), X AS sign(X.amount) = prev(sign(X.amount)) ) 最后的结果如下：\nID VALUE_DATE AMOUNT TRG 9997 2014-03-18 + 99.17 9981 2014-03-16 - 71.44 9979 2014-03-16 - 94.60 X 9977 2014-03-16 - 6.96 9971 2014-03-15 - 65.95 9964 2014-03-15 + 15.13 9962 2014-03-15 + 17.47 9960 2014-03-15 + 3.55 9959 2014-03-14 - 32.00 感谢， ORACLE！\n另外别要期待我继续介绍 Oracle 白皮书（如果你在使用 Oracle 12c, 那强烈建议看一下她的 文档）的其他特性了。\n9. 数据表行列转换（Pivoting and Unpivoting） # 如果你已经读到这里，接下来的内容都太简单了，和大家过一下：\n这是我们的数据，主演，电影名以及电影评级：\nNAME TITLE RATING A. GRANT ANNIE IDENTITY G A. GRANT DISCIPLE MOTHER PG A. GRANT GLORY TRACY PG-13 A. HUDSON LEGEND JEDI PG A. CRONYN IRON MOON PG A. CRONYN LADY STAGE PG B. WALKEN SIEGE MADRE R 我们想要转换成：\nNAME NC-17 PG G PG-13 R A. GRANT 3 6 5 3 1 A. HUDSON 12 4 7 9 2 A. CRONYN 6 9 2 6 4 B. WALKEN 8 8 4 7 3 B. WILLIS 5 5 14 3 6 C. DENCH 6 4 5 4 5 C. NEESON 3 8 4 7 3 如果是用过 Excel 的 透视表 的可以略过接下来的两段解释。\n大家注意到了，我可以根据演员进行分组，然后把该演员每个评级分组下的电影数量转化成列（PIVOTING）。不以关系的形式显示，（例如，每组一行），我们把所有的组都转换成列。之所以可以这么做，是因为我们实现知道所有可能出现的分组。\n列转行（Unpivoting） 则是相反的操作。\nNAME RATING COUNT A. GRANT NC-17 3 A. GRANT PG 6 A. GRANT G 5 A. GRANT PG-13 3 A. GRANT R 6 A. HUDSON NC-17 12 A. HUDSON PG 4 这个实际上很简单。下面是 PostgreSQL 的实现：\nSELECT first_name, last_name, count(*) FILTER (WHERE rating = 'NC-17') AS \u0026quot;NC-17\u0026quot;, count(*) FILTER (WHERE rating = 'PG' ) AS \u0026quot;PG\u0026quot;, count(*) FILTER (WHERE rating = 'G' ) AS \u0026quot;G\u0026quot;, count(*) FILTER (WHERE rating = 'PG-13') AS \u0026quot;PG-13\u0026quot;, count(*) FILTER (WHERE rating = 'R' ) AS \u0026quot;R\u0026quot; FROM actor AS a JOIN film_actor AS fa USING (actor_id) JOIN film AS f USING (film_id) GROUP BY actor_id 我们可以向聚合函数附加一个简单的 FILTER 子句，以便只计算相关的数据。\n在其他数据库下，我们可以这么做：\nSELECT first_name, last_name, count(CASE rating WHEN 'NC-17' THEN 1 END) AS \u0026quot;NC-17\u0026quot;, count(CASE rating WHEN 'PG' THEN 1 END) AS \u0026quot;PG\u0026quot;, count(CASE rating WHEN 'G' THEN 1 END) AS \u0026quot;G\u0026quot;, count(CASE rating WHEN 'PG-13' THEN 1 END) AS \u0026quot;PG-13\u0026quot;, count(CASE rating WHEN 'R' THEN 1 END) AS \u0026quot;R\u0026quot; FROM actor AS a JOIN film_actor AS fa USING (actor_id) JOIN film AS f USING (film_id) GROUP BY actor_id 现在，如果你在使用 SQL Server 或者 Oracle 的话，你还可以使用内建的 PIVOT 或 UNPIVOT 语句，就像 MODEL 和 MATCH_RECOGNIZE 一样，在一个表的后面添加新的关键词就好：\n-- 行转列 SELECT something, something FROM some_table PIVOT ( count(*) FOR rating IN ( 'NC-17' AS \u0026quot;NC-17\u0026quot;, 'PG' AS \u0026quot;PG\u0026quot;, 'G' AS \u0026quot;G\u0026quot;, 'PG-13' AS \u0026quot;PG-13\u0026quot;, 'R' AS \u0026quot;R\u0026quot; ) ) -- 列转行 SELECT something, something FROM some_table UNPIVOT ( count FOR rating IN ( \u0026quot;NC-17\u0026quot; AS 'NC-17', \u0026quot;PG\u0026quot; AS 'PG', \u0026quot;G\u0026quot; AS 'G', \u0026quot;PG-13\u0026quot; AS 'PG-13', \u0026quot;R\u0026quot; AS 'R' ) ) 10. 滥用 XML 和 JSON # 首先\nJSON 只是更少特性和语法的 XML\n现在，所有人都知道 XML 很棒。必然的结果是：\nJSON 就逊多了\n不要使用 JSON。\n现在我们已经解决了这一点，我们可以安全地忽略正在进行的 JSON 数据库中的炒作（你们大多数将在五年后遗憾），并继续到最后的例子。 如何在数据库中做XML。\n给定一个原始的 XML 文档，我们想要解析他，不要在每个演员中嵌套逗号分隔的电影列表，在一个表中产生演员/电影的无范式表示。\n开始，我们接下来要写三个 CTE：\n第一个，我们简单的解析 XML。下面使用 PostgreSQL：\nn3xt-test=# WITH RECURSIVE x(v) AS (SELECT ' \u0026lt;actors\u0026gt; \u0026lt;actor\u0026gt; \u0026lt;first-name\u0026gt;Bud\u0026lt;/first-name\u0026gt; \u0026lt;last-name\u0026gt;Spencer\u0026lt;/last-name\u0026gt; \u0026lt;films\u0026gt;God Forgives... I Don’t, Double Trouble, They Call Him Bulldozer\u0026lt;/films\u0026gt; \u0026lt;/actor\u0026gt; \u0026lt;actor\u0026gt; \u0026lt;first-name\u0026gt;Terence\u0026lt;/first-name\u0026gt; \u0026lt;last-name\u0026gt;Hill\u0026lt;/last-name\u0026gt; \u0026lt;films\u0026gt;God Forgives... I Don’t, Double Trouble, Lucky Luke\u0026lt;/films\u0026gt; \u0026lt;/actor\u0026gt; \u0026lt;/actors\u0026gt;'::xml) SELECT * FROM x; 简单。\n然后，我们使用 XPATH 来提取 XML 结构中的每一个值，并把它放到字段中：\nn3xt-test=# WITH RECURSIVE x(v) AS (SELECT '...'::xml), actors(actor_id, first_name, last_name, films) AS ( SELECT row_number() OVER (), (xpath('//first-name/text()', t.v))[1]::TEXT, (xpath('//last-name/text()' , t.v))[1]::TEXT, (xpath('//films/text()' , t.v))[1]::TEXT FROM unnest(xpath('//actor', (SELECT v FROM x))) t(v) ) SELECT * FROM actors; actor_id | first_name | last_name | films ----------+------------+-----------+------------------------------------------------------------------ 1 | Bud | Spencer | God Forgives... I Don’t, Double Trouble, They Call Him Bulldozer 2 | Terence | Hill | God Forgives... I Don’t, Double Trouble, Lucky Luke (2 rows) 同样很简单。\n最后，使用递归正则表达式模式匹配，然后就结束教程吗^_^\nn3xt-test=# WITH RECURSIVE x(v) AS (SELECT '...'::xml), actors(actor_id, first_name, last_name, films) AS (...), films(actor_id, first_name, last_name, film_id, film) AS ( SELECT actor_id, first_name, last_name, 1, regexp_replace(films, ',.+', '') FROM actors UNION ALL SELECT actor_id, a.first_name, a.last_name, f.film_id + 1, regexp_replace(a.films, '.*' || f.film || ', ?(.*?)(,.+)?', '\\1') FROM films AS f JOIN actors AS a USING (actor_id) WHERE a.films NOT LIKE '%' || f.film ) SELECT * FROM films; actor_id | first_name | last_name | film_id | film ----------+------------+-----------+---------+------------------------- 1 | Bud | Spencer | 1 | God Forgives... I Don’t 2 | Terence | Hill | 1 | God Forgives... I Don’t 1 | Bud | Spencer | 2 | Double Trouble 2 | Terence | Hill | 2 | Double Trouble 1 | Bud | Spencer | 3 | They Call Him Bulldozer 2 | Terence | Hill | 3 | Lucky Luke (6 rows) 开始总结陈词：\n解语 # 这文章中所有的内容都是声明式的。并且相对简单。当然，我尝试然这篇文章更有趣味性，一切夸张的 SQL 被采用，并让大部分东西都比较易学。也不是都很简单，你必须多练习写 SQL。和其他语言一样，但是稍微难度大一点，因为：\n时不时出现比较晦涩的语法 使用声明式的思想并不太容易。至少，和面向对象和面向过程式的思想差异很大。 但是一旦你习惯了，使用 SQL 的 声明式编程 还是值得你学习的。您可以通过描述要从数据库获取的结果，在非常少的代码中表达您的数据之间的复杂关系。\n是不是很棒？\n附录-1: 随机生成用户登录行为: # 1 CREATE TABLE user_login AS 2 WITH RECURSIVE users(id) AS ( 3 SELECT 1 4 UNION ALL 5 SELECT id + 1 6 FROM users WHERE id \u0026lt;= 20 7 ) 8 SELECT u.id, login_time.login_time 9 FROM 10 (SELECT id FROM users) u, 11 LATERAL( 12 SELECT login_date.*, login_time.* 13 FROM 14 (SELECT date(generate_series(now() - '10 days'::INTERVAL, now(), '1 day')) login_date, (random()*6)::int login_per_day) login_date, 15 LATERAL( 16 SELECT * FROM 17 generate_series(login_date.login_date, login_date.login_date + '1 days'::INTERVAL, '1 hour') login_time 18 ORDER BY random() LIMIT login_date.login_per_day 19 ) login_time 20 ) login_time 21 ORDER BY login_time DESC; 附录-2: 生成订单数据： # 1 CREATE TABLE orders AS 2 SELECT *, round((100-random()*200)::NUMERIC, 2) amount 3 FROM generate_series(1, 20) id; 参考文献：\n10 SQL Tricks That You Didn’t Think How to Find the Closest Subset Sum with SQL 如何在 PostgreSQL 中正确使用 FETCH FIRST? 子集和问题 ","date":"2017-02-13","permalink":"/n3xtchen/2017/02/13/10-sql-tricks-that-you-didnt-think-were-possible/","section":"时间线","summary":"TLDR;","title":"你不知道的10个SQL杀手级特性"},{"content":" 什么是简单移动平均值 # 简单移动平均（英语：Simple Moving Average，SMA）是某变数之前n个数值的未作加权算术平均。例如，收市价的10日简单移动平均指之前10日收市价的平均数。\n直接看例子吧 # val df = List( (\u0026quot;站点1\u0026quot;, \u0026quot;2017-01-01\u0026quot;, 50), (\u0026quot;站点1\u0026quot;, \u0026quot;2017-01-02\u0026quot;, 45), (\u0026quot;站点1\u0026quot;, \u0026quot;2017-01-03\u0026quot;, 55), (\u0026quot;站点2\u0026quot;, \u0026quot;2017-01-01\u0026quot;, 25), (\u0026quot;站点2\u0026quot;, \u0026quot;2017-01-02\u0026quot;, 29), (\u0026quot;站点2\u0026quot;, \u0026quot;2017-01-03\u0026quot;, 27) ).toDF(\u0026quot;site\u0026quot;, \u0026quot;date\u0026quot;, \u0026quot;user_cnt\u0026quot;) import org.apache.spark.sql.expressions.Window import org.apache.spark.sql.functions._ val wSpec = Window.partitionBy(\u0026quot;site\u0026quot;) .orderBy(\u0026quot;date\u0026quot;) .rowsBetween(-1, 1) 这个 window spec 中，数据根据用户(customer)来分去。每一个用户数据根据时间排序。然后，窗口定义从 -1(前一行)到 1(后一行)\t，每一个滑动的窗口总用有3行\ndf.withColumn(\u0026quot;movingAvg\u0026quot;, avg(df(\u0026quot;user_cnt\u0026quot;)).over(wSpec)).show() 这段代码添加了一个新列，movingAvg，在滑动的窗口中使用了均值函数：\n+----+------------+--------+---------+ |site| date|user_cnt|movingAvg| +----+------------+--------+---------+ | 站点1|2017-01-01| 50| 47.5| | 站点1|2017-01-02| 45| 50.0| | 站点1|2017-01-03| 55| 50.0| | 站点2|2017-01-01| 25| 27.0| | 站点2|2017-01-02| 29| 27.0| | 站点2|2017-01-03| 27| 28.0| +----+----------+--------+---------+ 窗口函数和窗口特征定义 # 正如上述例子中，窗口函数主要包含两个部分：\n指定窗口特征（wSpec） \u0026ldquo;partitionyBY\u0026rdquo; 定义数据如何分组；在上面的例子中，他是用户 \u0026ldquo;orderBy\u0026rdquo; 定义分组中的排序 \u0026ldquo;rowsBetween\u0026rdquo; 定义窗口的大小 指定窗口函数函数 你可以使用 org.apache.spark.sql.functions 的“聚合函数（Aggregate Functions）”和”窗口函数（Window Functions）“类别下的函数 累计汇总 # val wSpec = Window.partitionBy(\u0026quot;site\u0026quot;) .orderBy(\u0026quot;date\u0026quot;) .rowsBetween(Long.MinValue, 0) df.withColumn(\u0026quot;cumSum\u0026quot;, sum(df(\u0026quot;user_cnt\u0026quot;)).over(wSpec)).show() .rowsBetween(Long.MinValue, 0) ：窗口的大小是按照排序从最小值到当前行\n+----+----------+--------+------+ |site| date|user_cnt|cumSum| +----+----------+--------+------+ | 站点1|2017-01-01| 50| 50| | 站点1|2017-01-02| 45| 95| | 站点1|2017-01-03| 55| 150| | 站点2|2017-01-01| 25| 25| | 站点2|2017-01-02| 29| 54| | 站点2|2017-01-03| 27| 81| +----+----------+--------+------+ 前一行数据 # val wSpec = Window.partitionBy(\u0026quot;site\u0026quot;) .orderBy(\u0026quot;date\u0026quot;) df.withColumn(\u0026quot;prevUserCnt\u0026quot;, lag(df(\u0026quot;user_cnt\u0026quot;), 1).over(wSpec)).show() lag(field, n): 就是取从当前字段往前第n个值，这里是取前一行的值\n+----+----------+--------+-----------+ |site| date|user_cnt|prevUserCnt| +----+----------+--------+-----------+ | 站点1|2017-01-01| 50| null| | 站点1|2017-01-02| 45| 50| | 站点1|2017-01-03| 55| 45| | 站点2|2017-01-01| 25| null| | 站点2|2017-01-02| 29| 25| | 站点2|2017-01-03| 27| 29| +----+----------+--------+-----------+ 如果计算环比的时候，是不是特别有用啊？！\n在介绍几个常用的行数：\nfirst/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到 lag/lead(field, n): lead 就是 lag 相反的操作，这个用于做数据回测特别用，结果回推条件 排名 # val wSpec = Window.partitionBy(\u0026quot;site\u0026quot;) .orderBy(\u0026quot;date\u0026quot;) df.withColumn(\u0026quot;rank\u0026quot;, rank().over(wSpec)).show() 这个数据在提取每个分组的前n项时特别有用，省了不少麻烦。\n+----+----------+--------+----+ |site| date|user_cnt|rank| +----+----------+--------+----+ | 站点1|2017-01-01| 50| 1| | 站点1|2017-01-02| 45| 2| | 站点1|2017-01-03| 55| 3| | 站点2|2017-01-01| 25| 1| | 站点2|2017-01-02| 29| 2| | 站点2|2017-01-03| 27| 3| +----+----------+--------+----+ ","date":"2017-01-24","permalink":"/n3xtchen/2017/01/24/spark200---window-function/","section":"时间线","summary":"什么是简单移动平均值 # 简单移动平均（英语：Simple Moving Average，SMA）是某变数之前n个数值的未作加权算术平均。例如，收市价的10日简单移动平均指之前10日收市价的平均数。","title":"Spark 实现简单移动平均值（SMA） - 窗口函数（Window Function）"},{"content":"一时兴起，开始 webpack2，配置成功，却是比前一个版本人性化了不少；接着添加 react + redux，结果遇到了一堆坑（一部分是之前遇到），于是有了这篇博客来记录此次踩坑过程。\n#1 Module build failed: SyntaxError: Unexpected token - 无法正常解析JSX # 先来看看错误：\nERROR in ./src/index.js Module build failed: SyntaxError: Unexpected token (4:2) 2 | 3 | ReactDOM.render( \u0026gt; 4 | \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt;, | ^ 5 | document.getElementById('root') 6 | ) 这个是我的 babel 配置:\n# package.json ... \u0026quot;babel\u0026quot;: { \u0026quot;presets\u0026quot;: [ \u0026quot;es2015\u0026quot; ] } ... 因为我的babel 转码规则（presents）中没有 react。解决方法：\n安装 react 转码规则\n$ yarn add -D babel-present-react 配置 babel：\n# package.json ... \u0026quot;babel\u0026quot;: { \u0026quot;presets\u0026quot;: [ \u0026quot;es2015\u0026quot;, \u0026quot;react\u0026quot;\t# 添加了这一行 ] } ... #2 error \u0026lsquo;React\u0026rsquo; is defined but never used no-unused-vars # 把错误解决了，紧接着 eslint 开始发难，下面是我报错的脚本:\nimport React from 'react' import { render } from 'react-dom' render( \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt;, document.getElementById('root') ) 为了解析 JSX，我们需要引入 React，这种类似 scala 隐式转换，把强迫症的 eslint 弄晕了，下面是我 eslint 的配置：\n# package.json ... \u0026quot;eslintConfig\u0026quot;: { \u0026quot;extends\u0026quot;: [ \u0026quot;eslint:recommended\u0026quot; ], \u0026quot;env\u0026quot;: { \u0026quot;browser\u0026quot;: true, \u0026quot;node\u0026quot;: true } }, ... 需要为 eslint 添加 react 规则。解决方法：\n安装 react 规则：\n$ yarn add -D eslint-plugin-react 修改 eslint 配置：\n# package.json ... \u0026quot;eslintConfig\u0026quot;: { \u0026quot;plugins\u0026quot;: [\u0026quot;react\u0026quot;],\t# 添加react规则插件 \u0026quot;extends\u0026quot;: [ \u0026quot;eslint:recommended\u0026quot;, \u0026quot;plugin:react/recommended\u0026quot;\t# 使用react规则 ], \u0026quot;env\u0026quot;: { \u0026quot;browser\u0026quot;: true, \u0026quot;node\u0026quot;: true } }, ... 你以为问题已经解决，还没完呢；\n#3 error Parsing error: The keyword \u0026lsquo;import\u0026rsquo; is reserved # 原因 es5 没有 import 这个关键词， eslint 还不知道通过 Babel 使用的 ES6 特性，于是作出如下修改：\n# package.json ... \u0026quot;eslintConfig\u0026quot;: { \u0026quot;parserOptions\u0026quot;: { \u0026quot;sourceType\u0026quot;: \u0026quot;module\u0026quot;, \u0026quot;ecmaFeatures\u0026quot;: { \u0026quot;jsx\u0026quot;: true } },\t# 添加了这几行 \u0026quot;plugins\u0026quot;: [ \u0026quot;react\u0026quot; ], \u0026quot;extends\u0026quot;: [ \u0026quot;eslint:recommended\u0026quot;, \u0026quot;plugin:react/recommended\u0026quot; ], \u0026quot;env\u0026quot;: { \u0026quot;browser\u0026quot;: true, \u0026quot;node\u0026quot;: true } }, ... 总算大功告成，🤝\n","date":"2017-01-23","permalink":"/n3xtchen/2017/01/23/webpack2-babel-react-eslint---error/","section":"时间线","summary":"一时兴起，开始 webpack2，配置成功，却是比前一个版本人性化了不少；接着添加 react + redux，结果遇到了一堆坑（一部分是之前遇到），于是有了这篇博客来记录此次踩坑过程。","title":"踩坑大回放：webpack2 + babel + react + eslint"},{"content":"首先，在 vim 中键入如下命令打开 NERDTree：\n:NERDTree 通过 ctrl + w 加上方向键切换到 NERDTree 工具条，键入 m：\nNERDTree Menu. Use j/k/enter and the shortcuts indicated ========================================================== \u0026gt; (a)dd a childnode\t# 一个节点，可以是文件或者文件夹 (m)ove the current node\t# 移动或者重命名当前节点 (d)elete the current node\t# 删除当前节点 (r)eveal in Finder the current node\t# 文件系统中打开当前节点 (o)pen the current node with system editor # 使用系统默认的编辑器打开 (q)uicklook the current node (c)opy the current node (l)ist the current node 现在讲解几个常用的操作 # 添加文件或者文件夹 # m 模式下，键入 a 进入如下界面：\nAdd a childnode ========================================================== Enter the dir/file name to be created. Dirs end with a '/'\t# 文件夹要以斜杠结尾 /private/tmp/test/1/[这里你输入你要的文件名] 重命名或者移动文件 # m 模式下，键入 m 进入如下界面：\nRename the current node ========================================================== Enter the new path for the node: /private/tmp/test/1/my\t# 这里修改你要名称或者路径 删除文件 # m 模式下，键入 d 进入如下界面：\nDelete the current node ========================================================== Are you sure you wish to delete the node: /private/tmp/test/1/my (yN): ","date":"2017-01-22","permalink":"/n3xtchen/2017/01/22/vim---nerdtree-operate-file/","section":"时间线","summary":"首先，在 vim 中键入如下命令打开 NERDTree：","title":"Vim - NERDTree 文件(夹)操作"},{"content":"一旦我打开多个 Tmux 窗口，我的 Vim 就变得超级慢。\n我的第一反应就是 tmux 在拖慢 vim 的数据，于是我迅速 Google 下确认它。\n我去掉所有的 Vim/Tmux 插件之后，开始挖掘 iTerm2 的配置，发现了这一行：\n移除掉 Save lines to scrollback in alternate screen mode 选项，并设置成合理的回滚行数（我设成了 1000），延迟几乎消失了。\n","date":"2017-01-18","permalink":"/n3xtchen/2017/01/18/osx---iterm2vim-slow/","section":"时间线","summary":"一旦我打开多个 Tmux 窗口，我的 Vim 就变得超级慢。","title":"MacOs—修复 iTerm2 和 Tmux/Vim 变慢的问题"},{"content":"","date":"2016-12-12","permalink":"/n3xtchen/tags/ggplot2/","section":"标签","summary":"","title":"ggplot2"},{"content":"我们先来看一段代码：\nlibrary(ggplot2) set.seed(100) d.sub \u0026lt;- diamonds[sample(nrow(diamonds), 500),] ggplot(data=d.sub, aes(x=carat,y=price)) + geom_point() 这是一段非常简单的 ggplot 绘图代码，是绘制的数据是钻石。当时如果我们想看钻石其他参数之间的关系，比如切面与价格，需要修改：\naes(x=cut, y=price) 粗看没什么问题，只是这么简单，但是如果你实现了一个模型，里面的特征特别多，你总不会每次想看其他特征，都要修改代码，这显然不科学，所以我们以传参的形式，把上面的代码改成如下：\n# filename: demo.R # Usage: Rscript demo.R \u0026quot;cut:x\u0026quot; \u0026quot;price:y\u0026quot; library(ggplot2) set.seed(100) d.sub \u0026lt;- diamonds[sample(nrow(diamonds), 500),] args \u0026lt;- commandArgs(T) x = args[1] y = args[2] ggplot(data=d.sub, aes_string(x=x,y=y))+ geom_point() 你注意到了吗？我使用 aes_string 替换 aes。因为 Rscript 传入的参数是字符串，我们要使用字符串映射变量。我可以在终端查看下 aes_string 的帮助文档：\n\u0026gt; library(\u0026quot;ggplot2\u0026quot;) \u0026gt; ?aes_string\t...此处省去无数行... Examples: # Three ways of generating the same aesthetics aes(mpg, wt, col = cyl) aes_(quote(mpg), quote(wt), col = quote(cyl)) aes_(~mpg, ~wt, col = ~cyl) aes_string(\u0026quot;mpg\u0026quot;, \u0026quot;wt\u0026quot;, col = \u0026quot;cyl\u0026quot;) 这几种形式是等价，但是如果我们想要参数化，只能使用 aes_string。\naes_string 的坑（变量名包含操作符或者语法符号） # 在使用过程成，如果你的变量名字符串不符合变量命名规则就会出现问题，如\n\u0026gt; ggplot(data=d.sub, aes_string(x=\u0026quot;cut\u0026quot;,y=\u0026quot;price(元)\u0026quot;)) Error in eval(expr, envir, enclos) : 没有\u0026quot;price\u0026quot;这个函数 比如出现操作符或者语法符号都会报类似的错误，怎么办呢？\n\u0026gt; ggplot(data=d.sub, aes_string(x=\u0026quot;cut\u0026quot;,y=\u0026quot;`price(元)`\u0026quot;)) 你只需要在字符两边添加反引号就好了。\n今天就是为了备忘这个坑，写了一堆东西。^_^，希望对大家有用！\n","date":"2016-12-12","permalink":"/n3xtchen/2016/12/12/r---ggplot-aes_string/","section":"时间线","summary":"我们先来看一段代码：","title":"GGplot2 - 参数化与 aes_string"},{"content":"","date":"2016-12-12","permalink":"/n3xtchen/categories/r/","section":"分类页","summary":"","title":"R"},{"content":"LATERAL 的用途，可以是 SELECT 中的结果作为条件，并把查询的结果，直接引用到 SELECT 子句中，先来看一个语句:\nselect variety, (select \u0026quot;close\u0026quot; from prices b where a.variety=b.variety order by cur_date limit 1 ) -- 嵌套 select 的子查询 from prices a group by variety; 这个语句用来查询每一个商品的最新报价，SELECT 中的子查询，我们称之为嵌套 SELECT 子查询（sub-queries in select）。\nLATERAL 关键词可以在前缀一个 SELECT FROM 子项. 这能让 SELECT 子项在 FROM 项出现之前就引用到 FROM 项中的列. (没有 LATERAL 的话, 每一个 SELECT 子项彼此都是独立的，因此不能够对其它的 FROM 项进行交叉引用.)\n一、 使用 LATERAL 获取分组的TopN\n建立测试表：\nCREATE TABLE test ( username TEXT, some_ts timestamptz, random_value INT4 ); 生成测试数据\nINSERT INTO test (username, some_ts, random_value) SELECT 'user #' || cast(floor(random() * 10) as int4), now() - '1 year'::INTERVAL * random(), cast(random() * 100000000 as INT4) FROM generate_series(1,2000000); 查询每个用户最近五条的随机值\nselect x.* from (select t.username from test t group by t.username order by username ) as t1, LATERAL( select t.* from test t where t.username=t1.username order by t.some_ts desc limit 5 ) as x; ","date":"2016-12-07","permalink":"/n3xtchen/2016/12/07/pgsql-lateral/","section":"时间线","summary":"LATERAL 的用途，可以是 SELECT 中的结果作为条件，并把查询的结果，直接引用到 SELECT 子句中，先来看一个语句:","title":"PostgreSQL 的 For Each 语句 - Lateral 联表"},{"content":"","date":"2016-12-07","permalink":"/n3xtchen/tags/sql/","section":"标签","summary":"","title":"sql"},{"content":"","date":"2016-12-06","permalink":"/n3xtchen/categories/java/","section":"分类页","summary":"","title":"Java"},{"content":"你肯定遇到过，开发新的应用的时候需要使用新版本，但是你想要为你原先的 Java 项目 保留旧的 Java 版本。似乎我需要一个类似 rbEnv 这样的工具（Ruby 版本切换工具）…… 祭出 Java 版的。\n通过 Google 搜索，最符合预期的就是 Jenv - 一个用来设置 JAVA_HOME 的命令行工具。\nOs X 下安装 # 前提，你已经安装了 HomeBrew（如果你还没有，可以参考 Mac 骇客指南 - 自动化配置）；\n首先，你需要安装多版本 Java：\nichexw → brew tap caskroom/versions ichexw → brew cask install java7 ichexw → brew cask install java\t# 安装当前最新的 Java 8 你可以使用各自版本的 JAVA_HOME:\nichexw → /usr/libexec/java_home -v 1.8 /Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Home ichexw → /usr/libexec/java_home -v 1.7 /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 第二步，安装今天的主角 Jenv：\nichexw → brew install jenv\t添加如下行到你的 Shell 启动脚本中：\nichexw → echo “# Init jenv\\nif which jenv \u0026gt; /dev/null; then eval \u0026quot;$(jenv init -)\u0026quot;; fi” \u0026gt;\u0026gt; ~/.bash_profile // 我使用的是，这里使用的就是 ～/.zshrc 第三步，配置 Jenv：\njEnv 不能安装 JDK，所以我们需要告诉 Jenv 去哪里寻找 JAVA。键入如下命令，把我们安装 JDK 注册到 Jenv 中：\nichexw → jenv add /Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Home ichexw → jenv add /Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Home 最后，你运行如下命令，查看登记的版本：\nichexw → jenv versions * system (set by /Users/ichexw/.jenv/version) 1.7 1.7.0.80 1.8 1.8.0.112 oracle64-1.7.0.80 oracle64-1.8.0.112 星号说明是当前使用的版本\n使用 # 首先，我们先设置全局的 JDK 版本，因为我们大部分项目使用 1.7，所以我们就把全局设置成 1.7 版本的：\nichexw → jenv global oracle64-1.7.0.80 然后，我在我的 Java8 的项目目录中，设置本地 JDK 模式：\nichexw → jenv local oracle64-1.8.0.112 你会在你的项目跟目录中找到 .java-version 的文件：\nichexw → cat .java-version oracle64-1.8.0.112 你只要进入这个目录，就会自动把 JDK 设置成 Java8 的，是不是智能。而且，它不会拖慢 Shell。\n之前有使用一个国人的项目 linux-china/jenv，他也是改写 jEnv，还增加了安装的功能，很强大，把 Shell 弄的好慢，果断弃用。\n另外，Jenv 也适用于 Unix/Linux，只是没有那么方便的安装命令，有时间我也分享一下。\n祝大家玩的愉快，免受版本之苦。\n引用自： * Install Multiple Java Versions on Mac * jEnv 项目主页\n","date":"2016-12-06","permalink":"/n3xtchen/2016/12/06/java---jenv/","section":"时间线","summary":"你肯定遇到过，开发新的应用的时候需要使用新版本，但是你想要为你原先的 Java 项目 保留旧的 Java 版本。似乎我需要一个类似 rbEnv 这样的工具（Ruby 版本切换工具）…… 祭出 Java 版的。","title":"Java 多版本共存工具 - Jenv"},{"content":"","date":"2016-12-06","permalink":"/n3xtchen/tags/version/","section":"标签","summary":"","title":"version"},{"content":"有时候，我需要从 Github 上克隆一个超级大的代码项目，我发现获取的数据超级慢（Kb/s）。\n对于快速克隆，我有两个诀窍：\n杀掉命令，重试一边。这么做几次后，看看是不是会的到一个快一点的连接。这个在大部分情况下，是有效的。\n先获取最新修改版本，然后获取剩下的：\nichexw → git clone --depth=1 git@github.com:n3xtchen/hello-world.git ichexw → cd hello-world ichexw → git fetch --unshallow 参考： Slow speed on git clone\n","date":"2016-12-05","permalink":"/n3xtchen/git/2016/12/05/git-clone-so-slow/","section":"时间线","summary":"有时候，我需要从 Github 上克隆一个超级大的代码项目，我发现获取的数据超级慢（Kb/s）。","title":"Git - 拉取（Git Clone）代码太慢了"},{"content":"","date":"2016-12-04","permalink":"/n3xtchen/categories/node/","section":"分类页","summary":"","title":"Node"},{"content":"如果以为只是简简单单的 npm list -g ，那你就 too young too simple。\n发现查看 Node 的时候，都是一溜一大串的列表，这些信息感觉一点用都没有，和 ES 回调地狱一般，哪个 Package 不依赖几个包，而安装的时候却不取引用其他包装过的依赖，\n看看我的安装的包，我只装了 http-server，但是查看我的安装包名，天啦，居然有 289 行\n➜ ichexw ~ npm list -g | wc -l 289 这些信息对于我们来说，大部分情况是没用，那我们怎么查看我手动安装的，而不包含它们自行安装的依赖：\n➜ ichexw ~ npm list -g --depth 0 /usr/local/lib ├── http-server@0.9.0 └── npm@3.10.9 这是是不是大部分想要看的呢？如果是，你就把它收下吧，我肯定会常用这个命令的。\n引用自 StackOverflow - How to list npm user-installed packages?\n","date":"2016-12-04","permalink":"/n3xtchen/2016/12/04/node---list-package-without-dep/","section":"时间线","summary":"如果以为只是简简单单的 npm list -g ，那你就 too young too simple。","title":"Node - 查看自己安装的包"},{"content":"","date":"2016-12-04","permalink":"/n3xtchen/tags/npm/","section":"标签","summary":"","title":"npm"},{"content":"如果在编译代码的时候出现：\nXX: Unsupported major.minor version 52.0 说明的使用的 JDK 版本不兼容，52.0 代表就是对应的 Java 版本，下面是常见 java 版本对应的 Code：\nJ2SE 8 = 52 J2SE 7 = 51 J2SE 6.0 = 50 J2SE 5.0 = 49 JDK 1.4 = 48 JDK 1.3 = 47 JDK 1.2 = 46 JDK 1.1 = 45 这时，你需要做的就是下载安装指定版本，并配置你的环境到该版本，重新编译的代码，这是就能够成功编译了。\n参考自\nUnsupported major.minor version 52.0 [duplicate] Java_class_file ","date":"2016-12-03","permalink":"/n3xtchen/2016/12/03/java---unsupported-majorminor-version-520/","section":"时间线","summary":"如果在编译代码的时候出现：","title":"Java 版本不兼容 - Unsupported major.minor version 52.0"},{"content":"由于 Os X 默认浏览器 Safari 下，Jupyter 的样式会有异常，只好切换到 Chrome 上了。\n生成用户根目录下的配置文件，如果你在的用户根目录下能找到这个文件 ~/.jupyter/jupyter_notebook_config.py，则忽略此步骤；默认情况下，木有这个文件的，命令如下：\n$ jupyter notebook --generate-config 修改 ~/.jupyter/jupyter_notebook_config.py 配置表中的 c.NotebookApp.browser 值，我使用的是 Chrome：\n... # 我的这个配置在第86行 c.NotebookApp.browser = 'open -a /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome %s' # c.NotebookApp.browser = 'open -a {这里替换成你要使用的浏览器app所在路径} %s' ... 测试你的打开浏览器的脚本是否可用 # $ open -a {这里替换成你要使用的浏览器app所在路径} 这是浏览器会自动弹出，否则你得检查一下的浏览器安装路径是不是写错（应该不可能没装好吧，^_^）\n搞定，你就可以用你最喜欢的浏览器上开发了\n参考： Changing browser for IPython Notebook from system default\n","date":"2016-12-03","permalink":"/n3xtchen/2016/12/03/jupyter---default-browser/","section":"时间线","summary":"由于 Os X 默认浏览器 Safari 下，Jupyter 的样式会有异常，只好切换到 Chrome 上了。","title":"Jupyter: Os X 下修改默认打开的浏览器"},{"content":"","date":"2016-12-03","permalink":"/n3xtchen/tags/r/","section":"标签","summary":"","title":"R"},{"content":"","date":"2016-12-03","permalink":"/n3xtchen/tags/sbt/","section":"标签","summary":"","title":"sbt"},{"content":"如果你遇上如下错误：\n[info] Compilation completed in 16.154 s [warn] Class java.time.Duration not found - continuing with a stub. [warn] Class java.time.Duration not found - continuing with a stub. [warn] there were 2 feature warning(s); re-run with -feature for details [warn] three warnings found [warn] Multiple main classes detected. Run 'show discoveredMainClasses' to see the list 说明你的依赖缺失了，我使用的是 sbt， 在依赖列表中添加如下依赖\n\u0026quot;org.joda\u0026quot; % \u0026quot;joda-convert\u0026quot; % \u0026quot;1.2\u0026quot; 现在他就不再报错了。\n参考自： class-broken-error-with-joda-time-using-scala\n","date":"2016-12-03","permalink":"/n3xtchen/2016/12/03/scale---warn-class-javatimeduration-not-found/","section":"时间线","summary":"如果你遇上如下错误：","title":"Scale - [warn] Class java.time.Duration not found"},{"content":" 第一步：生成 Key # 在你的机子上，使用 ssh-keygen 生成 RSA 私钥（你已经做了，可以跳过这一步）\n$ ssh-keygen -t rsa Generating public/private rsa key pair. # 这一步一般不修改，直接Enter使用默认值，否则你需要输入路径 Enter file in which to save the key ({你的用户根目录}/.ssh/id_rsa): # 这一步很重要，以为本文的主题是无密码登录，所以就直接回车 Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/username/.ssh/id_rsa Your public key has been saved in {你的用户根目录}/.ssh/id_sra.pub The key fingerprint is: ar:bc:d3:9e:g3:1f:63:6f:6b:32:2e:97:ee:42:e1:be n3xtchen@aybe.me The key’s randomart image is: +--[ RSA 2048]----+ | ..+**B.o++o | | . o+==o. o | | . .oo.= | | . +E+ . | | S . | | | | | | | | | +-----------------+ 第二步：部署你的公钥到你需要登录的服务器上 # Linux 下的步骤 # $ ssh-copy-id -i ~/.ssh/id_rsa.pub username@hostname /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed == if you are prompted now it is to install the new keys username@hostname's password:\t# 输入服务器密码 Number of key(s) added: 1 Os X 下的步骤 # $ cat ~/.ssh/id_rsa.pub | ssh username@server.dreamhost.com \u0026quot;mkdir ~/.ssh; cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026quot; The authenticity of host 'server.dreamhost.com (208.113.136.55)' can't be established. RSA key fingerprint is 50:46:95:5f:27:c9:fc:f5:f5:32:d4:3a:e9:cb:4f:9f. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'm.aybe.me, 0.0.0.0' (RSA) to the list of known hosts. username@hostname's password: 步骤三：验证 # 必须确认文件权限：\n服务器上的 .ssh 目录必须是 700 你的机子的 .ssh 目录必须是 600 然后使用命令登录：\n$ ssh username@hostname 大功告成！🍻\n参考文献\n* https://help.dreamhost.com/hc/en-us/articles/216499537-How-to-configure-passwordless-login-in-Mac-OS-X-and-Linux * https://coolestguidesontheplanet.com/make-passwordless-ssh-connection-osx-10-9-mavericks-linux/\n","date":"2016-10-22","permalink":"/n3xtchen/2016/10/22/ssh---passwordless-login/","section":"时间线","summary":"第一步：生成 Key # 在你的机子上，使用 ssh-keygen 生成 RSA 私钥（你已经做了，可以跳过这一步）","title":"SSH - 无密码登录"},{"content":"前一段时间，发现 homebrew 怎么都不更新了。每天都有很多的更新，正和小伙伴们讨论是不是这个项目快要去了，Σ（ﾟдﾟlll）\n于是上它的 github 看了下，原来是出 bug（估计是哪位大神不小心，没测试 PR 代码就合并上来了）。今天有空就做个搬运工，希望对大家有帮助。\n解决方案就在她们的 项目首页：\nIf Homebrew was updated on Aug 10-11th 2016 and brew update always says Already up-to-date. you need to run:\ncd \u0026quot;$(brew --repo)\u0026quot; \u0026amp;\u0026amp; git fetch \u0026amp;\u0026amp; git reset --hard origin/master \u0026amp;\u0026amp; brew update 翻译如下：\n如果你的 Homebrew 在 2016 年 8 月 10-11 号更新的，并且 brew update 永远都是 up-to-date（连着好几天都这样，作为 重度OCD患者 来说，真是伤不起），你则需要运行如下命令：\ncd \u0026quot;$(brew --repo)\u0026quot; \u0026amp;\u0026amp; git fetch \u0026amp;\u0026amp; git reset --hard origin/master \u0026amp;\u0026amp; brew update 然后你的 Mac 就可以每天愉快地接收更新了。\n就到这里。。。^_^\n","date":"2016-09-17","permalink":"/n3xtchen/2016/09/17/brew---bug-on-update-2016-08-10/","section":"时间线","summary":"前一段时间，发现 homebrew 怎么都不更新了。每天都有很多的更新，正和小伙伴们讨论是不是这个项目快要去了，Σ（ﾟдﾟlll）","title":"Homebrew: 解决无法更新问题(2016-08-10)"},{"content":"","date":"2016-09-06","permalink":"/n3xtchen/tags/hg/","section":"标签","summary":"","title":"hg"},{"content":" 检出代码 # $ hg checkout http://url 更新代码 # $ hg pull $ hg update 回滚代码 # 代码合并 # 创建分支 # 合并分支 # 添加和修改远端服务器 # .hgignore # ","date":"2016-09-06","permalink":"/n3xtchen/2016/09/06/hg-tutorial/","section":"时间线","summary":"检出代码 # $ hg checkout http://url 更新代码 # $ hg pull $ hg update 回滚代码 # 代码合并 # 创建分支 # 合并分支 # 添加和修改远端服务器 # .","title":"hg 简易教程"},{"content":"","date":"2016-09-06","permalink":"/n3xtchen/categories/version-control/","section":"分类页","summary":"","title":"Version-Control"},{"content":"","date":"2016-09-04","permalink":"/n3xtchen/tags/database/","section":"标签","summary":"","title":"database"},{"content":"闲的无事，满试着总结归纳下：\n类型1：JDBC-ODBC桥：这种类型的驱动把所有JDBC的调用传递给ODBC，再让后者调用数据库本地驱动代码 类型2：本地API驱动，这种类型的驱动通过客户端加载数据库厂商提供的本地代码库（C／C++等）来访问数据库，而在驱动程序中则包含了Java代码； 类型3：网络协议驱动，这种类型的驱动给客户端提供了一个网络API，客户端上的JDBC驱动程序使用套接字（Socket）来调用服务器上的中间件程序，后者在将其请求转化为所需的具体API调用； 类型4：本地协议驱动，这种类型的驱动使用Socket，直接在客户端和数据库间通信。 类型越往后，中间件越少，限制越少，越灵活，而速度则越快\n","date":"2016-09-04","permalink":"/n3xtchen/2016/09/04/jdbc-type/","section":"时间线","summary":"闲的无事，满试着总结归纳下：","title":"JDBC 的驱动类型"},{"content":"","date":"2016-08-31","permalink":"/n3xtchen/tags/dba/","section":"标签","summary":"","title":"dba"},{"content":"长时间运行的查询会影响整体数据库的性能，它们可能停留在一些后台进程中。尤其当遇上表锁的时候，就更蛋疼了，于是有了下面的文章。\n我们可以使用下面语句来查询长时间的运行的查询：\ndb_guard=# SELECT db_guard-# pid, db_guard-# now() - pg_stat_activity.query_start AS duration, db_guard-# query, db_guard-# state db_guard-# FROM pg_stat_activity db_guard-# WHERE now() - pg_stat_activity.query_start \u0026gt; interval '5 minutes'; pid | duration | query | state -----+----------+---------------------+------- (1 rows) 30036| 300 | select * from ..... | idle in transaction 这里的 pid 就是系统中的进程 ID。\n如果它的状态是 idle，那你就不用 care，但是活动状态下查询还是会对性能产生影响的。\n下面撤销该查询：\ndb_guard=# SELECT pg_cancel_backend(30036);\t然后，我们可能需要花些时间等待它停下来。\n如果发现程序卡住，那我们只能祭出杀手锏（杀伤力过大，慎用！）：\ndb_guard=# SELECT pg_terminate_backend(30036); 为什么我们一步到位呢？ 因为 pg_cancel_backend 比 pg_terminate_backend 更安全，就好和使用 kill 和 kill -9 的区别，给它时间善后，避免直接中断导致数据丢失或损坏。\nSA 的处理方式 # 既然之前提到了系统进程，我这边提供另一种方式关闭方式\n首先，我们从进程查看一下：\n$ ps -ef | grep 30036 root 24780 5448 0 10:42 pts/5 00:00:00 grep postgres postgres 30036 8535 0 Aug08 ? 00:10:13 postgres: admin db_guard 10.xx.xxx.xx(26289) idle in transaction\n这里还提供了登陆的用户(admin)，数据库(db_guard)，客户机的IP(10.xx.xxx.xx)，以及查询状态(idle in transaction)\n现在开始料理：\n$ kill 30036\n如果不灵，只能强杀了：\n$ kill -9 30036\n","date":"2016-08-31","permalink":"/n3xtchen/2016/08/31/kill-postgresql/","section":"时间线","summary":"长时间运行的查询会影响整体数据库的性能，它们可能停留在一些后台进程中。尤其当遇上表锁的时候，就更蛋疼了，于是有了下面的文章。","title":"如何查找和杀掉 PostgreSQL 中长时间运行的查询"},{"content":"","date":"2016-08-30","permalink":"/n3xtchen/categories/javascript/","section":"分类页","summary":"","title":"Javascript"},{"content":"","date":"2016-08-30","permalink":"/n3xtchen/tags/jquery/","section":"标签","summary":"","title":"Jquery"},{"content":"","date":"2016-08-30","permalink":"/n3xtchen/tags/json/","section":"标签","summary":"","title":"json"},{"content":"让我们先从错误的姿势开始：\nvar json = {\u0026quot;name\u0026quot;: \u0026quot;I have space\u0026quot;}; $.post('some_url_need_json', json) 首先这两条语句有两个细节需要注意\n默认，POST 的 Content-Type 长这样：\nContent-Type: application/x-www-form-urlencoded; charset=UTF-8 JSON 在发送的过程中会被 URLEncode 掉，变成这样:\nname＝I+have+space 当然，前后端都是自己部门开发，相互协调都不会出现问题；但是，记住但是如果使用外包或者开源组件的就会把你坑到死。\n为了严谨（借用我一同事的名言）， 我们要这么做：\n$.ajaxSetup({contentType: \u0026quot;application/json; charset=utf-8\u0026quot;}); $.post('some_url_need_json', JSON.stringify(json)) $.ajaxSetup: 配置请求的头信息； JSON.stringify: 避免发送的内容被 URLEncode 掉。 ","date":"2016-08-30","permalink":"/n3xtchen/2016/08/30/jquery-post-json/","section":"时间线","summary":"让我们先从错误的姿势开始：","title":"使用 Jquery 发送 Json 数据的正确姿势"},{"content":"发现每次使用 py.test 的时候，总是记不住它的命令，每次都得查一遍文档，所以写这篇博客备忘下。\n指定测试范围 # py.test test_mod.py # 运行这个文件下的所有测试 py.test somepath # 运行这个路径下的所有测试文件 py.test -k stringexpr # 只测试与 stringexpr 匹配的测试 py.test test_mod.py::test_func # 测试指定测试文件下的测试函数 py.test test_mod.py::TestClass::test_method # 测试指定测试文件下的指定测试类的测试方法 py.test --pyargs pkg\t＃ 测试 pkg 文件夹下所有的测试 py.test -k 详解 # py.test -k \u0026quot;method_a or method_b\u0026quot;\t测试类或函数包含 method_a 或 method_b 中的测试将被运行\npy.test -k \u0026quot;SomeClass and no method_a\u0026quot; 测试类名包含 SomeClass，并且该测试类中包含 method_a 将被跳过\n获取程序输出 # py.test -s\t# = capature=no，将不捕获输出，直接打印 ","date":"2016-07-17","permalink":"/n3xtchen/2016/07/17/pytest---commandline/","section":"时间线","summary":"发现每次使用 py.","title":"py.test - 常见的命令"},{"content":"","date":"2016-07-17","permalink":"/n3xtchen/tags/test/","section":"标签","summary":"","title":"test"},{"content":"为了统一管理配置项，我在真是操碎了心啊。现在我分享下这两天的研究成果。\n首先先介绍下 typesafe.config，Scala 语言下一个流行的配置管理库，由 Lightend（前身是 typesafe，Scala 编程语言的发明者）公司开发的。所以它的流行自然就不言而喻了。它的项目地址： https://github.com/typesafehub/config。\n安装和使用 typesafe.config # 在你的 sbt 依赖中添加如下：\nbraryDependencies += \u0026quot;com.typesafe\u0026quot; % \u0026quot;config\u0026quot; % \u0026quot;1.3.0\u0026quot; 这个版本你需要关注，如果你的 Java 1.6 及以下，其版本就是 1.2.1；1.3.0 则是为 java 8 构建的\n下面是演示代码：\nimport com.typesafe.config.ConfigFactory val conf = ConfigFactory.load(); int bar1 = conf.getInt(\u0026quot;foo.bar\u0026quot;); Config foo = conf.getConfig(\u0026quot;foo\u0026quot;); int bar2 = foo.getInt(\u0026quot;bar\u0026quot;); 在这里，我就不对用法进行详细介绍，自行 google 或者看官方 API（它的 README.md 将的已经足够详细了）。\n可选的覆盖配置方案 # 这几天我一直思考一个问题：在不同环境下，如何能够自动切换配置，而减少上线和调试成本，降低配置错误带来的风险。目前有如下几个方案：\nJava System Properties # Java 系统属性，通过 -D 标签传递给命令行来达到覆盖配置的目的：\njava -Dsys_args=value com.cyou.fz.config 如果需要的覆盖的参数占少数，那么这个方式是一个不错的方案，但是配置一多就蛋碎了一地，举个例子：\njava -Ddb1=0.0.0.0/db -Ddb1_pass=u -Ddb1_pass \\ -Ddb2=0.0.0.0/db -Ddb2_pass=u -Ddb2_pass \\ -Ddb3=0.0.0.0/db -Ddb3_pass=u -Ddb3_pass \\ -Ddb4=0.0.0.0/db -Ddb3_pass=u -Ddb4_pass \\ com.cyou.fz.config 用这种方式来传递多个数据配置，是不是很蛋疼啊；总结下优缺点：\n优点：\n需要传递命令行 flag，操作起来简便 缺点：\n如果需要覆盖的配置数量多（个人认为，超过 5 个就不适用了） 使用环境变量 # 现在来看看我们的 HOCON 配置：\napp { db { host = localhost\t// 默认参数 host = ${?DB_PORT_3306_TCP_ADDR}\t// 环境变量 DB_PORT_3306_TCP_ADDR 有设置，将会替换 host 的值 port = \u0026quot;3306\u0026quot; port = ${?DB_PORT_3306_TCP_PORT} } } 就不过多解释了，自己看注释。\nHOCON 中使用 include # 首先我们将系统配置以文件形式存储和统一管理，下面是的数据库连接串管理：\n# 生产环境数据库配置 # 路径 /usr/local/etc/db.conf db { jdbcUrl = \u0026quot;jdbc:mysql://0.0.0.0:3306/MyDatabase\u0026quot; user = \u0026quot;dba\u0026quot; pass = \u0026quot;pass\u0026quot; } 这时，你只需要在你的 resoures 中的配置文件引入我们公用的配置：\n# src/main/scala/resources/application.conf include \u0026quot;/usr/local/etc/db.conf\u0026quot; 测试下：\nscala\u0026gt; val conf = ConfigFactory.load(); Iconf: com.typesafe.config.Config = ... scala\u0026gt; conf.getString(\u0026quot;db.jdbcUrl\u0026quot;) res0: String = jdbc:mysql://0.0.0.0:3306/MyDatabase 这样子，你就可以在的开发环境，测试环境的同一个路径下配置自己的数据连接，就很方便。\n另外，另外你觉得路径太长太丑，你也可以使用：\ninclude classpaht(\u0026quot;db.conf\u0026quot;) 你要把你的配置文件追加到 classpath 中，这边，我需要这么操作：\nexport CLASSPATH = /usr/local/etc/:$CLASSPATH 这样，测试结果同上。\n你还可以通过 web 服务，来统一管理配置，那么你就可以直接试用 Url 引入配置：\ninclude url(\u0026quot;http://0.0.0.0/db.conf\u0026quot;) 结语 # 这里，提供了三种配置管理的方法，任君挑选。个人更偏向于最后一种；第一种的弊端已经讲过了，置于第二种，如果没有线上环境变量的配置权限，就无法操作了。最后一种，你只要有目录的操作权限，就可以轻松部署了。\n","date":"2016-07-14","permalink":"/n3xtchen/2016/07/14/scala---typesafe-config/","section":"时间线","summary":"为了统一管理配置项，我在真是操碎了心啊。现在我分享下这两天的研究成果。","title":"Scala - 使用 typesafe.config 管理你的配置文件"},{"content":"1: val bools = Seq(true, false) 2: for (bool in bools) { 3:\tbool match { 4:\tcase true =\u0026gt; println(\u0026quot;真的\u0026quot;) 5:\tcase false =\u0026gt; println(\u0026quot;假的\u0026quot;) 6:\t} 7: } 这个就是我们所说的模式匹配；看起来很像 C 风格，但是不一样，尤其要记住：=\u0026gt;，我经常把它写成 :。\n现在讲第一个特性：需要处理对每个匹配规矩。\n怎么说？我们把第 5 行注释掉，编译下：\n\u0026lt;console\u0026gt;:12: warning: match may not be exhaustive. It would fail on the following input: false bool match { ^ 真的 scala.MatchError: false (of class java.lang.Boolean) at .\u0026lt;init\u0026gt;(\u0026lt;console\u0026gt;:11) at .\u0026lt;clinit\u0026gt;(\u0026lt;console\u0026gt;) ... 这个警告告诉我们，没有穷尽匹配，意思就是要我们处理所有可能被匹配的可能；说明 Scala 是相当严谨的。\n匹配值，类型以及变量： # for { x \u0026lt;- Seq(1, 2, 2.7, \u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;, 'four } { val str = x match { case 1 =\u0026gt; \u0026quot;int 1\u0026quot; case _: Int =\u0026gt; \u0026quot;other int: \u0026quot;+x case _: Double =\u0026gt; \u0026quot;a double: \u0026quot;+x case \u0026quot;one\u0026quot; =\u0026gt; \u0026quot;string one\u0026quot; case _: String =\u0026gt; \u0026quot;other string: \u0026quot;+x case _ =\u0026gt; \u0026quot;unexpected value: \u0026quot; + x } println(str) } 下面是输出：\nint 1 other int: 2 a double 2.7 string one other string: two unexpected value: 'four 这个很简单，自己领悟下就好。下面讲一个难点，如果要匹配一个变量的值的话，你怎么做：\n1: def checkY(y: Int) { 2:\tfor { 3:\tx \u0026lt;- Seq(99, 100, 101) 4:\t} { 5:\tval str = x match { 6:\tcase y =\u0026gt; \u0026quot;found Y\u0026quot; 7:\tcase i: Int =\u0026gt; \u0026quot;int:\u0026quot; + i 8:\t} 9:\tprintln(str) 10:\t} 11:} checkY(100) 下面是输出：\n\u0026lt;console\u0026gt;:12: warning: patterns after a variable pattern cannot match (SLS 8.1.1) If you intended to match against parameter y of method checkY, you must use\t# 我加的：如果你你想要匹配参数 y 的值，你必须使用反引号 backticks, like: case `y` =\u0026gt; case y =\u0026gt; \u0026quot;found y!\u0026quot; ^ \u0026lt;console\u0026gt;:13: warning: unreachable code due to variable pattern 'y' on line 12 case i: Int =\u0026gt; \u0026quot;int: \u0026quot;+i ^ \u0026lt;console\u0026gt;:13: warning: unreachable code case i: Int =\u0026gt; \u0026quot;int: \u0026quot;+i ^ checkY: (y: Int)Unit found y! found y! found y! 如果是初学者，这个坑肯定躺过。这里又要夸一下 Scala 智能了，报错的信息非常有参考价值。编译器已经猜到你的意图了(认真看我的错误注释)。\n注意： # 在 case 语句中，小写字符开头的词会被假设一个新变量名，存储提取出来的值。为了引用之前定义的变量，你需要用反引号。相反，如果一个单词首字母大写，Scala 会把它当作一种类型名。\n你只需要将第6行的 y 替换成 `y` 即可。\n你在运行一次你的程序：\nint: 99 found y! int: 101 最后，我们还可以在一个 case 语句匹配多个。为了避免重复，我们可以使用 or 或者 ｜:\nfor { x \u0026lt;- Seq(1, 2, 2.7, \u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;, 'four) } { val str = x match { case _: Int | _: Double =\u0026gt; \u0026quot;a number:\u0026quot; + x\t# 注意看这一行 case \u0026quot;one\u0026quot; =\u0026gt; \u0026quot;string one\u0026quot; case _: String =\u0026gt; \u0026quot;other string: \u0026quot; + x case _ =\u0026gt; \u0026quot;unexpected value: \u0026quot; + x } println(str) } 匹配 Seq # 首先，Seq ，List， Vector 都是 Seq 的子类，仅有它们可以使用此类模式匹配\nval nonEmptySeq = Seq(1, 2, 3, 4, 5) val emptySeq = Seq.empty[Int]\t# 空 Seq val nonEmptyList = List(1, 2, 3, 4, 5)\tval emptyList = Nil\t# 空列表 val nonEmptyVector = Vector(1, 2, 3, 4, 5)\tval emptyVector = Vector.empty[Int]\t# 空向量 val nonEmptyMap = Map(\u0026quot;one\u0026quot; -\u0026gt; 1, \u0026quot;two\u0026quot; -\u0026gt; 2, \u0026quot;three\u0026quot; -\u0026gt; 3)\tval emptyMap = Map.empty[String,Int] def seqToString[T](seq: Seq[T]): String = seq match {\tcase head +: tail =\u0026gt; s\u0026quot;$head +: \u0026quot; + seqToString(tail)\tcase Nil =\u0026gt; \u0026quot;Nil\u0026quot; } for (seq \u0026lt;- Seq(\tnonEmptySeq, emptySeq, nonEmptyList, emptyList, nonEmptyVector, emptyVector, nonEmptyMap.toSeq, emptyMap.toSeq)) { println(seqToString(seq)) } 需要注意的是，对于 Map 类型，我们需要使用 toSeq，因为它不是 Seq 的子类。 下面是输出：\n1 +: 2 +: 3 +: 4 +: 5 +: Nil Nil1 +: 2 +: 3 +: 4 +: 5 +: Nil Nil 1 +: 2 +: 3 +: 4 +: 5 +: Nil Nil (one,1) +: (two,2) +: (three,3) +: Nil Nil 直接看代码就好了。\n元组匹配 # val langs = Seq( (\u0026quot;Scala\u0026quot;, \u0026quot;Martin\u0026quot;, \u0026quot;Odersky\u0026quot;), (\u0026quot;Clojure\u0026quot;, \u0026quot;Rich\u0026quot;, \u0026quot;Hickey\u0026quot;), (\u0026quot;Lisp\u0026quot;, \u0026quot;John\u0026quot;, \u0026quot;McCarthy\u0026quot;)) for (tuple \u0026lt;- langs) { tuple match { case (\u0026quot;Scala\u0026quot;, _, _) =\u0026gt; println(\u0026quot;Found Scala\u0026quot;) // case (lang, first, last) =\u0026gt; // println(s\u0026quot;Found other language: $lang ($first, $last)\u0026quot;) } } 在 case 中使用 Guards # for (i \u0026lt;- Seq(1,2,3,4)) { i match { case _ if i%2 == 0 =\u0026gt; println(s\u0026quot;even: $i\u0026quot;)\tcase _ =\u0026gt; println(s\u0026quot;odd: $i\u0026quot;)\t} } 下面是输出：\nodd: 1 even: 2 odd: 3 even: 4 case class 匹配 # case class Address(street: String, city: String, country: String) case class Person(name: String, age: Int, address: Address) val alice = Person(\u0026quot;Alice\u0026quot;, 25, Address(\u0026quot;1 Scala Lane\u0026quot;, \u0026quot;Chicago\u0026quot;, \u0026quot;USA\u0026quot;)) val bob = Person(\u0026quot;Bob\u0026quot;, 29, Address(\u0026quot;2 Java Ave.\u0026quot;, \u0026quot;Miami\u0026quot;, \u0026quot;USA\u0026quot;)) val charlie = Person(\u0026quot;Charlie\u0026quot;, 32, Address(\u0026quot;3 Python Ct.\u0026quot;, \u0026quot;Boston\u0026quot;, \u0026quot;USA\u0026quot;)) for (person \u0026lt;- Seq(alice, bob, charlie)) { person match { case Person(\u0026quot;Alice\u0026quot;, 25, Address(_, \u0026quot;Chicago\u0026quot;, _) =\u0026gt; println(\u0026quot;Hi Alice!\u0026quot;) case Person(\u0026quot;Bob\u0026quot;, 29, Address(\u0026quot;2 Java Ave.\u0026quot;, \u0026quot;Miami\u0026quot;, \u0026quot;USA\u0026quot;)) =\u0026gt; println(\u0026quot;Hi Bob!\u0026quot;) case Person(name, age, _) =\u0026gt; println(s\u0026quot;Who are you, $age year-old person named $name?\u0026quot;) } } 这是输出:\nHi Alice! Hi Bob! Who are you, 32 year-old person named Charlie? 正则表达式匹配 # val BookExtractorRE = \u0026quot;\u0026quot;\u0026quot;Book: title=([^,]+),\\s+author=(.+)\u0026quot;\u0026quot;\u0026quot;.r val item = \u0026quot;Book: title=Programming Scala Second Edition, author=Dean Wampler\u0026quot; println(item match { case BookExtractorRE(title, author) =\u0026gt; println(s\u0026quot;\u0026quot;\u0026quot;Book \u0026quot;$title\u0026quot;, written by $author\u0026quot;\u0026quot;\u0026quot;) case entry =\u0026gt; println(s\u0026quot;Unrecognized entry: $entry\u0026quot;) }) 输出:\nBook \u0026quot;Programming Scala Second Edition\u0026quot;, written by Dean Wampler 变量绑定 # 如果你在做模式匹配的时候，除了需要提取匹配的内容，还需要被匹配对象本身的时候，你可以这么处理（以上一个例子的基础）：\nprintln(item match { case b @ BookExtractorRE(title, author) =\u0026gt; { println(s\u0026quot;\u0026quot;\u0026quot;原始：$b\u0026quot;\u0026quot;\u0026quot;) println(s\u0026quot;\u0026quot;\u0026quot;格式化后:Book \u0026quot;$title\u0026quot;, written by $author\u0026quot;\u0026quot;\u0026quot;) } case entry =\u0026gt; println(s\u0026quot;Unrecognized entry: $entry\u0026quot;) }) 输出：\n原始：Book: title=Programming Scala Second Edition, author=Dean Wampler 格式化后:Book \u0026quot;Programming Scala Second Edition\u0026quot;, written by Dean Wampler 其它常用场景 # 提取 case class：\nscala\u0026gt; case class Person(name: String, age: Int) defined class Person scala\u0026gt; val Person(name, age) = Person(\u0026quot;Dean\u0026quot;, 29) name: String = Dean age: Int = 29 集合匹配：\nscala\u0026gt; val head +: tail = List(1,2,3) head: Int = 1 tail: List[Int] = List(2, 3) scala\u0026gt; val head1 +: head2 +: tail = Vector(1,2,3) head1: Int = 1 head2: Int = 2 tail: scala.collection.immutable.Vector[Int] = Vector(3) scala\u0026gt; val Seq(a,b,c) = List(1,2,3) a: Int = 1 b: Int = 2 c: Int = 3 直接提取正则：\nscala\u0026gt; val selectRE = | s\u0026quot;\u0026quot;\u0026quot;SELECT\\\\s*(DISTINCT)?\\\\s+($cols)\\\\s*FROM\\\\s+($table)\\\\s*($tail)?;\u0026quot;\u0026quot;\u0026quot;.r selectRE: scala.util.matching.Regex = \\ SELECT\\s*(DISTINCT)?\\s+(\\*|[\\w, ]+)\\s*FROM\\s+(\\w+)\\s*(.*)?; ","date":"2016-07-13","permalink":"/n3xtchen/2016/07/13/scala---pattern-match/","section":"时间线","summary":"1: val bools = Seq(true, false) 2: for (bool in bools) { 3:\tbool match { 4:\tcase true =\u0026gt; println(\u0026quot;真的\u0026quot;) 5:\tcase false =\u0026gt; println(\u0026quot;假的\u0026quot;) 6:\t} 7: } 这个就是我们所说的模式匹配；看起来很像 C 风格，但是不一样，尤其要记住：=\u0026gt;，我经常把它写成 :。","title":"Scala - 只谈模式匹配"},{"content":"","date":"2016-07-13","permalink":"/n3xtchen/tags/scalatra/","section":"标签","summary":"","title":"scalatra"},{"content":"发现对于新手来说，配置都挺蛋疼，所以还是写一篇博客备忘下。\n我的环境 # Ubuntu Linux 16.04-64bit Java 1.7.0_80 Scala 2.10.4 sbt 0.13.7 Gitter8 0.6.8 scalatra_version 2.4.1 第一步：安装 Gitter8 # Gitter8 （后面简称 G8）是用来生成发布在 github 中的项目模版的命令行工具，即脚手架（Skeleton）。它由 Scala 实现，通过 sbt 运行，但是可以用于任何用途。\n安装 G8 之前需要安装 Conscript：\n$ curl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh 确保安装完，把执行路径添加到 path 中，这边安装的默认路径是： ~/.conscript。\nConscript 项目地址： https://github.com/foundweekends/conscript\n现在正是安装 G8：\n$ cs foundweekends/giter8 第二步：安装 Scalatra # $ g8 scalatra/scalatra-sbt organization [com.example]: name [My Scalatra Web App]: version [0.1.0-SNAPSHOT]: servlet_name [MyScalatraServlet]: package [com.example.app]: scala_version [2.11.8]: 2.10.4 sbt_version [0.13.11]: 0.13.7 scalatra_version [2.4.1]: Template applied in ./my-scalatra-web-app 第三步：配置端口和开关服务器 # 修改端口 # 由于 Scalatra 默认使用的端口号是 8080，而这个端口（这个端口挺常见的，很用 app 都是用它，发生冲突也稀疏寻常）已经被我的其它应用占用了，所以我要修改，首先先看看生成的目录结构：\n. ├── project │ ├── build.properties │ ├── build.scala │ └── plugins.sbt ├── README.md ├── sbt └── src ├── main │ ├── resources │ │ └── logback.xml │ ├── scala │ │ ├── com │ │ │ └── example │ │ │ └── app │ │ │ ├── MyScalatraServlet.scala │ │ │ └── MyScalatraWebAppStack.scala │ │ └── ScalatraBootstrap.scala │ └── webapp │ └── WEB-INF │ ├── templates │ │ ├── layouts │ │ │ └── default.jade │ │ └── views │ │ └── hello-scalate.jade │ └── web.xml └── test └── scala └── com └── example └── app └── MyScalatraServletSpec.scala 这些都是由 G8 生成的，现在我们要修改 project/build.scala，首先要引入两个包，我是追加到第 8 行后：\n8: import com.earldouglas.xwp.JettyPlugin.autoImport._ 9: import com.earldouglas.xwp.ContainerPlugin.autoImport._ 然后，修改默认端口，如果已经插入上述两个包则你可以追加到第 48 行：\t48: },\t// 后面都好需要我们添加 49: containerPort in Jetty := 8090 // 改变端口\n开启和关闭服务器 # $ sbt [info] Loading project definition from /tmp/my-scalatra-web-app/project [info] Updating {file:/tmp/my-scalatra-web-app/project/}my-scalatra-web-app-build... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Compiling 1 Scala source to /tmp/my-scalatra-web-app/project/target/scala-2.10/sbt-0.13/classes... [info] Set current project to My Scalatra Web App (in build file:/tmp/my-scalatra-web-app/) \u0026gt; jetty:start\t// 启动服务器 [warn] Scala version was updated by one of library dependencies: [warn] * org.scala-lang:scala-library:(2.10.0, 2.10.4, 2.10.3) -\u0026gt; 2.10.6 [warn] To force scalaVersion, add the following: [warn] ivyScala := ivyScala.value map { _.copy(overrideScalaVersion = true) } [warn] Run 'evicted' to see detailed eviction warnings [info] Compiling Templates in Template Directory: /tmp/my-scalatra-web-app/src/main/webapp/WEB-INF/templates ... 2016-07-13 21:05:15.173:INFO:oejs.ServerConnector:main: Started ServerConnector@516368e1{HTTP/1.1}{0.0.0.0:8090} // 上一行，你就可以看到你使用的端口号 2016-07-13 21:05:15.173:INFO:oejs.Server:main: Started @4245ms \u0026gt; jetty:stop\t// 关闭服务器 [info] waiting for server to shut down... [success] Total time: 0 s, completed 2016-7-13 21:06:34 如何持续编译？ # 如果你在开发过程中，尤其在调试的时候，如果代码修改后可以自动编译并重启服务，不很方便吗？\n$ sbt ... \u0026gt; ~jetty:start ... [info] starting server ... [success] Total time: 0 s, completed 2016-7-13 20:28:22 1. Waiting for source changes... (press enter to interrupt) 这个功能是不是很人性化。\n。。。终于可以愉快地开始开发 Scalatra。\n","date":"2016-07-13","permalink":"/n3xtchen/2016/07/13/scalatra---installationconfig/","section":"时间线","summary":"发现对于新手来说，配置都挺蛋疼，所以还是写一篇博客备忘下。","title":"Scalatra: 安装和端口配置"},{"content":"","date":"2016-06-15","permalink":"/n3xtchen/tags/jvm/","section":"标签","summary":"","title":"jvm"},{"content":"TL;DR\n作为一个初学者，经过一个月系统的系统学习，用惯了动态语言的我来说，Scala 编译器型语言的编程体验真的是太棒了。作为阶段性的总结，我将给出我对 Scala 最佳初体验的 Top 10：\n漂亮的操作系统调用方式 # Scala 2.9里也提供类似功能：新增加了package: scala.sys 及scala.sys.process, 这些代码最初由 SBT (a simple build tool for Scala)项目贡献，主要用于简化与操作系统进程的交互与调用。现在看看用法：\nscala\u0026gt; import scala.language.postfixOps import scala.language.postfixOps scala\u0026gt; import scala.sys.process._ import scala.sys.process._ scala\u0026gt; \u0026quot;java -version\u0026quot; ! java version \u0026quot;1.7.0_80\u0026quot; Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) res2: Int = 0 当你引入 scala.sys.process 后，scala 会为你给字符串或数组动态注入 ! 方法来调用系统命令。\n这是我见过调用 shell 最爽最优雅的方式之一，不是吗？现在看个复杂点的例子，爬取一个页面：\nscala\u0026gt; import java.io.File import java.io.File scala\u0026gt; import java.net.URL import java.net.URL scala\u0026gt; new URL(\u0026quot;http://www.scala-lang.org/\u0026quot;) #\u0026gt; new File(\u0026quot;scala-lang.html\u0026quot;) ! res4: Int = 0 这里我们又学到一个新的操作符 #\u0026gt;，结果重定向。这条命令等价于如下 Bash：\n$ curl \u0026quot;http://www.scala-lang.org/ \u0026gt; scala-lang.html 看看结果:\nscala\u0026gt; \u0026quot;ls\u0026quot; ! ... scala-lang.html ... res6: Int = 0 参考: http://www.scala-lang.org/api/rc2/scala/sys/process/package.html\n管道（Pipeline） # scala\u0026gt; import scala.language.implicitConversions import scala.language.implicitConversions scala\u0026gt; object Pipeline { | implicit class toPiped[V](value:V) { | def |\u0026gt;[R] (f : V =\u0026gt; R) = f(value) | } | } defined module Pipeline scala\u0026gt; import Pipeline._ import Pipeline._ scala\u0026gt; 1 |\u0026gt; ((i:Int)=\u0026gt; i*10) res3: Int = 10 这样就可以将，函数返回的值作为后面函数参数，一条链式调用看起来是那么的优雅（你觉得呢？）。\n短短几行的代码，足以让你领教到 隐式转化（implicit）的威力吧！因为这个话题比较大，就不在这里作详细阐述了。\n使用 {...} 替代 (...) 的语法糖 # 声明一个多参数表函数，如下\nscala\u0026gt; def m[A](s: A)(f: A=\u0026gt; String) = f(s) m: [A](s: A)(f: A =\u0026gt; String)String 你可以这样调用它：\nscala\u0026gt; m(100)(i =\u0026gt; s\u0026quot;$i + $i\u0026quot;) res2: String = 100 + 100 你可以使用 {...} 替代 (...) 的语法糖，就可以把上面改写成下面的模式\nscala\u0026gt; m(100){ i =\u0026gt; s\u0026quot;$i + $i\u0026quot; } res3: String = 100 + 100 竟然可以如此优雅优雅地调用函数，看起来就像标准的块代码（像 if 和 for 表达式）\n创建自己的字符解释器 # import scala.util.parsing.json._ object Interpolators { implicit class jsonForStringContext(val sc: StringContext) { def json(values: Any*): JSONObject = { val keyRE = \u0026quot;\u0026quot;\u0026quot;^[\\s{,]*(\\S+):\\s*\u0026quot;\u0026quot;\u0026quot;.r val keys = sc.parts map { case keyRE(key) =\u0026gt; key case str =\u0026gt; str } val kvs = keys zip values JSONObject(kvs.toMap) } } } import Interpolators._ val name = \u0026quot;Dean Wampler\u0026quot; val book = \u0026quot;Programming Scala, Second Edition\u0026quot; val jsonobj = json\u0026quot;{name: $name, book: $book}\u0026quot; println(jsonobj) 哈哈，有意思吧！\n在大部分情况下，中缀(Infix)标记和后缀(Postfix)标记可省略 # 1 + 2 这段语句大家再熟悉不过了，但是在 Scala 中，所有的表达式都是方法，实际上完整的写法，如下：\n1.+(2) 即，+ 是整型对象（在 Scala 中，一切都是对象）的一个方法，如果了解到这里，会不会觉得能写成之前的样子很神奇。这里是 Scala 一个特性：\n如果一个对象有一个带有一个参数的方法， 它的中缀标记可以省略，在这里，点号和括号都可以省略。 类似的，如果一个方法没有参数表，你调用它的时候，可以它的后缀标记（即括号）可以省略：\nscala\u0026gt; 1 toString\t// 1.toString() warning: there were 1 feature warning(s); re-run with -feature for details res2: String = 1 因为忽略后缀，有时候会让人很迷惑，方法？属性？傻傻分不清楚！所以在版本 2.10 之后，如果没有明确告诉编译器的话，会给出一个警告：\nscala\u0026gt; import scala.language.postfixOps scala\u0026gt; 1 toString res2: String = 1 警报解除，神奇吧？现在我们看一个复杂的例子：\ndef isEven(n: Int) = (n % 2) == 0 List(1, 2, 3, 4) filter isEven foreach println // 等同于 .filter(isEven).foreach(println) 简便的类声明： # Java 党可以看过来，不论是 IDE 和文本编辑器党，代码中到处充斥着大量的 setter 和 getter：\n// src/main/java/progscala2/basicoop/JPerson.java package progscala2.basicoop; public class JPerson { private String name; private int age; public JPerson(String name, int age) { this.name = name; this.age = age; } public void setName(String name) { this.name = name; } public String getName() { return this.name; } public void setAge(int age) { this.age = age; } public int getAge() { return this.age; } } 看看，Scala 可以这么写：\nclass Person(var name: String, var age: Int) 有没有像流泪的感觉，哈哈，That is it！如果想要覆盖某些 setter 和 getter，只要类声明中，覆盖相应方法即可。\n有 var，自然有 val，如果你的对象属性都是不可变，那还可以使用如下声明：\ncase class Person(name: String, age: Int) 用 Java 的同学会不会很心动啊？^_^\n密封类型（Sealed）强制其子类只能定义在同一个文件中 # seals 关键字可以用在 class 和 trait 上。\n第一个作用如题，举个栗子，Scala 源码中 List 的实现用到 sealed 关键字:\nscala\u0026gt; class NewList extends List \u0026lt;console\u0026gt;:7: error: illegal inheritance from sealed class List class NewList extends List 这样子，妈妈再也不用担心我的类被人滥用。\n如果就这么个功能，怎么能称得上 Top 10 呢？在看一个黑魔法：\nscala\u0026gt; sealed abstract class Drawing defined class Drawing scala\u0026gt; case class Point(x: Int, y: Int) extends Drawing defined class Point scala\u0026gt; case class Circle(p: Point, r: Int) extends Drawing defined class Circle scala\u0026gt; case class Cylinder(c: Circle, h: Int) extends Drawing defined class Cylinder 如果你如之前，少写了其中一個案例：\nscala\u0026gt; def what(d: Drawing) = d match { | case Point(_, _) =\u0026gt; \u0026quot;点\u0026quot; | case Cylinder(_, _) =\u0026gt; \u0026quot;柱\u0026quot; | } \u0026lt;console\u0026gt;:14: warning: match may not be exhaustive. It would fail on the following input: Circle(_, _) def what(d: Drawing) = d match { ^ what: (d: Drawing)String 编译器在告訴你，有些模式的类型你沒有列在 match 運算式的案例串（Case sequence）之中。你应该每個都列出來才合理：\nscala\u0026gt; def what(d: Drawing) = d match { | case Point(_, _) =\u0026gt; \u0026quot;点\u0026quot; | case Circle(_, _) =\u0026gt; \u0026quot;圆\u0026quot; | case Cylinder(_, _) =\u0026gt; \u0026quot;柱\u0026quot; | } what: (d: Drawing)String 有時候，你使用別人密封過的案例类别，但也許你真的只想比对其中几個案例类型，如果不想要编译器饶人的警告，则可以在最后使用万用字元模式（_），例如：\nscala\u0026gt; def what(d: Drawing) = d match { | case Point(_, _) =\u0026gt; \u0026quot;點\u0026quot; | case Cylinder(_, _) =\u0026gt; \u0026quot;柱\u0026quot; | case _ =\u0026gt; \u0026quot;\u0026quot; // 作你想作的事，或者丟出例外 | } what: (d: Drawing)String 如果你真心不想要使用万用字元作额外处理，那么还可以可以使用 @unchecked 标注來告訴编译器住嘴：\nscala\u0026gt; def what(d: Drawing) = (d: @unchecked) match { | case Point(_, _) =\u0026gt; \u0026quot;點\u0026quot; | case Cylinder(_, _) =\u0026gt; \u0026quot;柱\u0026quot; | } what: (d: Drawing)String 参考：http://openhome.cc/Gossip/Scala/SealedClass.html\n有趣的权限控制 private[] # protected 或 private 這表示权限限制到 x 的范围。\nclass Some { private val x = 10 def doSome(s: Some) = s.x + x } 对于大多数语言，访问控制就严格无非就这两种。在 Scala 中，可以更加严格，让 x 完全无法透过实例存取，则可以使用 private[this]，這表示私有化至 this 实例本身才可以存取，也就是所謂物件私有（Object-private），例如以下就通不過编译了：\nclass Some { private[this] val x = 10 def doSome(s: Some) = s.x + x // 编译错误 } 作为入门，就知道这里就可以了，可以跳到下一个话题了！\n如果你看过 Spark（一个分布式计算框架）的源码，会发现这样的权限控制符到处都是，所以这个还是有必要搞清楚的。\n如果能看懂下面这段代码，那你对 Scala 的域访问控制算是了解了：\npackage scopeA { class Class1 { private[scopeA] val scopeA_privateField = 1 protected[scopeA] val scopeA_protectedField = 2 private[Class1] val class1_privateField = 3 protected[Class1] val class1_protectedField = 4 private[this] val this_privateField = 5 protected[this] val this_protectedField = 6 } class Class2 extends Class1 { val field1 = scopeA_privateField val field2 = scopeA_protectedField val field3 = class1_privateField // ERROR val field4 = class1_protectedField val field5 = this_privateField // ERROR val field6 = this_protectedField } } package scopeB { class Class2B extends scopeA.Class1 { val field1 = scopeA_privateField // ERROR val field2 = scopeA_protectedField val field3 = class1_privateField // ERROR val field4 = class1_protectedField val field5 = this_privateField // ERROR val field6 = this_protectedField } } 不懂也没关系，后面会花一个大篇幅来讲这块内容。\n类型擦除和 implicit # scala\u0026gt; :paste object M { def m(seq: Seq[Int]): Unit = println(s\u0026quot;Seq[Int]: $seq\u0026quot;) def m(seq: Seq[String]): Unit = println(s\u0026quot;Seq[String]: $seq\u0026quot;) } \u0026lt;ctrl-d\u0026gt; \u0026lt;console\u0026gt;:8: error: double definition: method m:(seq: Seq[String])Unit and method m:(seq: Seq[Int])Unit at line 7 have same type after erasure: (seq: Seq)Unit def m(seq: Seq[String]): Unit = println(s\u0026quot;Seq[String]: $seq\u0026quot;) 由于历史原因，JVM 忘记了 参数化类型的参数类型。例如例子中的 Seq[Int] 和 Seq[String]，JVM 看到的只是 Seq，而附加的参数类型对于 JVM 来说是不可见，这就是传说中的类型擦除。于是乎，从 JVM 的视角来看，代码是这样子的：\nobject M { // 两者都是接受一个 Seq 参数，返回Unit def m(seq: Seq): Unit = println(s\u0026quot;Seq[Int]: $seq\u0026quot;) def m(seq: Seq): Unit = println(s\u0026quot;Seq[String]: $seq\u0026quot;) } 这不就报错了吗？重复定义方法。那怎么办呢？（JAVA 语言的痛点之一）Scala 给你提供一种比较优雅的解决方案，使用隐式转换：\nscala\u0026gt; :paste object M { implicit object IntMarker implicit object StringMarker // 对于 JVM 来说是，接受 Seq 参数和 IntMarker.type 型 i 参数， // 返回 Unit def m(seq: Seq[Int])(implicit i: IntMarker.type): Unit = println(s\u0026quot;Seq[Int]: $seq\u0026quot;) // 对于 JVM 来说是，接受 Seq 参数和 StringMarker.type 型 i 参数， // 返回 Unit // def m(seq: Seq[String])(implicit s: StringMarker.type): Unit = println(s\u0026quot;Seq[String]: $seq\u0026quot;) } \u0026lt;ctrl-d\u0026gt; scala\u0026gt; import M._ scala\u0026gt; m(List(1,2,3))\t// 在调用的时候，忽略隐式类型 scala\u0026gt; m(List(\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;, \u0026quot;three\u0026quot;)) 异常捕捉与 Scalaz # 先来看看，异常捕捉语句：\ntry { source = Some(Source.fromFile(fileName)) val size = source.get.getLines.size println(s\u0026quot;file $fileName has $size lines\u0026quot;) } catch { case NonFatal(ex) =\u0026gt; println(s\u0026quot;Non fatal exception! $ex\u0026quot;) } finally { for (s \u0026lt;- source) { println(s\u0026quot;Closing $fileName...\u0026quot;) s.close } } 这里是打开文件的操作，并且计算行数；NonFatal 非致命错误，如内存不足之类的非致命错误，将会被抛掉。finnaly 操作结束后，关闭文件。这是 Scala 模式匹配的又一大应用场景，你会发现倒是都是模式匹配：\ncase NonFatal(ex) =\u0026gt; ... 如果每条异常的处理语句只是单条的话，Scala 写起来应该会挺爽的。\nScalaz 验证 # 在传统的异常处理，无法一次性汇总运行过程中的错误。如果我们在做表单验证的时候，我们就需要考虑到这个场景。而传统的做法就是通过一层层 try ... catch ...，把错误追加到一个列表中来实现，而 scalaz 中提供一个适用于这个场景的封装，直接看例子吧：\nimport scalaz._, std.AllInstances._ /* 验证用户名，非空和只包含字母 */ def validName(key: String, name: String): Validation[List[String], List[(String,Any)]] = { val n = name.trim // remove whitespace if (n.length \u0026gt; 0 \u0026amp;\u0026amp; n.matches(\u0026quot;\u0026quot;\u0026quot;^\\p{Alpha}$\u0026quot;\u0026quot;\u0026quot;)) Success(List(key -\u0026gt; n)) else Failure(List(s\u0026quot;Invalid $key \u0026lt;$n\u0026gt;\u0026quot;)) } /* 验证数字，并且大于0 */ def positive(key: String, n: String): Validation[List[String], List[(String,Any)]] = { try { val i = n.toInt if (i \u0026gt; 0) Success(List(key -\u0026gt; i)) else Failure(List(s\u0026quot;Invalid $key $i\u0026quot;)) } catch { case _: java.lang.NumberFormatException =\u0026gt; Failure(List(s\u0026quot;$n is not an integer\u0026quot;)) } } /* 验证表单 */ def validateForm(firstName: String, lastName: String, age: String): Validation[List[String], List[(String,Any)]] = { validName(\u0026quot;first-name\u0026quot;, firstName) +++ validName(\u0026quot;last-name\u0026quot;, lastName) +++ positive(\u0026quot;age\u0026quot;, age) } validateForm(\u0026quot;Dean\u0026quot;, \u0026quot;Wampler\u0026quot;, \u0026quot;29\u0026quot;) validateForm(\u0026quot;\u0026quot;, \u0026quot;Wampler\u0026quot;, \u0026quot;0\u0026quot;) // Returns: Failure(List(Invalid first-name \u0026lt;\u0026gt;, Invalid age 0)) //告知你名字和年龄填写有误 validateForm(\u0026quot;Dean\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;0\u0026quot;) // Returns: Failure(List(Invalid last-name \u0026lt;\u0026gt;, Invalid age 0)) // 告知你姓氏和年龄填写有误 validateForm(\u0026quot;D e a n\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;29\u0026quot;) // Returns: Failure(List(Invalid first-name \u0026lt;D e a n\u0026gt;, Invalid last-name \u0026lt;\u0026gt;)) // 告知你名字和姓氏填写错误 这方式还不错吧？\n就到这里就结束了，写着写着，就写这么多了，赶紧收住。\n","date":"2016-06-15","permalink":"/n3xtchen/2016/06/15/scala---top-10-for-new/","section":"时间线","summary":"TL;DR","title":"Scala 新手眼中的十种有趣用法"},{"content":"","date":"2016-05-27","permalink":"/n3xtchen/tags/ios/","section":"标签","summary":"","title":"ios"},{"content":"","date":"2016-05-27","permalink":"/n3xtchen/tags/swift/","section":"标签","summary":"","title":"Swift"},{"content":"","date":"2016-05-27","permalink":"/n3xtchen/categories/swift/","section":"分类页","summary":"","title":"Swift"},{"content":"作为 Swift 初学者（高手绕行），暂时让 iOS 和 Mac Os（洗完这篇文章的时候，Os X 已经更名了） 编程见鬼去；所以，一开始就不使用 XCode 作为演示工具，直接使用 Swift 的 REPL 来进行演示，让我们一起 focus 语言本身。\n前提，你需要安装 Xcode (从 **App Store **中下载)以及它的 Command-Line-Tools, 这样你就可以直接打开终端：\n$ swift Welcome to Apple Swift version 2.2 (swiftlang-703.0.18.8 clang-703.0.30). Type :help for assistance. 1\u0026gt; print(\u0026quot;Hello, world\u0026quot;) Hello, world 这里我们可以看到，Swift 的当前版本为 2.2。\n这样, 我们就可以开始探索我们的 Swift World\n什么是 Swift # Swift是一種支持多编程范式和編譯式的編程語言，是用來撰寫 Mac OS，iOS 和Watch OS 的语言之一，但是可能在不久的将来也可能作为 Linux 的界面开发程序。\n语句（Statement）和注释（comment） # 1\u0026gt; print(\u0026quot;Statement\u0026quot;)\t// 这就是一条语句 Statement 在 Swift 中，换行符代表一条语句的结束。当然和其他大多数语言一样，分号也可以用来表示一条语句的结束，而且当你想在一行中编写多个语句，那么分号将必不可少：\n2\u0026gt; print(\u0026quot;Hello\u0026quot;); print(\u0026quot;World\u0026quot;); Hello World\t你注意到第一条语句后面的双斜杠了吗？这就是注释（comment），双斜杠的意义就是告诉你，注释从这里开始，编译器将忽略后面到行末的内容。\n作为个老猿，还是要唠叨下，良好的注释行为是程序员的美德之一。\nvar 和 let # var, 故名思意，会变化的量；该变量存储的值是可以变化：\n1\u0026gt; var x = 1 x: Int = 1 2\u0026gt; x = 2 3\u0026gt; x $R0: Int = 2 let, 与 var 相对立的，常量（constant），即不变化的量：\n4\u0026gt; let y = 3 y: Int = 3 5\u0026gt; y = 2\t// 编译错误 repl.swift:5:3: error: cannot assign to value: 'y' is a 'let' constant y = 2 ~ ^ repl.swift:4:1: note: change 'let' to 'var' to make it mutable let y = 3 ^~~ var 这里，我们还看到的 Swift 的错误提示还是很友好的，还给出了修改建议。\n但你尝试修改常量的时候，编译器将无法通过。\n数据类型 # Swift 最吸引的的特点之一就是它的类型系统嘛！静态、強类型、类型推论；这意味着当你声明一个变量或常量时，它的类型在后续的操作中时不可以改变，否则将无法编译通过：\n3\u0026gt; var x = 1 x: Int = 1 4\u0026gt; x = \u0026quot;1\u0026quot; repl.swift:4:5: error: cannot assign value of type 'String' to type 'Int' x = \u0026quot;1\u0026quot; ^~~ 变量声明 # 声明数据类型的格式：\nvar|let 变量名 : 变量类型 [= 变量值] 来看一个实际的例子：\nvar num : Int = 1 这里我们声明一个名为 num 的整型变量。\n操作符（Operator） # 作为程序，加减乘除是基本功能，那就像从算数操作符（Arithmetic operations）谈起：\n1\u0026gt; 1 + 1 $R0: Int = 2 2\u0026gt; 2 - 1 $R1: Int = 1 3\u0026gt; 2 * 3 $R2: Int = 6 4\u0026gt; 4 / 2 $R3: Int = 2 5\u0026gt; 5 % 3 $R4: Int = 2 太简单，不知道怎么解释，所以直接上代码。\n位运算（bitwise operations） # let initialBits: UInt8 = 0b00001111 let invertedBits = ~initialBits // equals 11110000 let firstSixBits: UInt8 = 0b11111100 let lastSixBits: UInt8 = 0b00111111 let middleFourBits = firstSixBits \u0026amp; lastSixBits // equals 00111100 let someBits: UInt8 = 0b10110010 let moreBits: UInt8 = 0b01011110 let combinedbits = someBits | moreBits // equals 11111110 let firstBits: UInt8 = 0b00010100 let otherBits: UInt8 = 0b00000101 let outputBits = firstBits ^ otherBits // equals 00010001 let shiftBits: UInt8 = 4 // 00000100 in binary shiftBits \u0026lt;\u0026lt; 1 // 00001000 shiftBits \u0026lt;\u0026lt; 2 // 00010000 shiftBits \u0026lt;\u0026lt; 5 // 10000000 shiftBits \u0026lt;\u0026lt; 6 // 00000000 shiftBits \u0026gt;\u0026gt; 2 // 00000001 对比表达式（comparison operators）： # 1 == 1\t// true 2 ~= 1\t// true 1 \u0026lt; 2\t// true 2 \u0026lt;= 2\t// true 3 \u0026gt; 2\t// true 3 \u0026gt;= 4\t// false 逻辑表达式 # var zhen : Bool = true var jia : Bool = false zhen \u0026amp;\u0026amp; zhen // 真 与 真 ＝\u0026gt; true zhen \u0026amp;\u0026amp; jia // 真 与 假 =\u0026gt; false jia \u0026amp;\u0026amp; jia // 假 与 假 =\u0026gt; false jia \u0026amp;\u0026amp; zhen // 假 与 真 =\u0026gt; false zhen || jia // 真 或 假 =\u0026gt; true jia || zhen // 假 或 真 =\u0026gt; true !zhen // 非 真 =\u0026gt; false !jia // 非 假 =\u0026gt; true 范围(Range) # 先来看几个例子：\n1\u0026gt; for index in 1...5 { 2. print(\u0026quot;\\(index) times 5 is \\(index * 5)\u0026quot;) 3. } 1 times 5 is 5 2 times 5 is 10 3 times 5 is 15 4 times 5 is 20 5 times 5 is 25 4\u0026gt; let names = [\u0026quot;Anna\u0026quot;, \u0026quot;Alex\u0026quot;, \u0026quot;Brian\u0026quot;, \u0026quot;Jack\u0026quot;] names: [String] = 4 values { [0] = \u0026quot;Anna\u0026quot; [1] = \u0026quot;Alex\u0026quot; [2] = \u0026quot;Brian\u0026quot; [3] = \u0026quot;Jack\u0026quot; } 5\u0026gt; let count = names.count count: Int = 4 6\u0026gt; for i in 0..\u0026lt;count { 7. print(\u0026quot;Person \\(i + 1) is called \\(names[i])\u0026quot;) 8. } Person 1 is called Anna Person 2 is called Alex Person 3 is called Brian Person 4 is called Jack 总结下：\na...b: a 与 b 的区间内，包含 a 和 b a..\u0026lt;b: a 与 b 的区间内，包含 a, 但不包含 b，我在想这可能是作者为了方便操作数组而设计 内置的数据类型 # 每一个变量，每一个常量，都需要一个类型。所以我们先从基础数据类型入手，同时介绍这些类型的实例方法（instance method），全局方法（global method）以及操作符。\n数字（Number） # Int 和 Double 是 Swift 最主要的数值类型；另外你还可以使用 C 和 Object-C 的数值类型。\n1\u0026gt; Int.min $R0: Int = -9223372036854775808 2\u0026gt; Int.max $R1: Int = 9223372036854775807 3\u0026gt; let x = 10 x: Int = 10 1\u0026gt; let pi = 3.14159 pi: Double = 3.1415899999999999 2\u0026gt; let anotherPi = 3 + 0.14159 anotherPi: Double = 3.1415899999999999 整型的取值范围在 Int.min 和 Int.max 之间。\n在 Swift 中，你可以使用多种形式来编写整型：\n1\u0026gt; let decimalInteger = 17 decimalInteger: Int = 17 2\u0026gt; let binaryInteger = 0b10001\t// 17 的二进制形式 binaryInteger: Int = 17 3\u0026gt; let octalInteger = 0o21\t// 17 的八进制形式 octalInteger: Int = 17 4\u0026gt; let hexadecimalInteger = 0x11\t// 17 的十六进制形式 hexadecimalInteger: Int = 17 对于十进制的来说，可以使用如下方式来表示科学计数：\n3e2: 3 x 10的2次方, 即 300 3e-2: 3 x 10的-2次方, 即 0.03 1.25e2: 1.25 x 102, 即 125.0 1.25e-2: 1.25 x 10-2, 即 0.0125 还可以使用千分位：\n8\u0026gt; let oneMillion = 1_000_000 oneMillion: Int = 1000000 对于十六进制的来说，可以使用如下方式来表示：\n0xFp2: 15 x 2的2次方, 即 60.0 0xFp-2: 15 x 2的-2次方, 即 3.75 https://developer.apple.com/library/ios/documentation/Swift/Conceptual/Swift_Programming_Language/TheBasics.html#//apple_ref/doc/uid/TP40014097-CH5-ID309\n字符串（String） # 在 Swift 中，字符串是有双引号包裹的字画量。\n1\u0026gt; let greeting = \u0026quot;hello\u0026quot; greeting: String = \u0026quot;hello\u0026quot; 2\u0026gt; let greeting_2 = 'hello' repl.swift:2:18: error: single-quoted string literal found, use '\u0026quot;' let greeting_2 = 'hello' ^~~~~~~ \u0026quot;hello\u0026quot; 在很多其他语言中，字符串可以使用单双引号包裹；但是，在 Swift 中不行，只能用双引号，双引号，双引号。\n另外，Swift 的字符串使用的 Unicode；作为最先进的语言，怎么可能不用 Unicode，现在还有很多语言默认采用的是 ASCII，对非英语国家来说，多痛苦啊！\n2\u0026gt; let checkmark = \u0026quot;\\u{21DA}\u0026quot;\t// /u{} checkmark: String = \u0026quot;⇚\u0026quot; 反斜杆（\\）作为转移符，意思是说接下来的一个字符将被特殊处理：举几个栗子：\n\\n: 代表换行 \\t: 代表制表符 \u0026quot;: 转移双引号，不至于被当作字符串的结尾。 \\: 转移自己，如果你想打印个反斜杠的话 字符串插值（string interpolation） # 先看一个栗子：\n3\u0026gt; let n = 5 n: Int = 5 4\u0026gt; let s = \u0026quot;U hava \\(n) widgets.\u0026quot; s: String = \u0026quot;U hava 5 widgets.\u0026quot; \\(var_name) 这样就可以把 var_name 这个值替换到字符串中了，即使这个变量不是字符串类型的。\n当然了，插值不仅仅只是变量，还可以是表达式，方法调用等等： 5\u0026gt; let m = 6 m: Int = 6 6\u0026gt; let s = \u0026ldquo;U hava (n+m) widgets.\u0026rdquo; s: String = \u0026ldquo;U hava 11 widgets.\u0026rdquo;\n另外，在 Swift 2.0 插值中可以使用双引号（在旧版中是不允许的）：\n7\u0026gt; let s = \u0026quot;U hava \\(greeting + \u0026quot;n3xtchen\u0026quot;) widgets.\u0026quot; s: String = \u0026quot;U hava hellon3xtchen widgets.\u0026quot; 字符串合并 # 最简单的方式，就是使用 +：\n1\u0026gt; let s = \u0026quot;Hello\u0026quot; s: String = \u0026quot;Hello\u0026quot; 2\u0026gt; let s2 = \u0026quot;World\u0026quot; s2: String = \u0026quot;World\u0026quot; 3\u0026gt; let greeting = s + s2 greeting: String = \u0026quot;HelloWorld\u0026quot; 使用 += 追加字符串：\n1\u0026gt; var s = \u0026quot;Hello \u0026quot; s: String = \u0026quot;Hello \u0026quot; 2\u0026gt; let s2 = \u0026quot;World\u0026quot; s2: String = \u0026quot;World\u0026quot; 3\u0026gt; s += s2 4\u0026gt; s $R0: String = \u0026quot;Hello World\u0026quot; 5\u0026gt; let exclamationMark: Character = \u0026quot;!\u0026quot; exclamationMark: Character = \u0026quot;!\u0026quot; 6\u0026gt; s.append(exclamationMark) 7\u0026gt; s $R1: String = \u0026quot;Hello World!\u0026quot; 注意，append 方法的参数只能是字符型，不能是字符串。\n索引或下标(subscript) # 这个是我感觉最蛋疼的API，好端端的整型不用，用什么结构体索引。。。既然入坑，也要讲讲：\ns[下标]\t// 这里的下标是一个专门的结构体，不能使用整型 首先是下标操作：\n1\u0026gt; let s = \u0026quot;hey you!!\u0026quot; x: String = \u0026quot;hey you!!\u0026quot; 2\u0026gt; s.startIndex $R1: Index = { _base = { _position = 0\t// 这个就是当前的位置 _core = { _baseAddress = 0x00000001004fe620 __lldb_expr_1.x : Swift.String + 32 _countAndFlags = 9\t// 这个是下标的总长度 _owner = nil } } _lengthUTF16 = 1 } 3\u0026gt; s.endIndex\t// 字符串的最后一个字符的索引好 $R2: Index = { _base = { _position = 9\t// 注意这里 _core = { _baseAddress = 0x00000001004fe620 __lldb_expr_1.x : Swift.String + 32 _countAndFlags = 9 _owner = nil } } _lengthUTF16 = 0 } 字符串的索引都是这样的结构，只是其中的 _position 和core._countAndFlags 发生的变化，表示偏移量，下面是字符串索引的相关函数:\nsuccessor(): successor, 下一任，故名思义，当前索引的后一个 predecessor()：predecessor，前任，当前索引的前一个 advancedBy(n)：从当前索引向后 n 个字符 它们本身也是返回索引的结构。\n栗子时间：\n1\u0026gt; let s = \u0026quot;hey you!!\u0026quot; s: String = \u0026quot;hey you!!\u0026quot; 2\u0026gt; s[s.startIndex] $R0: Character = \u0026quot;h\u0026quot; 3\u0026gt; s[s.endIndex] fatal error: Can't form a Character from an empty String Execution interrupted. Enter code to recover and continue. Enter LLDB commands to investigate (type :help for assistance.) 4\u0026gt; s[s.endIndex.predecessor()] $R1: Character = \u0026quot;!\u0026quot; 5\u0026gt; s[s.startIndex.advancedBy(3)] $R2: Character = \u0026quot; \u0026quot; 6\u0026gt; s[s.endIndex.predecessor().predecessor()] $R3: Character = \u0026quot;!\u0026quot; 虽然设计有点蛋疼，但是也挺好理解的。\n插入和删除 # 1\u0026gt; var welcome = \u0026quot;hello\u0026quot; welcome: String = \u0026quot;hello\u0026quot; 在尾部追加一个感叹号：\n2\u0026gt; welcome.insert(\u0026quot;!\u0026quot;, atIndex: welcome.endIndex) 3\u0026gt; welcome $R0: String = \u0026quot;hello!\u0026quot; 在尾部的感叹号前插入 there 字符：\n4\u0026gt; welcome.insertContentsOf(\u0026quot; there\u0026quot;.characters, at: welcome.endIndex.predecessor()) 5\u0026gt; welcome $R1: String = \u0026quot;hello there!\u0026quot; 删除最后一个字符:\n6\u0026gt; welcome.removeAtIndex(welcome.endIndex.predecessor()) $R2: Character = \u0026quot;!\u0026quot; 7\u0026gt; welcome $R3: String = \u0026quot;hello there\u0026quot; 范围删除：\n8\u0026gt; let range = welcome.endIndex.advancedBy(-6)..\u0026lt;welcome.endIndex range: Range\u0026lt;Index\u0026gt; = { startIndex = { _base = { _position = 5 ... } endIndex = { _base = { _position = 11 ... } } 9\u0026gt; welcome.removeRange(range) 10\u0026gt; welcome $R4: String = \u0026quot;hello\u0026quot; ..\u0026lt; 这个表达式大家应该还有印象吧？不记得的，见操作符的部分。\n字符操作方法有点多，这里做一个小结：\n在某个位置插入字符串 s.insert(\u0026quot;要插入的的字符串\u0026quot;, atIndex: /* 这里插入的位置，字符索引 */) s.insertContentsOf(/* 这里是一个字符数组 */, at: /* 这里插入的位置，字符索引 */) 删除一个字符: s.removeAtIndex(/* 这里插入的位置，字符索引 */) 删除字符串: s.removeRange(/* 这里插入的位置，字符索引范围 */) 在尾部追加字符串： s.append(c: Character) s.appendContentsOf(/* 字符串或者字符数组 */) 替换: s.replaceRange(/* 这里插入的位置，字符索引范围 */, with: /* 字符串或者字符数组 */) 字符串对比 # 1\u0026gt; welcome == \u0026quot;Hello World!\u0026quot; $R1: Bool = true 2\u0026gt; welcome.hasPrefix(\u0026quot;Hel\u0026quot;)\t// 前缀匹配 $R2: Bool = true 3\u0026gt; welcome.hasSuffix(\u0026quot;orld!\u0026quot;)\t// 后缀匹配 $R3: Bool = true 4\u0026gt; welcome.characters.contains(\u0026quot;e\u0026quot;)\t// 包含某个字母 $R4: Bool = true 布尔型（Bool） # 首先来看看布尔型的对象类型（实际上是个 struct），它只有两个值：true 和 false。\nvar selected : Bool = true 至于用法，在表达式的部分已经讲了，不记得可以回头看下。\n结余 # 这是我的第一个 Swift 分享。为了让初学者更容易看懂，写的挺纠结的，希望大家看起来不揪心就好，^_^。\n","date":"2016-05-27","permalink":"/n3xtchen/2016/05/27/swift-world/","section":"时间线","summary":"作为 Swift 初学者（高手绕行），暂时让 iOS 和 Mac Os（洗完这篇文章的时候，Os X 已经更名了） 编程见鬼去；所以，一开始就不使用 XCode 作为演示工具，直接使用 Swift 的 REPL 来进行演示，让我们一起 focus 语言本身。","title":"Swift World - 基础入门"},{"content":"","date":"2016-05-03","permalink":"/n3xtchen/tags/performance/","section":"标签","summary":"","title":"performance"},{"content":"","date":"2016-05-03","permalink":"/n3xtchen/2016/05/03/postgreql---performance/","section":"时间线","summary":"","title":"postgreql   performance"},{"content":"","date":"2016-05-02","permalink":"/n3xtchen/tags/extension/","section":"标签","summary":"","title":"extension"},{"content":" 安装 oracle_fdw # 设置环境变量\n$ export ORACLE_HOME=/mfs/lib/oracle/instantclient_12_1 $ pgxn install oracle_fdw pgxn 的安装见 PostgreSQL - 使用 PGXN 安装 Mysql-fdw\n启用 oracle_fdw # 创建扩展：\n$ pgsql —db=n3xt_pg n3xt_pg# CREATE EXTENSION oracle_fdw; CREATE EXTENSION 提示少了 so 文件,报错如下 # postgres=# create extension oracle_fdw; ERROR: could not load library \u0026quot;/opt/pgsql/lib/oracle_fdw.so\u0026quot;: libclntsh.so.12.1: cannot open shared object file: No such file or directory 说明动态链接库没有正确的引入\n使用 ln\n首先查找下这个动态库的位置\n$ locate libclntsh.so.12.1.so path/to/libclntsh.so.12.1.so 然后 ln 到 $PGHOME/lib:\n$ ln -s path/to/libclntsh.so.12.1.so $PGHOME/lib/libclntsh.so.12.1.so 同类的问题，你只需要定位该动态库的地址，把它 ln 到 $PGHOME/lib 即可解决。\n2.最简单的方法就是在全局引入需要的动态链接库：\n$ echo $ORACLE_HOME \u0026gt; /etc/ld.so.conf.d/oracle.conf $ ldconfig 重复 create extension 即可安装成功。\n使用 oracle_fdw # 创建远端服务器（foreign server） # n3xt_pg# CREATE SERVER db FOREIGN DATA WRAPPER oracle_fdw n3xt_pg# OPTIONS (dbserver '//127.0.0.1/TEST'); 映射关联用户 # n3xt_pg# CREATE USER MAPPING FOR admin SERVER db229 n3xt_pg# OPTIONS (USER 'test', password 'test'); 创建外部表（FOREIGN TABLE ） # CREATE FOREIGN TABLE users ( id INT NOT NULL, username\tCHARACTER varying(50) NOT NULL, password\tCHARACTER varying(256) NOT NULL ) SERVER db OPTIONS (TABLE 'USER' ); 查询测试 # n3xt_pg# select id, username from users; id | username ----+--------- 1 | n3xtchen (2 rows) ","date":"2016-05-02","permalink":"/n3xtchen/2016/05/02/postgresql---use-pgxn-install-oracle-fdw/","section":"时间线","summary":"安装 oracle_fdw # 设置环境变量","title":"PostgreSQL - 使用 PGXN 安装 oracle-fdw"},{"content":"TL;DR: 是的，但这不是一个好的问题。\n就在一年前，我们提出问题 “Is PostgreSQL Your Next JSON Database\u0026hellip;”。现在，随着 PostgreSQL-9.5 的发布，是时候验证下 Betteridge\u0026rsquo;s law 是否仍然有效。因此，我们一起来探讨下各个版本的 PostgreSQL 对 JSONB 的改进。\nPostgreSQL 的 JSON 史可以追溯到 9.2。\nJSON in 9.2 # 原始的 JSON 数据类型在 PostgreSQL-9.2 中强势引入，但实际上只是一个被标记为 JSON 类型的文本字段，通过解析器来处理。在 9.2 中，你只能对 JSON 中进行简单的存取；其他的任何事情都只能使用 PL 语言来处理。在一些场景下是很有用的，但是。。。你还需要更多的功能。\n为了说明，假设我们有如下 JSON 数据：\n{ \u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, \u0026quot;num_votes\u0026quot;: 1566874, \u0026quot;rating\u0026quot;: 9.3, \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, \u0026quot;can_rate\u0026quot;: true, \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, \u0026quot;image\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;, \u0026quot;width\u0026quot;: 933, \u0026quot;height\u0026quot;: 1388 } } 首先，创建一张表：\nCREATE TABLE filmsjson (id BIGSERIAL PRIMARY KEY, data JSON); 然后像这样插入数据：\nn3xt_pg=# INSERT INTO filmsjson (data) VALUES ('{ \u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, \u0026quot;num_votes\u0026quot;: 1566874, \u0026quot;rating\u0026quot;: 9.3, \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, \u0026quot;can_rate\u0026quot;: true, \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, \u0026quot;image\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;, \u0026quot;width\u0026quot;: 933, \u0026quot;height\u0026quot;: 1388 } }'); INSERT 0 1 n3xt_pg=# select * from filmsjson postgres-# ; id | data ---+------------------------------------------------------------------------------------------------------------- 1 | {\u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, + | \u0026quot;num_votes\u0026quot;: 1566874, + | \u0026quot;rating\u0026quot;: 9.3, + | \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, + | \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, + | \u0026quot;can_rate\u0026quot;: true, + | \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, + | \u0026quot;image\u0026quot;: { + | \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;, + | \u0026quot;width\u0026quot;: 933, + | \u0026quot;height\u0026quot;: 1388 + | } + | } (1 row) 我们的能做很受限。注意到了吗？空格和换行都会被保留。这个在后面很重要。。。\n接着到 9.3 # PostgreSQL-9.3 有了新的解析器，操作符可以用于提取 JSON 数据中的值。他们中使用率最高的就是 -\u0026gt; ，可以赋予整型，提取数组中的值；或者一个字符串，提取 JSON 对象成员；-\u0026gt;\u0026gt; 也一样，不过他返回的是文本。我们还可以使用 #\u0026gt; 和 ＃\u0026gt;\u0026gt; 来指定路径来获取数据。\n接着，我们之前的表，我们可以进一步操作 JSON，做如下查询：\nn3xt_pg=# select data-\u0026gt;'title' from filmsjson; ?column? ---------------------------- \u0026quot;The Shawshank Redemption\u0026quot; (1 row) n3xt_pg=# select data#\u0026gt;'{image,width}' from filmsjson; ?column? ---------- 933 (1 row) 路径实际上就是一个 key 列表来遍历 JSON 文档的。不要以为花括号只是用来展示 JSON 的 —— 他实际上是一个数组的字画量，在 PostgreSQL 中解释成 text[]。这个意味和下面的查询等价：\nn3xt_pg=# select data#\u0026gt;ARRAY['image', 'width'] from filmsjson; ?column? ---------- 933 (1 row) 虽然加入了很多功能函数，但是仍然很受限。它不允许复杂的查询，不能在特殊类型使用索引，而且只能创建新的 JSON 元素。另外，最严重的问题就是每次查询都要对文本字段进行实时解析，这样做相当的低效。\n切到 9.4 # PostgreSQL-9.4 引入了新的 JSON 类型是 JSONB。JSONB 是 JSON 的二进制编码版本，它高效地存储着键值。这意味着所有的空格都会被删除。缺点就是你不能在同级创建重复的 key(不知道这个实际场景是什么？？？)，你会失去文档的格式。但是这个牺牲是值得的，因为任何东西都变得更高效了，不再需要实时解析。它同时也拖慢了插入的速度，因为要等解析完成为止。现在来看看它们的不同，首先创建一个 JSONB 表，插入演示数据：\nn3xt_pg=# CREATE TABLE filmsjsonb (id BIGSERIAL PRIMARY KEY, data JSONB); CREATE TABLE n3xt_pg=# SELECT * from filmsjsonb; id | data ----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | {\u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, \u0026quot;image\u0026quot;: {\u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;, \u0026quot;width\u0026quot;: 933, \u0026quot;height\u0026quot;: 1388}, \u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, \u0026quot;rating\u0026quot;: 9.3, \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, \u0026quot;can_rate\u0026quot;: true, \u0026quot;num_votes\u0026quot;: 1566874} (1 row) 是的，长度非常宽。所有的空格和换行都被替换成一个空格。\n虽然它们拥有很多相同的特性，但是最大的区别就是：JSONB 没有创建函数。在 9.4 中，JSON 数据类型有一堆的创建函数：json_build_object(), json_build_array() 和 json_object(),也可以转化成 JSONB（::jsonb）类型。它同时也反应了 PostgreSQL 开发者的使用的逻辑 —— JSON 为了准确存储，JSONB 为的是快速，高效的查询。因此 JSON 和 JSONB 都有 -\u0026gt;, -\u0026gt;\u0026gt;, #\u0026gt; 和 #\u0026gt;\u0026gt; 操作符，而 JSONB 还有有包含和存在操作符 @\u0026gt;, \u0026lt;@, ?, ?| 和 \u0026amp;?。\n存在是用来检查 key 是否存在，因此我们首先检查下我们演示数据中是否存在 rating 字段：\nn3xt_pg=# select data-\u0026gt;'title' from filmsjsonb where data ? 'rating'; ?column? ---------------------------- \u0026quot;The Shawshank Redemption\u0026quot; (1 row) 但是数据中 url 不在最外层，所以无法检索到 ：\nn3xt_pg=# select data-\u0026gt;'title' from filmsjsonb where data ? 'url'; ?column? ---------- (0 rows) 但是我们可以这样子做：\nn3xt_pg=# select data-\u0026gt;'title' from filmsjsonb where data-\u0026gt;'image' ? 'url'; ?column? ---------------------------- \u0026quot;The Shawshank Redemption\u0026quot; (1 row) ?| 和 ?\u0026amp; 对 ? 的功能进行扩展了:\nn3xt_pg=# select data-\u0026gt;'title' from filmsjsonb where data ?| '{\u0026quot;image\u0026quot;, \u0026quot;rat\u0026quot;}';\t-- 相当于 data ? 'image' or data ? 'rate' ?column? ---------------------------- \u0026quot;The Shawshank Redemption\u0026quot; (1 row) n3xt_pg=# select data-\u0026gt;'title' from filmsjsonb where data ?\u0026amp; '{\u0026quot;image\u0026quot;, \u0026quot;rat\u0026quot;}';\t-- 相当于 data ? 'image' and data ? 'rate' ?column? ---------- (0 rows) n3xt_pg=# select data-\u0026gt;'title' from filmsjsonb where data ?\u0026amp; '{\u0026quot;image\u0026quot;, \u0026quot;rating\u0026quot;}'; ?column? ---------------------------- \u0026quot;The Shawshank Redemption\u0026quot; (1 row) ? 仅用来检查 key 存在，那么 @\u0026gt; 和 \u0026lt;@ 可以检查子串的功能。\nn3xt_pg=# select '{\u0026quot;a\u0026quot;:1, \u0026quot;b\u0026quot;:2}'::jsonb @\u0026gt; '{\u0026quot;b\u0026quot;:2}'::jsonb; ?column? ---------- t (1 row) n3xt_pg=# select '{\u0026quot;b\u0026quot;:2}'::jsonb \u0026lt;@ '{\u0026quot;a\u0026quot;:1, \u0026quot;b\u0026quot;:2}'::jsonb; ?column? ---------- t (1 row) 9.4 同样也带来了 GIN 索引类型，它覆盖所有 JSONB 文段中的字段。你还可以创建带 json_path_ops 的 GIN 索引类型，它更快，更小，但是只能用于 @\u0026gt; 包含操作符，用来检查子串很有用。\n因此，你可以使用 9.4 创建，检索和索引 JSON/JSONB 数据。同时，也失去对了修改 JSON 类型数据的能力。你还可以把 JSON 数据传递给 PLv8 或者 PLPerl 脚本处理。因此，这些东西已经接近一个完整服务的 JSON 文档处理环境，但是远远还不够。\n进入 9.5 # PostgreSQL-9.5 引入了处理 JSON 的新能力：修改和操作 JSONB 数据。先来看看 jsonb_pretty() 函数，打印更可读的 JSON：\nn3xt_pg=# select jsonb_pretty('{\u0026quot;a\u0026quot;:3,\u0026quot;b\u0026quot;:2}'::jsonb); jsonb_pretty -------------- { + \u0026quot;a\u0026quot;: 3, + \u0026quot;b\u0026quot;: 2 + } (1 row) 开删 # 最简单的修改莫过于删除了。为了这个，9.5 引入了 - 和 #- 操作符。- 后面带上 key，代表删除 JSON 的这个 key（如果是数组，则是跟着一个整型索引）。现在，我们来试验下：\nn3xt_pg=# select jsonb_pretty(data) from filmsjsonb; jsonb_pretty --------------------------------------------------------------------------------------------------------------- { + \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, + \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, + \u0026quot;image\u0026quot;: { + \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;,+ \u0026quot;width\u0026quot;: 933, + \u0026quot;height\u0026quot;: 1388 + }, + \u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, + \u0026quot;rating\u0026quot;: 9.3, + \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, + \u0026quot;can_rate\u0026quot;: true, + \u0026quot;num_votes\u0026quot;: 1566874 + } (1 row) n3xt_pg=# update filmsjsonb set data=data-'rating'; UPDATE 1 #- 以路径作为索引。\nn3xt_pg=# update filmsjsonb set data=data#-'{image,width}'; UPDATE 1 n3xt_pg=# update filmsjsonb set data=data#-'{image,height}'; UPDATE 1 n3xt_pg=# select jsonb_pretty(data) from filmsjsonb; jsonb_pretty -------------------------------------------------------------------------------------------------------------- { + \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, + \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, + \u0026quot;image\u0026quot;: { + \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;+ }, + \u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, + \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, + \u0026quot;can_rate\u0026quot;: true, + \u0026quot;num_votes\u0026quot;: 1566874 + } (1 row) 上一个例子中，需要执行2次，会不会觉得很蛋疼呢？还好，PostgreSQL 提供了一个简便的方法来处理：\nn3xt_pg=# update filmsjsonb set data#-\u0026rsquo;{image,height}\u0026rsquo;#-\u0026rsquo;{image,width}\u0026rsquo;;\nUPDATE 1\n你不仅可以在删除数据时使用，你还可以在输出中使用这些函数（pipeline 的思想，是不是很有同感！）：\nn3xt_pg=# select jsonb_pretty(data#-'{image,height}'#-'{image,width}') from filmsjsonb where id=1; jsonb_pretty -------------------------------------------------------------------------------------------------------------- { + \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, + \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, + \u0026quot;image\u0026quot;: { + \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;+ }, + .... 合并 # 另一个重要的数据操作就是合并操作 ||；它合并两个 JSONB 对象。它只能合并顶级的 key，如果两边都存在，它会选择右边那个。这意味着你也可以使用它作为一个更新机制（Replace）。开始，现在想要给我们的电影数据添加两个 key，并赋予初始值：\nn3xt_pg=# update filmsjsonb set data=data || '{\u0026quot;can_rate\u0026quot;:false,\u0026quot;num_votes\u0026quot;:0,\u0026quot;revote\u0026quot;:true }'; UPDATE 1 n3xt_pg=# select jsonb_pretty(data) from filmsjsonb; jsonb_pretty -------------------------------------------------------------------------------------------------------------- { + \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, + \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, + \u0026quot;image\u0026quot;: { + \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;+ }, + \u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, + \u0026quot;revote\u0026quot;: true, + \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, + \u0026quot;can_rate\u0026quot;: false, + \u0026quot;num_votes\u0026quot;: 0 + } (1 row) 它通常用于合并 JSONB 数据。如果使用它来更新一个 key，那似乎就有点 overkill；所以接下里我们将要看到杀手级的函数：\njsonb_set 来帮你 # jsonb_set 就是设计用来更新单一 key 值的。直接看例子吧：\nn3xt_pg=# update filmsjsonb SET data = jsonb_set(data,'{\u0026quot;image\u0026quot;,\u0026quot;width\u0026quot;}',to_jsonb(1024)) where id=1; UPDATE 1 它把 image.width 的值修改成 1024。jsonb_set 的参数很简单：第一个就是你要修改的 JSONB 数据类型字段；第二个是一个文本数组，用来指定修改的路径；第三个参数是要替换值（可以是 JSON）。如果给的路径不存在，json_set() 默认会创建他；如果想要禁用这个行为，那就把第四个参数设置成 false。\n现在我们想为图片添加版权属性：\nn3xt_pg=# update filmsjsonb SET data = jsonb_set(data,'{\u0026quot;image\u0026quot;,\u0026quot;quality\u0026quot;}','{\u0026quot;copyright\u0026quot;:\u0026quot;company X\u0026quot;,\u0026quot;registered\u0026quot;:true}'); UPDATE 1 n3xt_pg=# select jsonb_pretty(data) from filmsjsonb; jsonb_pretty --------------------------------------------------------------------------------------------------------------- { + \u0026quot;type\u0026quot;: \u0026quot;feature\u0026quot;, + \u0026quot;year\u0026quot;: \u0026quot;1994\u0026quot;, + \u0026quot;image\u0026quot;: { + \u0026quot;url\u0026quot;: \u0026quot;http://ia.media-imdb.com/images/M/MV5BODU4MjU4NjIwNl5BMl5BanBnXkFtZTgwMDU2MjEyMDE@._V1_.jpg\u0026quot;,+ \u0026quot;width\u0026quot;: 1024, + \u0026quot;quality\u0026quot;: { + \u0026quot;copyright\u0026quot;: \u0026quot;company X\u0026quot;, + \u0026quot;registered\u0026quot;: true + } + }, + \u0026quot;title\u0026quot;: \u0026quot;The Shawshank Redemption\u0026quot;, + \u0026quot;revote\u0026quot;: true, + \u0026quot;tconst\u0026quot;: \u0026quot;tt0111161\u0026quot;, + \u0026quot;can_rate\u0026quot;: false, + \u0026quot;num_votes\u0026quot;: 0 + } (1 row) jsonb_set() 可能是 9.5 版本关于 JSONB 的最重要更新了。他为我们提供修改 JSONB 数据的方法。另外需要记住的是，我们的例子只是使用简单的值；它还支持子查询。\n思考 # 所有的这些造就了 PostgreSQL 今日的有趣地位。9.5 对 PostgreSQL 的 JSON 的加强，意味着你可以使用 PostgreSQL 作为 JSON 数据库；它足够快，功能强大。你所需要的就是使用不同角度的思考。\n例如，很多 JSON 数据库没有相对简洁的 API 或者客户端库可用。这里，PosgreSQL 有自己的领域语言，SQL，来操作 JSON；它能和 SQL 一起爆发出强大的力量。这意味着你仍然需要学习 SQL，不幸的是，很多人希望把它作为 NoSQL 数据库使用。\n你可以使用 PostgreSQL 创建复杂的 JSON/JSONB 文档。但是如果你这么做的话，你可能需要思考下你是否能更好地使用它。如果文档的复杂度（比如嵌套的 JSON）来源于文档之间的关系，那么关系型模型可能是解决数据缠绕的更好选择。关系型数据模型还有个好处，就是避免了数据重复（三范式）。\nPostgreSQL 对 JSON 的完美支持消除了关系型环境中处理 JSON 数据的障碍，添加更多易用的，内建的有效函数和操作符来操作 JSONB 数据库。\nPostgreSQL-9.5 不是你的下一个 JSON 数据库，但他是一个带着完整 JSON 存储方案的关系型数据库。强化 JSON 处理的同时，还做了其他的不少改进，如 upsert，skip locked 以及更好的表随机等等。\n它可能不是你的下一个 JSON 数据库，但是 PostgreSQL 将会是下一个你可以同时用于处理关系型和 JSON 数据的数据库。\n译自 Could PostgreSQL 9.5 be your next JSON database?\n","date":"2016-04-24","permalink":"/n3xtchen/2016/04/24/postgresql---json/","section":"时间线","summary":"TL;DR: 是的，但这不是一个好的问题。","title":"PostgreSQL - 9.5 会成为你的下一个 JSON 数据库?"},{"content":"","date":"2016-03-08","permalink":"/n3xtchen/tags/d3/","section":"标签","summary":"","title":"d3"},{"content":"","date":"2016-03-08","permalink":"/n3xtchen/tags/react/","section":"标签","summary":"","title":"react"},{"content":"使用一个小小的例子来演示如何把 D3.js 数据可视化到 React 应用中。\n我最近在使用 D3.js 和 React。因此，我想要分享使用这两个库创建组件和交互接口的几种方式。我希望它们能帮助你更优雅地实现两个库的整合。\n这里，假设你有 React 和 D3.js 的基础知识。\n三大准则 # 首先，我认为 React 和 D3.js 的整合是可行的，因为它们都有共同的哲学（给我一些数据，告诉我如何显示，然后我将自行计算哪一些 DOM 需要被更新）。确实，React 有 Virtual DOM diffs，而 D3.js 有 update selections；它们都是在同步 UI 和数据同步的逻辑上相当有效。\n现在我们看看下面三条准则，就会发现 D3 作为组件可以很好地融合到 React 中：\nOne Source Of Truth（真正的单一源）：D3 可视化要求获取所有它需要渲染的数据。React 组件的 state，它既能被 D3 组件使用，也可以被其他的 React 组件使用。 Stateless All The Things(一切事务皆无状态)：D3 和 React 组件都需要尽可能的无状态，换句话说，相同的输入只会产生相同的渲染。 Don\u0026rsquo;t Make Too Many Assumptions(不要做太多假设)：组件在使用方法上不应该做太多的假设。这一点，当提示需要显示，它不需要规定，它只要要求它是否接受 tooltips 数据。这个让我们显示工具提示的显示，同样也很容易创建显示隐藏的切换。 太理论了，让我们写一些例子。\n第一个基础例子 # 我们将调用 D3.js 组件。让我们定义他的公用接口，同时也能展示它的生命周期：\nvar d3Chart = {}; d3Chart.create = function(el, props, state) { var svg = d3.select(el).append('svg') .attr('class', 'd3') .attr('width', props.width) .attr('height', props.height); svg.append('g') .attr('class', 'd3-points'); this.update(el, state); }; d3Chart.update = function(el, state) { // Re-compute the scales, and render the data points var scales = this._scales(el, state.domain); this._drawPoints(el, scales, state.data); }; d3Chart.destroy = function(el) { // Any clean-up would go here // in this example there is nothing to do }; 注意到了吗？D3.js 组件是完全无状态的（二条准则）；例如，它不能关联其他东西，数据都来源传值。我发现这么做使它更容易融入其他语境中（在我们的例子中，我把它放在 React 组件里）。\n_drawPionts() 函数是 D3 的最常见使用方式（enter，update 和 exit 模式）：\n// d3Chart.js d3Chart._drawPoints = function(el, scales, data) { var g = d3.select(el).selectAll('.d3-points'); var point = g.selectAll('.d3-point') .data(data, function(d) { return d.id; }); // ENTER point.enter().append('circle') .attr('class', 'd3-point'); // ENTER \u0026amp; UPDATE point.attr('cx', function(d) { return scales.x(d.x); }) .attr('cy', function(d) { return scales.y(d.y); }) .attr('r', function(d) { return scales.z(d.z); }); // EXIT point.exit() .remove(); }; 现在，我在 React 组件 \u0026lt;chart\u0026gt; 中实现：\n// Chart.js var d3Chart = require('./d3Chart'); var Chart = React.createClass({ propTypes: { data: React.PropTypes.array, domain: React.PropTypes.object }, componentDidMount: function() { var el = this.getDOMNode(); d3Chart.create(el, { width: '100%', height: '300px' }, this.getChartState()); }, componentDidUpdate: function() { var el = this.getDOMNode(); d3Chart.update(el, this.getChartState()); }, getChartState: function() { return { data: this.props.data, domain: this.props.domain }; }, componentWillUnmount: function() { var el = this.getDOMNode(); d3Chart.destroy(el); }, render: function() { return ( \u0026lt;div className=\u0026quot;Chart\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; ); } }); 我们把 D3 图表的 create，update，destroy 函数映射到 React 组件的生命周期函数 componentDidMount，componentDidUpdate，componentWillUnmount。\n最后，创建一个 React 组件应用 \u0026lt;App\u0026gt;，然后使用 \u0026lt;Chart\u0026gt; 来为一些数据描点：\n// App.js var Chart = require('./Chart'); var sampleData = [ {id: '5fbmzmtc', x: 7, y: 41, z: 6}, {id: 's4f8phwm', x: 11, y: 45, z: 9}, // ... ]; var App = React.createClass({ getInitialState: function() { return { data: sampleData, domain: {x: [0, 30], y: [0, 100]} }; }, render: function() { return ( \u0026lt;div className=\u0026quot;App\u0026quot;\u0026gt; \u0026lt;Chart data={this.state.data} domain={this.state.domain} /\u0026gt; \u0026lt;/div\u0026gt; ); } }); React.renderComponent(App(), document.body); 看见了吧！我们现在拥有一张漂亮的圆点图了\n添加分页和统计挂件（Widget） # 在前面，我们写了一个 React 组件 \u0026lt;App\u0026gt;；它遵循 One Source Of Truth（准则 #1）。并且，我们还拥有一个 D3 图表渲染组件：你传递一个带 data 和 domain 属性的对象，进行渲染。\n首先，我们增加分页（** pagination**）控制；这样，它就能帮助我们探索更大的数据集。我们创建一个 React 组件 \u0026lt;Pagination\u0026gt;，它根据用户点击 Next 或 Previous 按钮来控制数据的显示：\n// Pagination.js var Pagination = React.createClass({ propTypes: { domain: React.PropTypes.object, getData: React.PropTypes.func, setAppState: React.PropTypes.func }, render: function() { return ( \u0026lt;p\u0026gt; {'Pages: '} \u0026lt;a href=\u0026quot;#\u0026quot; onClick={this.handlePrevious}\u0026gt;Previous\u0026lt;/a\u0026gt; \u0026lt;span\u0026gt; - \u0026lt;/span\u0026gt; \u0026lt;a href=\u0026quot;#\u0026quot; onClick={this.handleNext}\u0026gt;Next\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; ); }, handlePrevious: function(e) { e.preventDefault(); this.shiftData(-20); }, handleNext: function(e) { e.preventDefault(); this.shiftData(+20); }, shiftData: function(step) { var newDomain = _.cloneDeep(this.props.domain); newDomain.x = _.map(newDomain.x, function(x) { return x + step; }); var newData = this.props.getData(newDomain); this.props.setAppState({ data: newData, domain: newDomain }); } }); 我们那这个功能添加到 \u0026lt;App\u0026gt; 中：\n// App.js var Pagination = require('./Pagination'); var App = React.createClass({ getInitialState: function() { var domain = [0, 30]; return { data: this.getData(domain), domain: {x: domain, y: [0, 100]}, }; }, _allData: [/* some big dataset, too much to display at once */], getData: function(domain) { return _.filter(this._allData, this.isInDomain.bind(null, domain)); }, isInDomain: function(domain, d) { return d.x \u0026gt;= domain[0] \u0026amp;\u0026amp; d.x \u0026lt;= domain[1]; }, render: function() { return ( \u0026lt;div className=\u0026quot;App\u0026quot;\u0026gt; \u0026lt;Pagination domain={this.domain} getData={this.getData} setAppState={this.setAppState} /\u0026gt; \u0026lt;Chart data={this.state.data} domain={this.state.domain} /\u0026gt; \u0026lt;/div\u0026gt; ); }, setAppState: function(partialState, callback) { return this.setState(partialState, callback); } }); 现在，我们已经实现它了！\u0026lt;Pagination\u0026gt; 组件通过使用 setAppState() 函数来改变我们的状态；只要有新的数据传入，就会重新渲染。如果我们想要去掉这个功能，或者其他的 widget，我们只需要删除 App.render() 中的 \u0026lt;Pagination ... /\u0026gt; 部分。\n我们还可以为 App.state.data 添加新的东西。例如，我们添加 \u0026lt;Stats\u0026gt; 挂件，让它展示一些必要的统计数据：\n// Stats.js var Stats = React.createClass({ propTypes: { data: React.PropTypes.array }, render: function() { var data = this.props.data; return ( \u0026lt;div className=\u0026quot;Stats\u0026quot;\u0026gt; {this.renderCount(data)} {this.renderAverage(data)} \u0026lt;/div\u0026gt; ); }, renderCount: function(data) { return ( \u0026lt;div className=\u0026quot;Stats-item\u0026quot;\u0026gt; {'Count: '}\u0026lt;strong\u0026gt;{data.length}\u0026lt;/strong\u0026gt; \u0026lt;/div\u0026gt; ); }, renderAverage: function(data) { var avg; var n = data.length; if (!n) { avg = '-'; } else { var sum = _.reduce(data, function(sum, d) { return sum + d.z; }, 0); avg = Math.round(sum/n * 10)/10; } return ( \u0026lt;div className=\u0026quot;Stats-item\u0026quot;\u0026gt; {'Average size: '}\u0026lt;strong\u0026gt;{avg}\u0026lt;/strong\u0026gt; \u0026lt;/div\u0026gt; ); } }); 然后，在 \u0026lt;App\u0026gt; 组件中，我们把 \u0026lt;Stats\u0026gt; 组件放进去：\n// App.js var Stats = require('./Stats'); var App = React.createClass({ // ... render: function() { return ( \u0026lt;div className=\u0026quot;App\u0026quot;\u0026gt; \u0026lt;Pagination domain={this.domain} getData={this.getData} setAppState={this.setAppState} /\u0026gt; \u0026lt;Chart data={this.state.data} domain={this.state.domain} /\u0026gt; \u0026lt;Stats data={this.state.data} /\u0026gt; \u0026lt;/div\u0026gt; ); } }); 感谢 One Source Of Truth，所展示的统计数据永远和 D3 图表相关联。\n添加工具提示（tooltip） # 最后，我为我们的界面添加 tooltip，来展示每一个圆点的数值。\n我们希望我们悬停在某个圆点上，该圆点的 tooltip 展示相应数值。由于 D3 图表创建的元素与圆点关联，所以我们需要一些方法来告诉 d3Chart 的父节点把鼠标悬停事件挂载在圆点上。我有几种方法可以用。这里，我们只是使用简单的 Node.js EventEmitter 来调用 dispatcher：\n// d3Chart.js var EventEmitter = require('events').EventEmitter; d3Chart.create = function(el, props, state) { // ... var dispatcher = new EventEmitter(); this.update(el, state, dispatcher); return dispatcher; }; d3Chart.update = function(el, state, dispatcher) { // ... this._drawPoints(el, scales, state.data, dispatcher); }; d3Chart._drawPoints = function(el, scales, data, dispatcher) { // ... // ENTER \u0026amp; UPDATE point.attr('cx', function(d) { return scales.x(d.x); }) .attr('cy', function(d) { return scales.y(d.y); }) .attr('r', function(d) { return scales.z(d.z); }); .on('mouseover', function(d) { dispatcher.emit('point:mouseover', d); }) .on('mouseout', function(d) { dispatcher.emit('point:mouseout', d); }); // ... }; 注意到了吗？ d3Chart 从 upstream 接受 domain 和 data，例如 它的父节点 \u0026lt;Chart\u0026gt; 和 \u0026lt;App\u0026gt;。我把这个过程当做一个数据下发的过程。我们可以使用 dispatcher 发送 mouseover 和 mouseout 事件和相关的数据回传给 upstream。而这个过程就是数据回流。\n解决这么多问题，就仅仅为了正确的展示 tooltip 吗？当然不是，我是为了演示第三准则（Don\u0026rsquo;t Make Too Many Assumptions），因此我们不想假设这段代码紧紧用于监听圆点的鼠标悬停事件，来展示它的 tooltip 。我们只是提供信息 “Hey， this circle was hovered”。\n另外，如果我们想直接显示 tooltip，我们将在 d3Chart 引入状态，这个违背了第二准则（Stateless All The Things）的指南。确实，相同的 domain 和 data，图表可能渲染会不同（无论圆点被悬停与否）。外部代码没办法知道图表渲染的状态。\n因此，我们需要鼠标事件回传数据，让我们做一些事。我们把 tooltip 对象添加到我们的 One Source Of Truth：\n// App.js var App = React.createClass({ getInitialState: function() { var domain = [0, 30]; return { data: this.getData(domain), domain: {x: domain, y: [0, 100]}, tooltip: null }; }, // ... }); 然后在 \u0026lt;Chart\u0026gt;中，我们通过 dispatcher 监听鼠标事件来更新 tooltip 对象：\n// Chart.js var Chart = React.createClass({ propTypes: { data: React.PropTypes.array, domain: React.PropTypes.object, setAppState: React.PropTypes.func }, dispatcher: null, componentDidMount: function() { var el = this.getDOMNode(); var dispatcher = d3Chart.create(el, { width: '100%', height: '300px' }, this.getChartState()); dispatcher.on('point:mouseover', this.showTooltip); dispatcher.on('point:mouseout', this.hideTooltip); this.dispatcher = dispatcher; }, componentDidUpdate: function(prevProps, prevState) { var el = this.getDOMNode(); d3Chart.update(el, this.getChartState(), this.dispatcher); }, // ... showTooltip: function(d) { this.props.setAppState({tooltip: d}); }, hideTooltip: function() { this.props.setAppState({tooltip: null}); } }); 很好，但是我们还没显示人和提示控件。让我们把一个 tooltips 数组传递给 d3Chart.update()（同时也是用 D3 绘制 tooltip 的函数）：\n// d3Chart.js d3Chart.update = function(el, state, dispatcher) { // ... this._drawTooltips(el, scales, state.tooltips); }; d3Chart._drawTooltips = function(el, scales, tooltips) { var g = d3.select(el).selectAll('.d3-tooltips'); var tooltipRect = g.selectAll('.d3-tooltip-rect') .data(tooltips, function(d) { return d.id; }); // ENTER tooltipRect.enter().append('rect') .attr('class', 'd3-tooltip-rect') .attr('width', TOOLTIP_WIDTH) .attr('height', TOOLTIP_HEIGHT); // ENTER \u0026amp; UPDATE tooltipRect.attr('y', function(d) { return scales.y(d.y) - scales.z(d.z)/2 - TOOLTIP_HEIGHT; }) .attr('x', function(d) { return scales.x(d.x) - TOOLTIP_WIDTH/2; }); // EXIT tooltipRect.exit() .remove(); var tooltipText = g.selectAll('.d3-tooltip-text') .data(tooltips, function(d) { return d.id; }); // ENTER tooltipText.enter().append('text') .attr('class', 'd3-tooltip-text') .attr('dy', '0.35em') .attr('text-anchor', 'middle') .text(function(d) { return d.z; }); // ENTER \u0026amp; UPDATE tooltipText.attr('y', function(d) { return scales.y(d.y) - scales.z(d.z)/2 - TOOLTIP_HEIGHT/2; }) .attr('x', function(d) { return scales.x(d.x); }); // EXIT tooltipText.exit() .remove(); }; 注意下 Don\u0026rsquo;t Make Too Many Assumption 的另一个实例。D3 图表请求 tooltips 数组（对比单个 tooltip 对象），因为谁告诉你不能够一次展示多个提示控件？（当然这个过程中，我们将只看到一个点）。\n让我们在 \u0026lt;Chart\u0026gt; 中创建这个 tooltips 数组，连同 domain 和 data 一起传递给 D3 图表：\n// Chart.js var Chart = React.createClass({ propTypes: { data: React.PropTypes.array, domain: React.PropTypes.object, tooltip: React.PropTypes.object, setAppState: React.PropTypes.func }, // ... componentDidUpdate: function(prevProps, prevState) { var el = this.getDOMNode(); d3Chart.update(el, this.getChartState(), this.dispatcher); }, getChartState: function() { return { data: this.props.data, domain: this.props.domain, tooltips: [this.props.tooltip] }; }, // ... }); 噢啦！我们现在拥有悬停的 tooltip 了：\n接下来，你就可以看到让事物无状态，不做过多假设的好处！让我们添加一个带按钮的 widget 来展示/隐藏所有 tooltip。我们所需要东西都已经准备就绪，剩下就很简单了！\n我们添加布尔型属性 showingAllTooltips 到我们的 One Source Of Truth：\n// App.js var App = React.createClass({ getInitialState: function() { var domain = [0, 30]; return { data: this.getData(domain), domain: {x: domain, y: [0, 100]}, tooltip: null, showingAllTooltips: false }; }, // ... }); 我们创建一个 React 组件 \u0026lt;ShowHideTooltips\u0026gt; 来切换 showingAllTooltips 状态（实现这个挂件的代码不是很有趣，所以我就不在这里贴出来了）。\n最后，我们调整下我们构建 tooltips 数组的传递方式：\n// Chart.js var Chart = React.createClass({ propTypes: { data: React.PropTypes.array, domain: React.PropTypes.object, tooltip: React.PropTypes.object, showingAllTooltips: React.PropTypes.bool, setAppState: React.PropTypes.func }, // ... getChartState: function() { var tooltips = []; if (this.props.showingAllTooltips) { tooltips = this.props.data; } else { tooltips = [this.props.tooltip]; } return { data: this.props.data, domain: this.props.domain, tooltips: tooltips }; }, // ... }); 搞定，只需要加几行代码就实现了 显示/隐藏所有提示控件 的功能：\n结语 # 实际上有很多种方式来组合 React 和 D3 的方式，也有很多现成工具和库供你使用。我展示的只是其中一种方式，是我个人认为更容易上手的一种方式。我相信一定还有其他种的方式，因此希望你也能分享下你自己的方法吧！\n完整代码： github 演示地址： demo\n译自 Integrating D3.js visualizations in a React app\n","date":"2016-03-08","permalink":"/n3xtchen/2016/03/08/reactd3/","section":"时间线","summary":"使用一个小小的例子来演示如何把 D3.","title":"React\u0026 D3.js: 整合 D3.js 可视化组件到 React 应用中"},{"content":"","date":"2016-02-19","permalink":"/n3xtchen/tags/nginx/","section":"标签","summary":"","title":"nginx"},{"content":"","date":"2016-02-19","permalink":"/n3xtchen/categories/nginx/","section":"分类页","summary":"","title":"Nginx"},{"content":" 介绍 # 这里，我们将介绍 Nginx 的 Http 代理功能（请求（request） 通过 Nignx 传递到后端服务器，进行后续处理）。Nginx 经常设置为 反向代理（Reverse Proxy） 帮助 横向扩展（scale out：通过增加独立服务器来增加运算能力） 基础架构（infrastructure） 来提升负载能力或者传递请求给下一级代理服务器。\n接下来，我们将讨论如何使用 Nginx 的 负载均衡（load balance ） 功能来 横向扩展（scale out） 服务器。我们同时还会探讨使用 缓冲（buffering） 和 缓存（caching） 技术来提升代理性能。\n常规的代理信息 # 如果你之前只是部署单台 Web 服务器，那你可能会想知道为什么需要代理请求。\n横向扩展（scale out） 提升 基础架构（infrastructure） 的能力是使用代理的原因之一。Nignx 的设计初衷就是被用来处理并发请求，是客户端接触点的理想选择。代理服务器可以传递 request 给多个能处理大量任务的后端服务器，达到跨设备分散负载的目的。这样的设计同样也能帮助你更容易得添加服务器或者下线需要维护的服务器。\n当应用服务器没有直接处理 request 的能力的时候，代理服务器就可以发挥作用了。很多框架（包括 Web 服务器）不如专门设计成高性能服务器（如 Nignx）那样健壮。这种场景下，把 Nginx 放在这些服务之前，可以提升用户体验和安全性。\nNigix 通过接收 request，把它转发给其他服务器处理来完成代理过程的。request 的处理结果会返回 Nginx，然后转发给客户端。实例中的其他服务器可以是远程机器，本地服务，甚至是由 Nginx 定义的其他虚拟服务。由 Nginx 代理的服务器，我们称之为 upstream（上游）服务。\nNginx 可以代理使用 http(s), FastCGI, SCGI 和 uwsgi 的请求，或者为每种代理类型指定不同指令的 memcached 协议。在这个指南中，我们专注于 http 协议。 Nginx 实例负责传递 request，并把各个信息融合成一个 upstream 可理解的格式。\n解构一个简单的 HTTP 代理传递过程 # 最简单的代理类型莫过于把一个 request 导向到单一使用 http 协议通信的服务器了。我们把这类代理统称为 proxy pass，由 proxy_pass 指令处理。\nproxy_pass 指令主要在 location 的 context（中文含义：语境或上下文） 中使用。它还可以在 location 和 limit_except 的 context 的 if 语法块中使用。当 request 匹配到一个包含 proxy_pass 的 location 中时， 该 request 将会被指令转发（Forward）到这个链接去。\n让我们看一个例子：\n# server context location /match/here { proxy_pass http://example.com; } . . . 在上面代码片段中，proxy_pass 语句中的服务器地址没有提供 URI。在该模式下， request 的 URI 会原封不动地直接传递给 upstream 服务器。来看个例子：\nNginx 所接受的 request 的原始 URI: /match/here/please example.com 从 Nginx 接收到的形式：http://example.com/match/here/please 让我们一起看看另外一个场景：\n# server context location /match/here { proxy_pass http://example.com/new/prefix; } . . . 上述例子中，代理服务器在尾部定义了 URI。当 URI 放到 proxy_pass 定义中时， request 中匹配这个 location 的部分会在传递的过程中将会被这个 URI 直接覆盖掉，再来看个例子：\nNginx 所接受的 request 的原始 URI: /match/here/please upstream 服务器 从 Nginx 接收到的形式: http://example.com/new/prefix/please，这里 /match/here 被替换成 /new/prefix 有时，这样的替换是失效。这时，定义在 proxy_pass 的尾部的 URI 会被忽略，Nginx 直接把来自客户端或被其他 Nginx 的指令修改的 URI 传递给 upstream 服务器。\n例如，使用正则表达式匹配 location， URI 的匹配出现争议时，Nginx 直接发送客户端 upstream 的原始 URI。还有另一个例子，当一个 rewrite 指令在同一个地址中使用，会导致客户端的 URI 被重写，但是仍然在同一个 block 下处理。这时，传递的 URI 是重写后的。\n理解 Nginx 处理 Headers 的方式 # 如果你希望 upstream 能合理地处理 request ，那仅仅传递 URI 是不够的。来自于 Nginx 的 request 和直接来源于客户端的 request 之间还是有区别的。这里最大的差异来自于 request 的 Headers（头信息）。\n当 Nginx 代理一个 request ，它会自动对 Headers 做一些调整：\nNginx 会去除任何空的 Headers。转发空值是没有意义的；它只会让 request 变得臃肿。 Nginx 默认把名称包含下划线的 Headers 视为无效，直接移除。如果你希望让这类型的信息生效，那你要把 underscores_in_headers 指令设置成 on，否则这样的头信息将不会把他发送给后端服务器。 Host 会被重写成由 $proxy_host 定义的值。它可以是由 proxy_pass 指令定义的 upstream 的 IP （或者名称）和端口。 Headers 中的 Connection 改成 close。这个 Headers 用在两个服务器创建特定连接的信号信息。在这个实例中，Nginx 把它设置成 close，一旦原始 request 被响应，upstream 的这个连接将被关闭。upstream 不应该期望这个连接被持久化。 从第一点看来，我们可以确定任何不希望被转发的 Headers 都应该被设置成空字符串。带空值的 Headers 会被完全删除掉。\n接下来一点用来设置如果你的后端应用想要接受非标准的 Headers，你应该确保它们不应该带下划线。如果你需要的 Headers 使用了下划线，你你需要把 underscores_in_headers 指令设置成 on（在 http 的 context 或者为这个 IP 和端口组合声明的默认服务器的 context 中有效）。如果你不想这么做，Nginx 将会把这类 Headers 标记为无效，并在传递给 upstream 之前把它丢弃。\nHeaders 的 Host 在大部分代理场景中都起着重要作用，它默认被设置成 $proxy_host 的值，一个由 proxy_pass 定义的包含 IP 或名称和端口的值。这样的默认设定是为了让 Nginx 确保 upsteam 可以响应的地址是唯一的（它直接从连接信息取出）。\nHost 常见的值如下：\n$proxy_host：它把 Host 设置成从 proxy_pass 定义的 IP 或名称加上端口的组合。从 Nginx 的角度看，它是默认以及安全的，但是经常不是被代理服务器需要的来正确处理请求的值。 $http_host：它把 Host 设置成客户端 request 的 Headers 中相关信息。这个 Headers 由客户端发送，可以被 Nginx 使用。这个变量名前缀是 $http_，后面紧跟着 Headers 的名称，以小写命名，任何斜杠都会被下划线替换。虽然 $http_host 在大部分情况可用的，但是当客户端的 request 没有有效的 Host 信息的时候，会导致传输失败。 $host：这个是偏好设置，它可以是来自 request 的主机名，请求中的 Host 或者匹配请求的服务器名。 在大部分情况，你将会把 Host 设置成 $host 变量。它是最灵活的，经常为被代理的服务器提供尽可能精确的 Host 信息。\n配置或者重置 Headers # 为了适配代理连接，我们可以使用 proxy_set_header 指令。例如，为了改变我们之前讨论的 Host 以及其它的 Headers 中的配置，我们可以这么做：\n# server context location /match/here { proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://example.com/new/prefix; } . . . 上述配置把 request 中 Headers 的 Host 设置成 $host 变量，它将包含 request 的原始主机名。Headers 的 X-Forwarded-Proto 提供了关于原始 request 的 Headers 中被代理服务器协议（http 还是 https）。\nX-Real-IP 被设置成客户端的 IP 地址，以便代理服务器做判定或者记录基于该信息的日志。X-Forwarded-For 是一个包含整个代理过程经过的所有服务器 IP 的地址列表。在上述例子中，我们把它设置成 $proxy_add_x_forwarded_for 变量。这个变量包含了从客户端获取的 X-Forwarded-For 和 Nginx 服务器的 IP（按照 request 的顺序）。\n当然，我们也可以把 proxy_set_header 指令移到 server 或者 http 的 context 中，让它同时在该 context 的多个 location 中生效：\n# server context proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_Header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location /match/here { proxy_pass http://example.com/new/prefix; } location /different/match { proxy_pass http://example.com; } 为负载均衡代理服务器定义 Upstream 语境（Context） # 在上一个例子中，我们演示了如何为了一个单台后端服务器实现简单的 http 代理。Nginx 让我们很容易通过指定一个后端服务器集群池子来扩展这个配置。\n我们使用 upstream 指令来定义服务器群的池子（pool）。这个配置假设服务器列表中的每台机子都可以处理来自客户端的 request。我们可以通过它轻轻松松地 横向扩展（scale out） 我们的 基础架构（infrastructure）。upstream 指令必须定义在 Nginx 的 http 的 context 中。\n让我们一起看个简单的例子：\n# http context upstream backend_hosts { server host1.example.com; server host2.example.com; server host3.example.com; } server { listen 80; server_name example.com; location /proxy-me { proxy_pass http://backend_hosts; } } 上述例子，我们设置一个叫做 backend_hosts 的 upstream context。一旦定义了，这个名称可以直接在 proxy_pass 中使用，就和常规的域名一样。如你所见，在我们的服务器块内，所有指向 example.com/proxy-me/\u0026hellip; 的 request 都会被传递到我们定义的池子中。在池子里，会根据配置的算法选取一台服务器。默认，它只是一个简单的round-robin（循环选择） 处理（即每一个请求都会按顺序传递给不同的服务器）。\n改变 Upstream 均衡算法 # 你可以通过指令修改 upstream 池子的 均衡算法（balancing algorithm）：\nround-robin（循环选择）：默认的算法。在其它算法没被指定的情况下，它会被使用。upstream context 定义的每一个服务器都会按顺序接受 request。 least_conn（最少连接）: 指定新的 request 永远只会传递给拥有最少连接的后端服务器。在后端连接需要被持久化的情况下，这个算法将很有效。 ip_hash：这种 均衡算法（balancing algorithm） 是基于客户端的 IP 来分发 request 到不同的服务器。把客户端 IP 的前三位八进制数作为键值来决定由哪台服务器处理。这样，同一 IP 的客户端每次只会由同一个台服务器处理；它能保证 session（会话） 的一致性。 hash：这种 均衡算法（balancing algorithm） 主要运用于缓存代理。这是唯一一种需要用户提供数据的算法；算法根据用户所提供数据的哈希值来决定服务器的分配。它可以是文本，变量或者文本和变量的组合。 修改 均衡算法（balancing algorithm） ，应该像下面那样：\n# http context upstream backend_hosts { least_conn; server host1.example.com; server host2.example.com; server host3.example.com; } . . . 上述例子中，拥有最少连接数的服务器将会优先选择。ip_hash 指令也可以用同样的方式设置，来保证 session（会话） 的一致性。\n至于 hash 方法，你应该提供要哈希的键。可以是任何你想要的：\n# http context upstream backend_hosts { hash $remote_addr$remote_port consistent; server host1.example.com; server host2.example.com; server host3.example.com; } . . . 上述的例子，request 的分发是基于客户端的 IP 和端口；我们还可以添加另外的参数 consistent，它实现了 ketama consistent hashing 算法。基本上，它意味着如果你的 upstream 服务器改变了，可以保证对 cache（缓存） 的最小影响。\n设置服务器权重 # 在后端服务器声明中，每一台的服务器默认是权重平分的。它假定每一台服务器都能且应该处理同一量级的负载（考虑到 均衡算法（balancing algorithm） 的影响）。然而，你也可以为服务器设置其它的权重。\n# http context upstream backend_hosts { server host1.example.com weight=3; server host2.example.com; server host3.example.com; } . . . 上述例子中，host1.example.com 可以比其它服务器多接受 2 倍的流量。默认，每一台服务器的权重都是 1。\n使用 Buffer 缓解后端的负载 # 对于大部分使用代理的用户来说，最关心的事情之一就是增加一台服务器对性能的影响。在大部分场景下，利用 Nginx 的 buffer（缓冲） 和 cache（缓存） 能力，可以大大地减轻负担。\n在代理过程中，两个连接速度不一会对客户端的体验带来不良的影响：\n从客户端到代理服务器的连接 从代理服务器到后端服务器的连接 Nginx 可以根据你希望优化哪一个连接来调整它的行为。\n没有 buffer（缓冲），数据将会直接从代理服务器传输到客户端。如果客户端的速度足够快（假设），你可以直接把 buffer（缓冲） 关掉，让数据尽可能快速地到达；如果使用 buffer（缓冲），Nginx 将会临时存储后端 response（响应），然后慢慢把数据推送给客户端；如果客户端很慢，Nginx 会提前关闭后端服务器的连接。它可以任意控制分发的节奏。\nNginx 默认的 buffer（缓冲） 设计的初衷就是因为客户端之间速度存在差异。我们可以使用如下指令来调整 buffer（缓冲） 速度。你可以把 buffer（缓冲） 配置在 http，server 或者 location 的 context 中。必须注意指令为每一个，request 配置的大小；在客户端的 request 很多的情况下，如果把值调的过大，会很影响性能：\nporxy_buffering：这个指令控制所在 context 或者子 context 的 buffer（缓冲） 是否打开。默认是 on。 proxy_buffers：这个指令控制 buffer（缓冲） 的数量（第一个参数）和大小（第二个参数）。默认是 8 个 buffer（缓冲），每个 buffer（缓冲） 大小是 1 个内存页（4k 或 8k）。增加 buffer 的数量可以缓冲更多的信息。 proxy_buffer_size：是来自后端服务器 response 信息的一部分，它包含 Headers，从 response 分离出来。这个指令设置 response 的缓冲。默认，它和 proxy_buffers 一样，但是因为它仅用于 Headers，所有它的值一般设置得更低。 proxy_busy_buffer_size：这个指令设置忙时 buffer（缓冲） 的最大值。一个客户端一次只能从一个 buffer（缓冲） 中读取数据的同时，剩下的 buffer（缓冲） 会被放到队列中，等待发送到客户端。这个指令控制在这个状态下 buffer（缓冲） 的空间大小 proxy_max_temp_file_size：当代理服务器的 response 太大超出配置的 buffer（缓冲） 的时候，它来控制 Nginx 单次可以写入临时文件的最大数据量。 proxy_temp_path：当代理服务器的 response 太大超出配置的 buffer（缓冲） 的时候，Nginx 写临时文件的路径。 正如你看到的，Nginx 提供了这几个指令来调整 buffer（缓冲） 行为。大部分时间，你不需要关心这些指令中的大部分；但是它们中的一些会很有用，可能最有用的就是 proxy_buffer 和 proxy_buffer_size 这两个指令。\n下面这个例子中增加每个 upstream 可用代理 buffer（缓冲） 数，减少存储 Headers 的 buffer（缓冲） 数：\n# server context proxy_buffering on; proxy_buffer_size 1k; proxy_buffers 24 4k; proxy_busy_buffers_size 8k; proxy_max_temp_file_size 2048m; proxy_temp_file_write_size 32k; location / { proxy_pass http://example.com; } 相反，如果你的客户端足够快到你可以直接传输数据，你就可以完全关掉 buffer（缓冲）。实际上，即使 upstream 比客户端快很多，Nginx 还是会使用 buffer（缓冲） 的，但是它会直接清空客户端的数据，不会让它进入到 buffer（缓冲） 池子。如果客户端很慢，这会导致 upstream 连接会一直开到客户端处理完为止。当 buffer（缓冲） 被设置为 off 的时候，只有 proxy_buffer_size 指令定义的 buffer（缓冲） 会被使用。\n# server context proxy_buffering off; proxy_buffer_size 4k; location / { proxy_pass http://example.com; } 高可用性（可选） # 你可以通过添加一个冗余的负载均衡器来使 Nginx 代理更加健壮，创建一个高可用性基础设施。\n一个 高可用（HA） 的配置是一种容许单点错误（single point of failure）的基础设施，你的负载均衡是这个配置的一部分。当你的负载均衡器不可用或者你需要下线维护，你可以通过配置多个负载均衡器防止潜在的停机风险。\n这是基本高可用架构图：\n这个例子中，在静态 IP （它可以映射到一台或多台服务器）背后配置多个负载均衡器（一个是激活的，其它的一或多个是被动激活的）。客户端 request 从静态 IP 路由到激活的负载均衡器，然后到后端服务器。想了解更多，请阅读 this section of How To Use Floating IPs。\n配置代理缓存来减少响应时间 # buffer（缓冲） 帮助减轻后端服务器负担达到处理更多 request 目的的同时，Nginx 还提供一个从后端服务器缓存内容的功能，减少要连接 upstream 的次数。\n配置代理缓存 # 我们使用 proxy_cache_path 指令来为代理的内容设置缓存。它会创建一个直接用于代理服务器返回的数据存储区域。proxy_cache_path 指令必须在 http 的 context 中设置。\n下面例子中，我们将会配置这个和相关指令来设置我们的缓存系统。\n# http context proxy_cache_path /var/lib/nginx/cache levels=1:2 keys_zone=backcache:8m max_size=50m; proxy_cache_key \u0026quot;$scheme$request_method$host$request_uri$is_args$args\u0026quot;; proxy_cache_valid 200 302 10m; proxy_cache_valid 404 1m; 我们可以使用 proxy_cache_path 指令来定义缓存的存储路径。在这个例子，我使用 /var/lib/nginx/cache 这个路径。如果这个路径不存在，你需要创建这个目录，并赋予正确的权限：\nsudo mkdir -p /var/lib/nginx/cache sudo chown www-data /var/lib/nginx/cache sudo chmod 700 /var/lib/nginx/cache 参数 levels= 用来指定缓存的组织形式。Nginx 将会通过哈希键值创建一个缓存 key（在下方配置）。上述我们选择的 level 采用 2 级目录结构，内存空间的大小是 8m，假设我们的哈希键值为 b7f54b2df7773722d382f4809d65087c，那存储该内容的目录结构是：/var/lib/nginx/cache/backcache/c/87/b7f54b2df7773722d382f4809d65087c，注意到规律没有？参数 keys_zone= 定义缓存区域（我们称之为 backzone）的名称。这个也是我们定义存储多少元数据的地方。在这个场景中，我们存储 8 MB 的键。Nginx 将每一兆会存储 8000 个实体。参数 max_size 用来定义实际缓存数据的最大尺寸。\n现在，我归纳下：\nproxy_cache_path {cache_root:缓存路径} levels={n:从缓存键值倒数n个字符作为一级目录}:{m:从缓存键值倒数第 n 个字符开始 m 个字符作为二级目录} keys_zone={cache_name:该缓存在缓存路径的目录名}:8m; 最终该键缓存的目录结构是：\n{cache_root}/{cache_name}/{数字:从缓存键值倒数n个字符作为一级目录}/{从缓存键值倒数n个字符}/{从缓存键值倒数第 n 个字符开始 m 个字符作为二级目录} 上面我们使用的另一个指令就是 proxy_cache_key。它用来设置用来存储缓存值的键。\nproxy_cache_valid 指令可以被指定多次。它允许我们基于不同状态码存储不同值。在我们的例子中，我们存储 ** 200 状态（成功）** 和 302 状态（重定向） 缓存时间为 10 分钟，和 404 状态 为 1 分钟后清除缓存。\n现在，我们已经配置好缓存区域，但是我们仍然需要告诉 Nginx 在哪里使用缓存。\n在我们定义代理到后端的 location 中，我们配置缓存的使用：\n# server context location /proxy-me { proxy_cache backcache; proxy_cache_bypass $http_cache_control; add_header X-Proxy-Cache $upstream_cache_status; proxy_pass http://backend; } . . . 使用 proxy_cache 指令，我们可以指定所在 context 可以使用 backcache 的缓存区域。Nginx 将在传递到后端之前检查可用的缓存实体。\nproxy_cache_bypass 指令用来设置 $http_cache_control 变量。它告知代理服务器发请求的客户端是否需要请求一个新鲜，未缓存版本的资源。\n我们还可以增加一个多余 Headers 信息（X-Proxy-Cache）。我们把这个 Headers 设置成 $upstream_cache_status。它设置 Headers 来告知用户该 request 的缓存是否被命中，丢失，或者被绕过。在 debug 的时候，该配置特别有用；并且对客户端来说也很重要。\n缓存结果的注意事项 # 缓存可以极大地提高代理服务器的性能。然而，配置缓存的时候，还是要需要考虑挺多的东西的。\n首先，任何用户相关的数据都不应该被缓存。因为这样会导致一个用户数据的结果被呈现到另一个用户。如果你的站点是纯静态的，那这可能就不是问题了。\n如果你的站点有些动态元素，那你就需要在后端服务器考虑到这一点。处理它的方式依赖于后端处理方式。对于隐私内容，你应该把 Cache-Control 设置成 no-cache,no-store,或者 private，这个依赖于数据本身：\nno-cache:表示必须先与服务器确认返回的响应是否被更改，然后才能使用该响应来满足后续对同一个网址的请求。因此，如果存在合适的验证令牌 (ETag)，no-cache 会发起往返通信来验证缓存的响应，如果资源未被更改，可以避免下载。 no-store：直接禁止浏览器和所有中继缓存存储返回的任何版本的响应 - 例如：一个包含个人隐私数据或银行数据的响应。每次用户请求该资源时，都会向服务器发送一个请求，每次都会下载完整的响应。 private:浏览器可以缓存private响应，但是通常只为单个用户缓存，因此，不允许任何中继缓存对其进行缓存 - 例如，用户浏览器可以缓存包含用户私人信息的 HTML 网页，但是 CDN 不能缓存。这个在缓存用户浏览器数据的时候很有用，但是代理服务器不会在后续的请求中承认数据的有效性。 public:说明请求的是公共信息，它可以被任意的缓存。 控制这个行为相关的 Headers 还有 max-age，它控制资源缓存的过期时间。\n根据数据的敏感度，正确地设置这些 Headers，将会帮助你有效地利用缓存，既能保障你的隐私数据安全的同时，还能让动态内容进行有效地刷新。\n如果你的后端服务器也使用 Nginx，你可以像下面这样使用 expires 指令，它将会为 Cache-Control 设置 max-age:\nlocation / { expires 60m;\t# 给请求的 Header 添加 Cache-Control:max-age=3660; } location /check-me { expires -1; # 给请求的 Headers 添加 Cache-Control:no-cache; } 在上面的例子中，第一个块允许内容被缓存 60 分钟。第二个块则把 Cache-Control 头设置成 no-cache。还想设置其他值，你可以使用 add_header 指令：\nlocation /private { expires -1; add_header Cache-Control \u0026quot;no-store\u0026quot;; # 给请求的 Headers 添加 Cache-Control:no-cache, no-store; } 结语 # Nginx 是第一个也是最重要的反向代理服务器，还可以作为 Web 服务器使用。因为这样的设计决策，代理请求到另个服务器变得更简单。Nginx 也足够灵活，允许你根据需求对代理配置进行灵活的控制。\n参考文献：\nUnderstanding Nginx HTTP Proxying, Load Balancing, Buffering, and Caching Google Developers Logo－HTTP 缓存 13 Caching in HTTP ","date":"2016-02-19","permalink":"/n3xtchen/2016/02/19/nginx-port-forwording/","section":"时间线","summary":"介绍 # 这里，我们将介绍 Nginx 的 Http 代理功能（请求（request） 通过 Nignx 传递到后端服务器，进行后续处理）。Nginx 经常设置为 反向代理（Reverse Proxy） 帮助 横向扩展（scale out：通过增加独立服务器来增加运算能力） 基础架构（infrastructure） 来提升负载能力或者传递请求给下一级代理服务器。","title":"Nginx —— 理解HTTP 代理，负载均衡，缓冲（Buffering）和缓存（Caching）"},{"content":" 文件结构 # 一个 Swift 程序有一个或多个文件组成。在 Swift 中，文件是一个有意义的个体。能存在于文件顶级目录有如下组件：\n模块导入语句：一个模块是除了文件外最高级的单元。一个模块可以由几个文件组成；一个带模块的文件，对于其它文件而言，是可见的；如果没有 import 语句，一个模块就不能看到另一个模块。例如，如果你想要在 iOS 程序中与 Cocoa 交互，你就需要在文件的第一行，引入 import UIKit。 变量声明：文件的最外层声明一个变量，我们称这个变量为全局变量：它将存活在整个程序运行周期中。 函数声明：文件的最外层声明一个函数，我们称这个函数为全局函数：所以代码都可见和可访问，不需要发送任何消息给任何对象。 对象类型声明：类，struct 或者 enum 的声明。 例如，一个合法的 Swift 文件包含一个导入语句，一个变量声明，一个函数声明，一个类声明，一个结构体声明和一个枚举声明：\nimport UIKit var one = 1 func changeOne() { } class Manny { } stuct Moa { } enum Jack { } 这个例子看似愚蠢（实际上也挺愚蠢的），但是说明清楚一个文件的结构。现在，我们看一个复杂点的例子：\nimport UIKit var one = 1 func changeOne() { let two = 2 func sayTwo() { println(two) } class Klass {} sturct Struct {} enum Enum {} one = two } class Manny { let name = \u0026quot;manny\u0026quot; func sayName() { println(name) } class Klass {} struct Struct {} enum Enum {} } struct Moe { let name = \u0026quot;moe\u0026quot; func sayName() { println(name) } class Klass {} struct Struct {} enum Enum {} } enum Jack { var name : String { return \u0026quot;jack\u0026quot; } func sayName() { println(name) } class Klass {} struct Struct {} enum Enum {} } 作用域和生命周期 # 在 Swift 中，也有作用域（scope）。这是关于对于其他东西可见的能力。一个东西嵌套在另一个东西中，构建嵌套等级关系。这个等级是：\n模块是一个域 文件是一个域 对象声明是一个域 花括号是一个域 当被声明的时候，它就已经被赋予了一定的等级。等级中它的位置——它的域——决定它是否可见。\n在上一个代码例子中，Manny 类中声明了一个变量 name 和一个方法 sayName；sayName 方法外部是可见的，因此 name 在这个方法中是可见的。类似，changOne 中的声明的方法，都可以访问这个文件声明的 one 变量；确实，这个文件中所有代码都可以访问文件级别的 one 变量\n域是分享信息方式中最重要的方式。Manny中的两个不同的函数都可以访问 Manny 等级声明的 name。Jack 和 Moe 中的代码也都可以访问文件等级的 one 变量。\n任何东西也都有它的生命周期。域中的东西和域的存活时间相同。因此，文件活多久，one 就能活多久——即程序的运行时长。它是全局的，持久的。然后，Manny 中声明的 name 只和 Manny 的生命周期一致。等级嵌套的越深，它的生命周期越短；举个例子：\nfunc silly() { if true { class Cat {} var one = 1 one = one + 1 } } 在这个代码中，Cat 类和变量 one 仅存活很短暂的时间，在 if 构造体内。当 silly 被调用，Cat 被声明，然后存在；然后 one 被声明，存活；然后执行 one = one + 1，然后域结束，然后 Cat 和 one 烟消云散。\n对象成员 # 三个对象类型中（类，结构体和枚举），在其顶级作用域声明的东西都有特别的名称，大部分是因为历史原因。让我们把 Manny 类作为例子：\nclass Manny { let name = \u0026quot;manny\u0026quot; func sayName() { println(name) } } 在代码中：\nname 是在对象声明的顶级域中声明的变量，我们称之为对象的 属性 sayName 是在对象声明的顶级域中声明的函数，我们称之为对象的方法 在对象声明的顶级域中声明的东西 —— 属性和变量，我们把它们统称为对象成员。对象成员是特别重要的，因为它定义了你可以传递给对象的信息。\n命名空间(NameSpace) # 命名空间就是程序的命名领域。命名空间有一个特性，如果不传递所在命名空间的名称，里面的东西就无法被访问。命名空间的好处就是允许我们在不同的地方使用同一个名字，而不会导致冲突。显然，命名空间和域是非常类似的概念。\n命名空间帮助解释在一个对象域的顶级声明一个对象的重要性，例如：\nclass Manny { class Klass {} } 声明 Klass 的方式有效地被隐藏在 Manny 中。Manny 就是一个命名空间！Manny 中的代码可以直接访问 Klass。但是 Manny 外部的代码就不能；它必须明确指定命名空间才能够正常访问。具体做法是，Manny后面跟着点号 .，然后才是 Klass，Manny.Klass。\n命名空间并不提供隐私功能；它只是一种惯例。这样，我们可以给 Manny 一个 Klass 类，也可以给 Moa 一个 Klass 类；它们之间不冲突，因为它们在不同命名空间内，而且我可以很容易区分它们。\n它并没有逃离你的注意力，应用命名空间的语法世界上就是点号信息传递语法。它们是同样的东西。\n结果信息传递允许你在各自域中看见自己的代码，而相互之间就不能直接访问。Moa 中的代码可以通过简单的方式访问 Manny 的 Klass，即 Manny.Klass；但是他可以这么做的前提就是 Manny 对 Moa 是可见的（显然，它们声明在同一个域内）。\n模块 # 顶级命名空间就是模块。默认，你的应用是一个模块，也是一个命名空间；命名空间名大致上也是应用的名称。例如，如果我的应用名为 MyApp，如果我声明一个 Manny 类在一个文件的最外层，那它的实际名称应该是 MyApp.Manny。但是我不需要使用这个名称，因为我的代码在同一个命名空间内，并可以直接访问 Manny。\n框架也是一个模块，即便它们也有命名空间。例如，Cocoa 基础框架（NSString 所在的地方） 就是一个模块。当你编写 iOS 应用的时候，你需要导入 Foundation(或者，更有肯能，你要导入 UIKit，它本身会导入 Foundation)，因此允许你使用 NSString，而不需要写 Foundation.NSString。当然如果你蠢到在你的模块中定义了 NSString，那你需要明确引用才能使用框架的 NSString。\n文件之上的层级就是文件导入的库了。你的代码永远都会隐性导入 Swift 本身；你也可以在你纹尖头明确引入它；虽然你不需要这么做，但也无害。\n这很重要，因为它是解决大部分问题：println 就在这里面，那为什么你可以在任何对象外部使用它呢？println实际上是在 Swift.h 中声明的函数；它集合其他普通顶级函数一样。你可以这么用 Swift.println(\u0026quot;hello\u0026quot;)，但是你不需要这么足，因为不需要有冲突需要解决。\n实例 # 对象类型 － 类，结构和美剧 —— 有一个重要的共性：他们都可以背诗丽华。结果，当你声明一个对象类型是，你只是定义了一个类型。实例一个类型就是创造一个东西 —— 那个类型的实例。\n因此，例如，我可以声明一个狗类：\nclass Dog { } 我可以赋予它一个方法：\nclass Dog { func bark() { println(\u0026quot;woof\u0026quot;) } } 但是实际上在我的程序还没有狗的对象。我仅仅是描述了我想要的狗的类型。为了得倒真的狗，我就必须创造一个。创造狗的时机过程就是实例化狗的过程。结果就是得到一个新对象——一个狗实例。\n在 Swifit 中，通过对象类型名称作为函数名，调用它来创建一个实例。它需要使用括号。当你在对象类型名后加上括号，你就会发送一个特殊的信息给对象类型：实例化你自己！现在，我来实例化一个对象：\nlet fido = Dog() 现在声明了一个 Dog 实例。注意下 let，如果我使用下述语句重新声明一个 Dog 对象：\nlet fida = Dog() fida 和 fido 是同一个对象，他们指向的是同一个内容空间；如果 let 修改成 var，情况就不一样了：\nvar fido = Dog() var fida = Dog() 这两个对象值是相同的，但是它们指向的是不同内存空间，意味着，修改其中一个，不会修改到另一个。\n还有一个需要注意的，就是使用 let 声明的对象不是常量，是可变的。\n既然声明好了一个对象，那我们试着调用下方法：\nfido.bark()\t// 输出 woof 默认情况，属性和方法是从属于实例的。你不能使用它们作为信息传递给对象类型本身；你必须得有一个接受信息的实例。下面的语句是不合法的：\nDog.bark()\t// 导致编译错误 当然，如果你声明的是一个函数，那这样做就是合法，它是另一类函数——类函数或者静态函数。\n属性也是同理的。现在我们声明一个实例属性：\nclass Dog { var name = \u0026quot;\u0026quot; } 它允许我们设置一个 Dog 名，但前提是你需要一个 Dog 实例：\nlet fido = Dog() fido.name = \u0026quot;fild\u0026quot; 如果你想要直接使用类型访问属性，你则需要声明特殊的属性——类属性或者静态属性。\n","date":"2016-02-14","permalink":"/n3xtchen/2016/02/14/swift---tut4/","section":"时间线","summary":"文件结构 # 一个 Swift 程序有一个或多个文件组成。在 Swift 中，文件是一个有意义的个体。能存在于文件顶级目录有如下组件：","title":"Swift 学习笔记4 —— 文件结构，作用域和生命周期"},{"content":" 变量 # 变量是一个对象的名称。技术角度，它是一个对象的索引。非技术角度，你可以把它当作装对象的一个鞋盒。一个对象可能会被改变，或者被放到另一个对象的鞋盒中，但是名称代表他们整体。\n在 Swift，所有的变量都必须明确声明。你可以使用这两个关键词中的一个来声明它：let 和 var。通常，声明会和初始化一起完成——你使用等号来给一个变量赋值，在声明语法的右边部分：\nlet one = 1 var two = 2 一旦被声明，你就可以自由使用它：\nlet one = 1 var two = 2 two = one 最后一行使用的是前两个声明的变量名。一个这样的语句，一个变量名在等号的左边，我们称之为赋值（assignment）；等号就是赋值操作符，不是等式断言，是一个命令。它的意思就是：“从我的右边对象获取值，使用它替换我左边对象的值。”\n使用 let 声明的变量为常量，它的值一旦被设定，就不能被改变：\nlet one = 1 var two = 2 one = two // 编译错误 尽可能使用 var 来声明变量，让它自己更灵活；但是你知道你永远不会改变它的值的时候，最好使用 let，它会让 Swift 更加的高效。\n变量也有类型。这个类型一旦被设定，就不能被改变。例如，下面的例子不会被编译通过：\nvar two = 2 two = \u0026quot;hello\u0026quot; 变量确实有它自己的生命——更准确地说，他们的生命周期。只要变量存在，他就能保持它的值。因此，变量不仅仅是方便命名的方式，还是保护它的一种方式。\n函数 # 可执行代码，像 fido.bark 或者 one = two，不能随便放置。通常，它只会存在函数体内。一个函数是一批代码，一连串的可以被批处理执行的代码。一个函数有一个名称，通过声明来获取这个名称。函数声明的语法：\nfunc go() { let one = 1 var two = 2 two = one } 它描述一系列动作——声明 one，声明 two，把 two 的值替换成 one 的 —— 给这序列一个名称，go；但是它不会被执行，只有某人调用的时候，它才会被执行：\ngo() 这个命令是自执行代码，因此他不能独立存在。它可以存在其它函数体内：\nfunc doGo() { go() } 这个优点疯狂。它也只是一个函数声明；必须有有人执行 doGo，并且它是可执行。如果所有可执行代码都在函数中，谁来执行这些函数？初始动力必须来源某个地方。\n在现实生活中，幸运的是，这是回归问题不会存在。记住，你的目标是就是写一个 iOS 应用。因此，你的应用将会运行在 iOS 设备中（或者模拟器）。因此，你们开始写特殊函数的时候，就应该意识到她会被 runtime 自己调用。这给予你的应用开始的方式，给予你放置函数的位置，在关键时刻被 runtime 调用——例如当应用启动的时候，或者当用户在你的应用界面点击乐意按钮的时候。\n提示 # Swift 也有一个特殊的规定：main.swift，异常情况下，可以在它的顶级域中函数体外部编写可执行代码；当程序运行时，代码会被执行。你可以在 main.swift 文件构建你的应用，但是一般情况，你不需要这么做。同时，XCode 允许你创建 playgrounds。一个 playground 文件有个特性，使它可以像 main.swift 文件运行，所以你可以吧可执行代码放到文件顶部。但是 playground 不能成为应用的一部分，所以它不再本书的讨论范围。\n","date":"2016-02-12","permalink":"/n3xtchen/2016/02/12/swift---tut3/","section":"时间线","summary":"变量 # 变量是一个对象的名称。技术角度，它是一个对象的索引。非技术角度，你可以把它当作装对象的一个鞋盒。一个对象可能会被改变，或者被放到另一个对象的鞋盒中，但是名称代表他们整体。","title":"Swift 学习笔记3 —— 变量和函数"},{"content":" 一切都是对象 # 在 Swift，“一切都是对象。” 这是个现代面向对象语言的最浮夸吹嘘点。\n让我们开始规定一个对象，粗略地说，你可以发动信息的东西。一条信息，粗略地说，一条指令说明。举个例子，你给一条狗发一个命令：“叫！””坐！“。做个类比，这些措辞就是信息，而狗就是你要发送信息的对象。\n在 Swift 中，信息发送的语法就是点标记（dot-notation）。我们从对象开始；然后是一个点；然后是信息。（一些信息后面也跟着括号，但是现在我们忽略它们；完整的语法我们后续会详细讲解）。下面是有效的 Swift 语法：\nfido.bark() rover.sit() 一切都是对象的理念就是建议原始语言实体都可以接受信息的一种方式。拿 1 做例子，如果你使用过其它变成语言，你可以在 Swift 这样使用它：\nlet sum = 1 + 2 但是 1 后面跟着一个点号和一个信息，就会让你吃惊。他在 Swift 中是合法有意义的：\nlet x = 1.successor() 类似的，一个文本表达片段作为一个字画量 —— 一个字符串 —— 也是一个对象。比如，hello 是一个字符字画量，也是合法的 Swift 语法：\nlet y = \u0026quot;hello\u0026quot;.generate() 但是我门再进一步。回到我们之前的例子看似无害的 1 ＋ 2。它们实际上是一种语法糖，表达和隐藏内在的一种方便方式。1 实际上是一个对象，+ 实际上是一个信息；但是它是一个特殊语法的信息（操作符语法）。在 Swift 中，每一个名词都是一个对象，所有动词都是信息。\n在 Swift 中测试一个东西是否是对象的最终完整性测试就是你是否可以改变它。一个对象类型可以被扩展（extended，我们也可以称之为猴子布丁），意味和你可以为这个对象定一个自己的信息。比如，你一般不会发送 sayHello 信息给一个数字。但是你可以改变一个数字类型，因此你能：\nextension Int { func sayHello() { println(\u0026quot;Hello, I'm \\(self)\u0026quot;) } } 1.sayHello() // outputs: \u0026quot;Hello, I'm 1\u0026quot; 在 Swift 中，1 是一个对象。在一些语言中，例如 Objective-C，它明显不是；它是一个原始变量或者纯内置数据类型。在 Swift 中， 没有纯量（Scalar）；所有的类型都是对象类型。这就是 一切都是对象 的真正意义。\n对象类型的三种形式 # 如果你知道 Object-C 或者一些其它面向对象语言，你可能会吃惊对象 1 是哪种 Swift 概念。在很多语言，例如 Objective-C，一个对象是一个类（class）或者一个类的实例。Swift 有类和实例，你可以发送信息给他们；但是 Swift 的 1 都不是这两种类型：它是一个结构体（Struct）。Swift 还有另一种类型你可以发送信息它，它就是 enum。\n因此，Swift 有三种对象类型：classes，structs 和 enums。他们只有在特定的场景下才会浮现出区别。但是他们实际上都是对象类型，他们之间的相似处远比他们的区别大。现在，我们只要记住有这三种类型。\n","date":"2016-02-11","permalink":"/n3xtchen/2016/02/11/swift---tut2/","section":"时间线","summary":"一切都是对象 # 在 Swift，“一切都是对象。” 这是个现代面向对象语言的最浮夸吹嘘点。","title":"Swift 学习笔记2 —— 一切都是对象"},{"content":" 语句 # 我们把一个完整的 Swift 命令称作语句（Statement）。一个 Swift 文本文件包含多行，其中换行（Line Break）是有实际意义的。标准的程序布局就是一个语句一行：\nprint(\u0026quot;hello \u0026quot;) print(\u0026quot;world\u0026quot;) 当然你也适用分号（;）来充当换行的作用：\nprint(\u0026quot;hello \u0026quot;); print(\u0026quot;world\u0026quot;) 这样你就可以在同一行中写多个语句。结尾的分号是可省略的（C 或者 Objective-C 的分号是必须，如果你喜欢可以添加）。\n相反，为了防止超长的语句给可读性带来麻烦，一条单一的语句可以被拆分成多行，但是前提条件，你不能让 Swift 迷惑了。现在来个例子，左括号是断句的一个比较好的方式：\nprint( \u0026quot;world\u0026quot;) 注释 # 听说写注释的程序猿才是好程序猿哦，哈哈！是的，注释可以让第三个人更快速地了解你的程序是运转方式，尤其在多人协作的开发环境，将大大提高开发效率，减少沟通成本。\n单行注释是采用 C++ 风格，就是跟在双斜杆（//）后面的内容：\nprint(\u0026quot;world\u0026quot;) // 这里就是注释, 因此 Swift 将会直接忽视它 多行注射则采用的是 C 风格，/* 注释在这里 */，或者像下面这样：\n/* 这样你爱写几行就几行， 哈哈哈 */ 花括号 # Swift 中的很多结构都是使用花括号作为定界符：\nclass Dog { func bark() { println(\u0026quot;woof\u0026quot;) } }\t按照惯例，花括号的内容的前面和后面都应该有换行符，并且使用缩进来帮助辨识，就和上面的代码一样，实际上 swift 并不关心这些，你这么写也可以：\nclass Dog { func bark() { println(\u0026quot;woof\u0026quot;) }} print 和 println # 它们都是在终端输出内容的命令，它们之间不同在于 pirntln 会在行尾添加换行符\nSwift 是一种编译型语言 # Swift 是一种编译型语言，意味着你代码必须被编译 —— 通过编译器，把文本转化成计算机能了解的低级形式 —— 在它可以运行，并且确认做的是它所说的事情之前。Swift 编译器非常严格；只要编译标记了一些错误，那你就不可能编译完成，直到你把这些错误修复为止。有时他会提示一些警告；代码虽然可以运行，但是你因该严肃对待这些警告，尽可能的修复它。编译器的严格性是 Swift 强大的来源之一，在运行之前能为你的代码提供最大限度的审计。\n","date":"2016-02-08","permalink":"/n3xtchen/2016/02/08/swift---tut/","section":"时间线","summary":"语句 # 我们把一个完整的 Swift 命令称作语句（Statement）。一个 Swift 文本文件包含多行，其中换行（Line Break）是有实际意义的。标准的程序布局就是一个语句一行：","title":"Swift 学习笔记1"},{"content":"最近我开始使用 React 和 webpack 进行开发，在效率方面得到了很大的提升。我发现这样的组合让前端开发更加有趣，你们也应该试试。\n第一部分－安装初始化 # 我们的第一个初始化安装工作，就是为这个应用创建一个文件夹（我们把它命名 react-webpack）；进入，执行它把你的项目初始化成一个 npm 包：\n$ npm init 安装 react 和 webpack # 然后当然需要安装 react 的 npm 包：\n$ npm install --save react 安装完，再安装我们所有需要开发依赖。为此，我门还需要安装 webpack 和 webpack-dev-server 来搭建我们的 App。我们还需要安装 jsx-loader 来让 webpack 把 jsx 文件转化成 js 文件：\n$ npm install --save-dev webpack webpack-dev-server $ npm install --save-dev jsx-loader 你的第一个组件 # 现在，我们开始编写我们的第一个 ReactJS 基本组件，我们称它为 Hello.jsx：\n/** @jsx React.DOM */ 'use strict' var React = require('react') module.exports = React.createClass({ displayName: 'HelloReact', render: function(){ return \u0026lt;div\u0026gt;Hello React\u0026lt;/div\u0026gt; } }) 创建 index.jsx 作为我们 App 的访问入口：\n/** @jsx React.DOM */ 'use strict' var ReactDOM = require('react-dom') var Hello = require('./Hello') ReactDOM.renderComponent( \u0026lt;Hello /\u0026gt;, document.getElementById('content') ) 配置 webpack # 然后，我们需要把所有文件打包到一个单一 bundle 文件中，这就是我们使用 webpack 的原因。我们需要创建一个 webpack.config.js （webpack 在启动时要查找的默认文件）。它是这样子的：\nmodule.exports = { entry: './index.jsx', output: { filename: 'bundle.js', // 这是默认的文件名 // 在这个文件夹下我们的 bundle 文件将可用a // 当启动 webpack-dev-server 时，确保 8090 端口可以被使用 publicPath: 'http://localhost:8090/assets' }, module: { loaders: [ { // 告诉 webpack 使用 jsx-loader 来解析所有的 *.jsx 文件 test: /\\.jsx$/, loader: 'jsx-loader?insertPragma=React.DOM\u0026amp;harmony' } ] }, externals: { // don't bundle the 'react' npm package with our bundle.js // but get it from a global 'React' variable 'react': 'React' }, resolve: { extensions: ['', '.js', '.jsx'] } } 这个 webpack 配置都在注释中解释，因此你应该对 webpack 有了一些初步的了解。现在已经足够，可以开始编写我们的 index.html，然后启动我们的 app。\nindex.html # 我们需要创建一个 index.html 文件，然后提供 web 服务，因此我们需要安装 http-module\n$ npm install --save-dev http-server index.html 应该像这样：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Basic Property Grid\u0026lt;/title\u0026gt; \u0026lt;!-- include react --\u0026gt; \u0026lt;script src=\u0026quot;./node_modules/react/dist/react-with-addons.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026quot;content\u0026quot;\u0026gt; \u0026lt;!-- this is where the root react component will get rendered --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- include the webpack-dev-server script so our scripts get reloaded when we make a change --\u0026gt; \u0026lt;!-- we'll run the webpack dev server on port 8090, so make sure it is correct --\u0026gt; \u0026lt;script src=\u0026quot;http://localhost:8090/webpack-dev-server.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- include the bundle that contains all our scripts, produced by webpack --\u0026gt; \u0026lt;!-- the bundle is served by the webpack-dev-server, so serve it also from localhost:8090 --\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; src=\u0026quot;http://localhost:8090/assets/bundle.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 注意：index.html 需要从 node_modules 文件夹中引入 react 模块；然后引入 webpack-dev-server.js 脚本（它由 webpack-dev-server 服务提供，之前我们设置成 8090 端口）。当我们的文件被变更的时候，这个脚本会接收到服务器端的通知，然后载入一个新版本 bundle 文件。\n最后，我们引入 bundle.js(也是由 webpack-dev-server 服务提供)。注意：bundle 文件存在于缓存中－它不会在你硬盘中创建。这个使得 webpack-dev-server 能够快速响应代码变更时重新绑定和载入。\n准备好启动脚本 # 最后一件事情就是添加三个脚本实体到 package.json 中：\n{ \u0026quot;name\u0026quot;: \u0026quot;d3-react\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;index.js\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;start\u0026quot;: \u0026quot;npm run serve | npm run dev\u0026quot;, \u0026quot;serve\u0026quot;: \u0026quot;./node_modules/.bin/http-server -p 8088\u0026quot;, \u0026quot;dev\u0026quot;: \u0026quot;webpack-dev-server --progress --colors --port 8090\u0026quot; }, \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;ISC\u0026quot;, \u0026quot;dependencies\u0026quot;: { \u0026quot;react-dom\u0026quot;: \u0026quot;^0.14.6\u0026quot;, \u0026quot;react\u0026quot;: \u0026quot;^0.14.6\u0026quot; }, \u0026quot;devDependencies\u0026quot;: { \u0026quot;http-server\u0026quot;: \u0026quot;^0.8.5\u0026quot;, \u0026quot;jsx-loader\u0026quot;: \u0026quot;^0.13.2\u0026quot;, \u0026quot;webpack\u0026quot;: \u0026quot;^1.12.11\u0026quot;, \u0026quot;webpack-dev-server\u0026quot;: \u0026quot;^1.14.1\u0026quot; } } 我们要做的就是增加三个命令，是它们能通过 npm run \u0026lt;cmd\u0026gt; 调用。\nserve - npm run serve - 在当前目录启动一个 http 服务，端口 8080 dev - npm run dev - 在端口 8090 启动 webpack-dev-server 服务 start - npm run start - 先执行 serve 再执行 dev 启动 # 现在我们都准备好了\n$ npm run start 如果一切顺利，你应该可以打开 http://localhost:8090/webpack-dev-server/ ，得到一条问候信息：\nHello React 现在如果你使用编辑器改变一行代码，浏览器将会自动重载。\n下面是最终的目录结构：\n. ├── hello.jsx ├── index.html ├── index.jsx ├── node_modules ├── package.json └── webpack.config.js 译自： React with webpack - part 1\n","date":"2016-01-24","permalink":"/n3xtchen/2016/01/24/react-with-webpack/","section":"时间线","summary":"最近我开始使用 React 和 webpack 进行开发，在效率方面得到了很大的提升。我发现这样的组合让前端开发更加有趣，你们也应该试试。","title":"ReactJS：使用 webpack 部署"},{"content":"","date":"2016-01-24","permalink":"/n3xtchen/tags/webpack/","section":"标签","summary":"","title":"webpack"},{"content":"","date":"2015-12-24","permalink":"/n3xtchen/categories/php/","section":"分类页","summary":"","title":"PHP"},{"content":"","date":"2015-12-24","permalink":"/n3xtchen/tags/yii/","section":"标签","summary":"","title":"yii"},{"content":" 前提 # php \u0026gt;= 5.4 Yii \u0026gt;= 2.0.6: 这个版本才开始支持数据库模式生成器（Schema Builder） 简单的入门 # 先看命令说明 # $ ./yii help migrate 。。。废话在此处省略。。。 The migration history is stored in a database table named as [[migrationTable]]. The table will be automatically created the first time this command is executed, if it does not exist. You may also manually create it as follows:\t# 我的翻译：迁移历史将被存储到数据库表 migration 中；如果他不存在这个表，触发迁移命令的时候就会被创建。你也许要手动创建他，语句如下 CREATE TABLE migration ( version varchar(180) PRIMARY KEY, apply_time integer ) Below are some common usages of this command: # creates a new migration named 'create_user_table' # 我的翻译：生成迁移文件 yii migrate/create create_user_table # applies ALL new migrations # 我的翻译：执行数据库迁移 yii migrate # reverts the last applied migration # 我的翻译：回滚数据库迁移 yii migrate/down SUB-COMMANDS - migrate/down Downgrades the application by reverting old migrations.\t# 回滚旧版本 - migrate/history Displays the migration history.\t# 显示迁移历史 - migrate/mark Modifies the migration history to the specified version.\t# 强制指定历史到特定的版本，只会修改 migrate 表中纪录 - migrate/new Displays the un-applied new migrations.\t# 查看可更新的迁移 - migrate/redo Redoes the last few migrations.\t# 重新最近一次的迁移 - migrate/to Upgrades or downgrades till the specified version.\t# 更新／回滚到指定版本 - migrate/up (default) Upgrades the application by applying new migrations.\t# 默认是执行迁移 To see the detailed information about individual sub-commands, enter: yii help \u0026lt;sub-command\u0026gt; 创建迁移（Migration） # 运行下面的命令：\n$ ./yii migrate/create \u0026lt;name\u0026gt; \u0026lt;name\u0026gt; 指定了迁移的简要描述，必须传入。例如，在本次迁移，你要建立名为 posts 的迁移，使用一下命令：\n$ ./yii migrate/create create_posts_table 由于参数 name 将用来作为迁移生成的类名，因此他只能包含字母，数字以及下划线。\n上述的命令将会在项目根目录下 migrations 目录中生成一个 PHP 类文件.\n$ cat migrateions/m151229_025650_create_posts_table \u0026lt;?php use yii\\db\\Migration; class m151229_025650_create_posts_table extends Migration { public function up() { } public function down() { echo \u0026quot;m151229_025650_create_posts_table cannot be reverted.\\n\u0026quot;; return false; } /* // Use safeUp/safeDown to run migration code within a transaction public function safeUp() { } public function safeDown() { } */ } 每一次数据库迁移都会被定义成 PHP 类（继承 yii\\db\\migration）。这个迁移类名将会自动生成为如下格式：\nm\u0026lt;YYYMMDD_HHMMSS: 执行命令的系统时间\u0026gt;_\u0026lt;Name: 命令行传入\u0026gt; 在迁移类中，你需要编写 up() 方法来告诉 yii 如何操作数据库。你可能还需要编写 down() 方法来回滚由 up() 带来的变动。下面演示如何使用 yii 来创建一张 news 表：\n\u0026lt;?php use yii\\db\\Schema; use yii\\db\\Migration; class m150101_185401_create_news_table extends Migration { public function up() { $this-\u0026gt;createTable('news', [ 'id' =\u0026gt; Schema::TYPE_PK, 'title' =\u0026gt; Schema::TYPE_STRING . ' NOT NULL', 'content' =\u0026gt; Schema::TYPE_TEXT, ]); } public function down() { $this-\u0026gt;dropTable('news'); } } 注意：不是所有的迁移都需要回滚。比如，如果 up() 操作是删除表的一行，你可能就不能通过 down() 来回滚这一条数据。有时，你只是太懒而不愿意实现 down() 方法，因为回滚也不是很普遍。这种情况，你应该在 down() 方法中返回 false 值来指明此处不可回滚。\n不应该使用数据库原生字段类型，而是使用 yii 提供的数据库抽象字段类型，这样就能让你的数据迁移便携独立于特定的数据库。在 yii\\db\\Schema 中定义了一系列常量来存放所支持的数据库抽象字段类型。这些常量的命名格式为 TYPE_\u0026lt;Name\u0026gt;。例如，TYPE_PK 代表自增主键类型；TYPE_STRING 代表字符型。当执行一次数据迁移时，数据库抽象类型将会被转换成相关数据库原生类型。以 MYSQL 为例，TYPE_PK 会转化成 int(11) NOT NULL AUTO_INCREMENT PRIMARY KEY，而 TYPE_STRING 会变成 varchar(255)。\n你还可以使用抽象类型追加额外的约束（constraints）。在上面例子中，NOT NULL 追加到 SCHEMA::TYPE_STRING 的后面来指明这个字段不可为空。\n注意：数据库抽象类型和原生类型的映射都被指定在 QueryBuilder 类中的 $typeMap 属性中。\n从 2.0.6 开始，你可以使用新引入的模式（Schema）生成器，它提供更多方便的方法定义表结构。因此，上述的迁移可以改成如下：\n\u0026lt;?php use yii\\db\\Migration; class m150101_185401_create_news_table extends Migration { public function up() { $this-\u0026gt;createTable('news', [ 'id' =\u0026gt; $this-\u0026gt;primaryKey(), 'title' =\u0026gt; $this-\u0026gt;string()-\u0026gt;notNull(), 'content' =\u0026gt; $this-\u0026gt;text(), ]); } public function down() { $this-\u0026gt;dropTable('news'); } } 所有可用定义字段类型的方法可以在 yii\\db\\SchemaBuilderTrait 的 API 文档中查找。\n生成迁移脚本 # 从 2.0.7 版本开始，命令行模式下支持一种便利的方法来创建迁移脚本。\n如果迁移名称指定成一种特殊的形式，如 create_xxx 或者 drop_xxx（不仅仅限于这两种），迁移类文件将自动生成一些额外的代码。\n创建表 # $ yii migrate/create create_post 生成\nclass m150811_220037_create_post extends Migration { public function up() { $this-\u0026gt;createTable('post', [ 'id' =\u0026gt; $this-\u0026gt;primaryKey() ]); } public function down() { $this-\u0026gt;dropTable('post'); } } 你可以通过 --fields 参数来创建相应的字段。\n$ yii migrate/create create_post --fields=title:string,body:text 生成\nclass m150811_220037_create_post extends Migration { public function up() { $this-\u0026gt;createTable('post', [ 'id' =\u0026gt; $this-\u0026gt;primaryKey(), 'title' =\u0026gt; $this-\u0026gt;string(), 'body' =\u0026gt; $this-\u0026gt;text() ]); } public function down() { $this-\u0026gt;dropTable('post'); } } 还可以指定更多字段定义\n$ yii migrate/create create_post --fields=title:string(12):notNull:unique,body:text 生成\nclass m150811_220037_create_post extends Migration { public function up() { $this-\u0026gt;createTable('post', [ 'id' =\u0026gt; $this-\u0026gt;primaryKey(), 'title' =\u0026gt; $this-\u0026gt;string(12)-\u0026gt;notNull()-\u0026gt;unique(), 'body' =\u0026gt; $this-\u0026gt;text() ]); } public function down() { $this-\u0026gt;dropTable('post'); } } 注意：主键会自动添加，默认的主键名称是 id；如果你想要使用其他名称，你可以通过 --fields=name:primaryKey 明确指定它。\n删除表 # $ yii migrate/create drop_post --fields=title:string(12):notNull:unique,body:text 生成\nclass m150811_220037_drop_post extends Migration { public function up() { $this-\u0026gt;dropTable('post'); } public function down() { $this-\u0026gt;createTable('post', [ 'id' =\u0026gt; $this-\u0026gt;primaryKey(), 'title' =\u0026gt; $this-\u0026gt;string(12)-\u0026gt;notNull()-\u0026gt;unique(), 'body' =\u0026gt; $this-\u0026gt;text() ]); } } 添加字段 # 如果迁移名称是 add_xxx_to_yyy 的形式，生成文件则会包含 addColumn 和 dropColumn 语句。\n为了添加字段：\n$ yii migrate/create add_position_to_post --fields=position:integer 生成\nclass m150811_220037_add_position_to_post extends Migration { public function up() { $this-\u0026gt;addColumn('post', 'position', $this-\u0026gt;integer()); } public function down() { $this-\u0026gt;dropColumn('post', 'position'); } } 删除字段 # 如果迁移名称是 drop_xxx_to_yyy 的形式，生成文件也会包含 addColumn 和 dropColumn 语句。\n$ yii migrate/create drop_position_from_post --fields=position:integer 生成\nclass m150811_220037_drop_position_from_post extends Migration { public function up() { $this-\u0026gt;dropColumn('post', 'position'); } public function down() { $this-\u0026gt;addColumn('post', 'position', $this-\u0026gt;integer()); } } 生成交叉表 # 如果迁移名称是 create_junction_xxx_and_yyy 的形式，生成文件也会包含创建交叉表的语句。\n$ yii create/migration create_junction_post_and_tag 生成\nclass m150811_220037_create_junction_post_and_tag extends Migration { public function up() { $this-\u0026gt;createTable('post_tag', [ 'post_id' =\u0026gt; $this-\u0026gt;integer(), 'tag_id' =\u0026gt; $this-\u0026gt;integer(), 'PRIMARY KEY(post_id, tag_id)' ]); $this-\u0026gt;createIndex('idx-post_tag-post_id', 'post_tag', 'post_id'); $this-\u0026gt;createIndex('idx-post_tag-tag_id', 'post_tag', 'tag_id'); $this-\u0026gt;addForeignKey('fk-post_tag-post_id', 'post_tag', 'post_id', 'post', 'id', 'CASCADE'); $this-\u0026gt;addForeignKey('fk-post_tag-tag_id', 'post_tag', 'tag_id', 'tag', 'id', 'CASCADE'); } public function down() { $this-\u0026gt;dropTable('post_tag'); } } 事务性（Transational）迁移 # 在进行复杂的数据库迁移的同时，必须保证每一次迁移整体成功或者失败，这对保持数据库的完整性和一致性是至关重要的。为了达到这个效果，我建议你把每次迁移执行的数据库操作放到事务中。\n最简单实现事务性迁移的方法就是把迁移脚本放到 safeUp() 和 safeDown() 方法中。它们和 up() 和 down() 的不同在于它们明确指定事务性操作。导致的结果是，只要在方法内的任何一个操作失败，所有在这之前的操作都会自动回滚。\n在接下来的例子中，包括建表和插入操作：\n\u0026lt;?php use yii\\db\\Migration; class m150101_185401_create_news_table extends Migration { public function safeUp() { $this-\u0026gt;createTable('news', [ 'id' =\u0026gt; $this-\u0026gt;primaryKey(), 'title' =\u0026gt; $this-\u0026gt;string()-\u0026gt;notNull(), 'content' =\u0026gt; $this-\u0026gt;text(), ]); $this-\u0026gt;insert('news', [ 'title' =\u0026gt; 'test 1', 'content' =\u0026gt; 'content 1', ]); } public function safeDown() { $this-\u0026gt;delete('news', ['id' =\u0026gt; 1]); $this-\u0026gt;dropTable('news'); } } 需要的注意的是，当你在 safeUp() 中进行多个数据库操作，那你就应该在 safeDown 中进行逆向操作。在上述操作中，我门在 safeUp() 中先建表然后插入数据；而在 safeDown() 中就应该先删除数据，然后删除表。\n注意: 不是所有的数据库都支持事务，并且有一些数据库操作不能放在事务中。具体例子请参见 implicit commit 。如果在这个情况下，你应该在 up() 和 down() 中实现。\n数据库访问方法 # 迁移基类 yii\\db\\Migration 提供了一系列方法来访问和操纵数据库。你可能发现这些方法的命名非常类似 Dao 方法（由 yii\\db\\Command 提供）。例如， yii\\db\\Migration::createTable() 方法允许你创建一个新表，就像 yii\\db\\Command::createTable() 一样。\n使用由 yii\\db\\Migration 提供方法的好处就是你不需要明确实例化 yii\\db\\Command 类，以及执行每一个方法都会自动显示有用的信息，来告诉你完成哪些数据库操作和花了多长时间。\n下面是所有的数据库方法方法：\nexecute()： 执行语句 insert()：插入一行 batchInsert()：批量插入 update()：更新 delete()：删除 createTable()：创建表 renameTable()：重命名表 dropTable()：删除表 truncateTable()：清空表中的数据 addColumn()：添加字段 renameColumn()：重命名字段 dropColumn()：删除字段 alterColumn()：变更字段 addPrimarykey()：添加主键 dropPrimarykey()：删除主键 addForeignKey()：添加外键 dropForeignKey()：删除外键 createIndex()：创建索引 dropIndex()：删除索引 信息：yii\\db\\Migration 不提供数据库查询方法，是因为你一般不需要关心从数据库中获取数据的多余信息。它也因为你可以使用强大的 Query Builder 来构建和运行复杂的查询。\n注意：当你在迁移中操作数据时，你可能会发现使用 Active Record 类会很有用，因为逻辑中一些已经在其中实现了。无论如何都要记住，和迁移类中的代码（他天生需要被永久固化）相比，应用的逻辑需要被变更。基于这种原因，迁移代码应该和其它应用逻辑保持独立，比如 Active Record 类。\n执行迁移 # 你可以使用下述命令来执行所有可用的迁移\n$ yii migrate 这个命令会显示所有还未执行的迁移。如果你确认你想要执行，他将会按它们的时间戳顺序执行每一个新类中的 up() 和 safeUp() 方法。如果其中任何一个迁移失败了，命令将直接退出，不在执行后续的迁移。\n为了识别已经成功执行的迁移，每进行一次成功的迁移，命令都会像数据库中 migration 表中插入一条数据来保存更新的状态。它将帮助迁移工具识别已执行和未执行的迁移。\n信息：这个迁移工具会自动在指定的数据库中创建 migration 表。默认，所使用的数据库会使用配置表（配置的默认路径：config/db.php）中已定义好的 DB 组件。\n如果你想一次性进行多个迁移，那你可以指定迁移个数。例如，你想要执行接下来的三个迁移：\n$ yii migrate 3 你还可以通过使用 migrate/to 命令更新到指定的迁移版本\n$ yii migrate/to 150101_185401 # 使用时间 $ yii migrate/to \u0026quot;2015-01-01 18:54:01\u0026quot; # 使用时间字符串 $ yii migrate/to m150101_185401_create_news_table # 使用名称 $ yii migrate/to 1392853618 # 使用时间戳 在这个版本之前的迁移，都会被执行。在这个版本之后的迁移，都会被回滚。\n迁移版本回滚 # 回滚一个或多个历史已执行的迁移版本，你可以使用如下命令：\n$ yii migrate/down # 回滚到最近的一次版本 $ yii migrate/down 3 # 回滚最近三次的版本\t注意：不是所有的迁移都可以回滚。如果尝试回滚不可回滚的迁移，将会导致一个错误，然后终止后续的所有回滚。\n重复（Redo）迁移 # 重复迁移意味着上一次回滚的版本会再次被执行：\n$ yii migrate/redo # 重复上一个撤销迁移的版本 $ yii migrate/redo 3 # 重复上三个撤销迁移的版本 注意：如果迁移不可回滚，你将无法重复它。\n迁移版本历史 # 为了显示迁移类执行与否，你可以使用如下命令：\n$ yii migrate/history # 显示最近十个被执行的迁移 $ yii migrate/history 5 # 显示最近五个被执行的迁移 $ yii migrate/history all # 显示所有被执行的迁移\t$ yii migrate/new # 显示头十个新迁移 $ yii migrate/new 5 # 显示头五个新迁移 $ yii migrate/new all # 显示头五个新迁移 修改迁移历史 # 与实际执行和回滚迁移历史不同，有时你只想简单把你的数据库版本标记到指定版本。当你手动变更数据库到指定状态的时候，你不想为了哪些变更重新执行迁移时，你就可以使用这个命令：\n$ yii migrate/mark 150101_185401 # 使用时间 $ yii migrate/mark \u0026quot;2015-01-01 18:54:01\u0026quot; # 使用时间串 $ yii migrate/mark m150101_185401_create_news_table # 使用迁移类名 $ yii migrate/mark 1392853618 # 使用时间戳 这个命令将修改 migration 这张表，修改到指定迁移版本。这个操作不会进行实质性的执行或者回滚操作。\n定制迁移 # 有以下几种方式定制迁移命令：\n使用命令行参数 # 可以通过命令行参数来对迁移进行定制化：\ninteractive: 布尔型（默认是 true），指定是否进入交互模式。如果是 true，用户将会实时输出执行特定的迁移动作。如果你想要他在后台执行，那就要把它设置成 false。\nmigrationPath：字符型（默认是 @app/migration），指定迁移脚本的存储路径。这个可以是目录路径或者路径别名。注意，必须保证目录存在，否则命令将会报错。\nmigrationTable：字符型（默认是 migration），指定存储迁移历史信息的数据库表名。如果不存在，这个表将会自动被创建。你也可以手动创建它（表结构：version varchar(255) primary key, apply_time integer）。\ndb：字符型（默认是 db），指定数据库应用组件的 ID。它表示数据库将被这个命令迁移。\ntemplateFile：字符型（默认是 @yii/views/migration.php），指定模版文件（它将帮助用户生成迁移脚本骨架）的路径。这个可以是目录路径或者路径别名。这个模版文件是一个 PHP 脚本，这个文件预定义了一个 $className 变量用来替换生成后的迁移类的类名。\ngeneratorTemplateFiles：数组型（默认是：\n[ 'create_table' =\u0026gt; '@yii/views/createTableMigration.php', 'drop_table' =\u0026gt; '@yii/views/dropTableMigration.php', 'add_column' =\u0026gt; '@yii/views/addColumnMigration.php', 'drop_column' =\u0026gt; '@yii/views/dropColumnMigration.php', 'create_junction' =\u0026gt; '@yii/views/createJunctionMigration.php' ] ），指定用于生成迁移代码的模版文件。具体看 生成迁移。\nfields：用于定义创建迁移类中的字段定义数组。默认是 []。每个字段定义的格式是 COLUMN_NAME:COLUMN_TYPE:COLUMN_DECORATOR。例如，--fields=name:string(12):notNull 生成一个长度为 12 的不为空的字段。\n下面的例子展示了使用这些参数的方法。\n例如，如果我门想要迁移个 forum 模块，迁移文件在这个模块所在目录的 migrations 目录，我们可以使用如下命令：\n# migrate the migrations in a forum module non-interactively $ yii migrate --migrationPath=@app/modules/forum/migrations --interactive=0 配置全局命令 # 与其每次执行迁移命令的时候都要重复配置参数，你可以一次配置供后续使用：\nreturn [ 'controllerMap' =\u0026gt; [ 'migrate' =\u0026gt; [ 'class' =\u0026gt; 'yii\\console\\controllers\\MigrateController', 'migrationTable' =\u0026gt; 'backend_migration', ], ], ]; 使用上述配置，每一次你执行迁移命令，backend_migration 表将用于记录迁移纪录。你不需要在每次执行迁移都要指定这个参数了。\n多数据库迁移策略 # 默认，数据迁移都只会在一个数据库（默认是 db 组件指定的数据库）中执行。如果你想它们应用在不同数据库中，你可以指定 db 命令参数：\n$ yii migrate --db=db2 上述命令你可以把迁移应用到 db2 数据库中。\n有时你想要把其中一些迁移应用在一个数据中，而其他的在另一个数据库中。为了达到这个目标，当你实现迁移类是，你应该明确指定 DB 组件 ID：\n\u0026lt;?php use yii\\db\\Migration; class m150101_185401_create_news_table extends Migration { public function init() { $this-\u0026gt;db = 'db2'; parent::init(); } } 上述迁移将应用 db2 表中，即使你通过命令行参数指定成其他数据库。注意，迁移历史仍然记录在有命令行参数指定的数据库中。\n如果你有多个使用同一个数据的迁移，建议你创建一个迁移基类（如上述的 init() 代码）。每一个迁移类都拓展这个基类。\n提示：除了设置 db 属性之外，你还可以在你的迁移类中创建新的数据库连接。你可以使用 DAO 方法 来操作不同的数据库。\n迁移多个数据的另外一个策略就是为不同数据库迁移放在不同的路径中。然后你可以分别来执行迁移：\n$ yii migrate --migrationPath=@app/migrations/db1 --db=db1 $ yii migrate --migrationPath=@app/migrations/db2 --db=db2 ... 第一个命令执行 db1 数据库的迁移，第二个是 db2，一次类推。\n译自 http://www.yiiframework.com/doc-2.0/guide-db-migrations.html\n","date":"2015-12-24","permalink":"/n3xtchen/2015/12/24/into-yii-db-migration/","section":"时间线","summary":"前提 # php \u0026gt;= 5.","title":"探索 Yii2: 数据库迁移（Database Migration）"},{"content":"","date":"2015-10-12","permalink":"/n3xtchen/tags/sping/","section":"标签","summary":"","title":"sping"},{"content":"这个教程我们将使用 Spring 创建一个 “Hello World” 的 Restful 服务。\n你将得到什么 # 你将创建一个服务接受 HTTP 的 GET 请求，返回如下内容：\n$ curl http://localhos:8080/greeting -X GET {\u0026quot;id\u0026quot;:1,\u0026quot;content\u0026quot;:\u0026quot;Hello, World!\u0026quot;} 你可以通过改变 name 参数来定制化响应内容：\n$ curl http://localhos:8080/greeting?name=Me -X GET {\u0026quot;id\u0026quot;:1,\u0026quot;content\u0026quot;:\u0026quot;Hello, Me!\u0026quot;} 教程要求 # JDK 1.8 或更后 Gradle 2.3＋ (我的版本是 2.7) 项目准备 # 创建项目目录 # $ mkdir -p src/main/java/hello 创建一个 Gradle 构建文件 # $ cat build.gradle buildscript { repositories { mavenCentral() } dependencies { classpath(\u0026quot;org.springframework.boot:spring-boot-gradle-plugin:1.2.7.RELEASE\u0026quot;) } } apply plugin: 'java' apply plugin: 'spring-boot' jar { baseName = 'n3xt-rest-service' version = '0.1.0' } repositories { mavenCentral() } sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile(\u0026quot;org.springframework.boot:spring-boot-starter-web\u0026quot;) testCompile(\u0026quot;junit:junit\u0026quot;) } task wrapper(type: Wrapper) { gradleVersion = '2.7' } Spring Boot Gradle 插件 提供了很多简便的特性：\n它自动收集所有的 jar 包并添加到 classpath 中，构建一个单一可执行的 jar 包，使其更容易执行和传输服务。 它检索所有的 public static void main() 方法，并标记其所在类可执行。 它提供内置的解决依赖，指定与其匹配的 Spring Boot 依赖 的版本。你可以覆盖你想要的任何版本，它将成为默认的 Boot 选择的版本。 进入正题 # 创建一个资源呈现类(Resource Representation Class) # 你已经设置了项目和构建系统，你现在可以开始创建的 web 服务了。\n首先思考一下交互。\n服务处理来时 /greetng 的 GET 请求，可选带一个 name 查询参数。GET 请求应该返回一个带有 JSON 的 200OK 的响应。它看起来如下：\n{ \u0026quot;id\u0026quot;: 1, \u0026quot;content\u0026quot;: \u0026quot;Hello, World!\u0026quot; } id 字段是 Greeting 的唯一标识，content 是一个问候的文本呈现。\n为了模块化问候呈现，你可以创建一个资源呈现类。提供一个带属性，构造器以及id 和 content 数据访问器的老式 Java 对象:\n// src/main/java/hello/Greeting.java package hello; public class Greeting { private final long id; private final String content; public Greeting(long id, String content) { this.id = id; this.content = content; } public long getId() { return id; } public String getContent() { return content; } } 注意：正如你所看到的，Spring 使用 Jackson JSON 库子自动把 Greeting 类转换成 JSON。\n下一步你创建资源控制器来服务这些问候请求。\n创建一个资源控制器(Resource Controller) # 使用 Spring 的方式来创建一个 Web 服务，HTTP 请求由控制器来处理。这个组件使用 @RestController 注释来标识，并由 GreetingController 处理 /greeting 的 GET 请求，返回一个新的 Greeting 类的实例：\n// src/main/java/hello/GreetingController.java package hello; import java.util.concurrent.atomic.AtomicLong; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @RestController public class GreetingController { private static final String template = \u0026quot;Hello, %s!\u0026quot;; private final AtomicLong counter = new AtomicLong(); @RequestMapping(\u0026quot;/greeting\u0026quot;) public Greeting greeting(@RequestParam(value=\u0026quot;name\u0026quot;, defaultValue=\u0026quot;World\u0026quot;) String name) { return new Greeting(counter.incrementAndGet(), String.format(template, name)); } } 这个控制器简洁而且简单，但是还是包含很多的概念。让我们一步一步拆解。\n@RequestMapping 注释确保 /greeting 的请求映射到 greeting() 方法。\n注意： 上面的例子没有指明 GET，PUT 或者 POST 等等，因为 @RequestMapping 默认可以响应上述所有种类的请求。使用 @RequestMapping(method=GET) 来明确指定它。\n@RequestParam 绑定请求参数 name 的值到 greeting() 的 name 方法参数上。这个查询字符串不是必须的；如果请求中未传递，将使用默认值(defaultValue, 这里的值是 World)。\n方法中创建返回一个新的 Greeting 对象，其中 id 的值基于 counter 的递增值，name 根据给予 name 参数和 greeting 模版 格式化的结果。\n传统的 MVC 控制器和 Restful 服务控制器最大的不同就是 HTTP 请求体的不同。不依赖于利用模版技术(View Technology) 来渲染 Greeting 对象成 HTML。对象数据将直接以 JSON 形式输出。\n这个代码使用了 Spring 4 的新注释 @RestController，它标记这个类作为一个控制器，其中每一个方法返回的是一个领域对象而不是视图(View)。简要的说就是把 @Controller 和 @ResponseBody 融合在一起。\nGreeting 对象必须转化成 JSON。对亏了 Spring 的 HTTP 信息转化器的支持，你不需要手动做这个转化。因为 Jackson 2 在 classpath 中，Spring 的MappingJackson2HttpMessageConverter 将自动被选择用来转化 Greeting 实例成 JSON。\n使应用可执行 # 虽然可以把这个服务打包成传统的 WAR 文件，直接部署到外部的应用服务，下面将演示如何创建一个独立应用。你把所有的东西打包到一个可执行的 JAR 文件，在古老的 main() 方法中。接着，你使用 Spring 内置支持的 Tomcat 服务容器来代替部署成外部实例。\n// src/main/java/hello/Application.java package hello; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } @SpringBootApplication 是一个很便捷的注释语法，可以添加如下配置：\n@Configuration 标记类作为一个 bean 源，来定义应用的上下文。 @EnableAutoConfiguration 告诉 Sprint Boot 基于 classplath 配置的 Bean，其他的 Bean 以及各种属性配置 正常情况你要加入一个 @EnableWebMvc 注释，但是当 spring-webmvc 存在于 classpath 中的时候，Spring Boot 会自动添加它。这个标记应用作为一个 Web 服务，激活关键的行为，例如 DisoatcherServlet。 @ComponentScan 告诉 Spring 查找在 hello 中的其他的组件，配置以及服务，允许它查找 GreetingController。 这个 main() 方法使用 Spring Boot 的 SpringApplication.run() 方法来启动应用。你注意到了吗？没有写一行的 XML？也没有 web.xml 文件。这个 Web 应用 100％ 的 JAVA，你不必关心任何的基础配置。\n构建一个可执行 JAR # 如果你使用 Gradle，你可以使用 ./gradlew bootRun 运行应用。\n你可以构建一个简单的可执行 JAR，包含所有的依赖，类以及资源。这样整个开发周期中有利于传递，版本控制以及部署，跨平台等等。\n./gradlew build 然后你运行你的 JAR 文件：\njava -jar build/libs/n3xt-rest-service-0.1.0.jar 如果你使用 Maven，你可以使用 mvn spring-boot:run 运行应用。或者你可以使用 mvn clean package 编译 JAR 文件和运行他：\njava -jar target/n3xt-rest-service-0.1.0.jar 注意：整个过程将会创建一个可执行 JAR 文件。你也可以选择编译成 WAR 文件。\n输出日志将会显示。服务将在几秒后启动和运行。\n测试你的服务 # 现在服务已经启动，访问 http://localhost:8080/greeting，你将看到：\n{\u0026quot;id\u0026quot;:1,\u0026quot;content\u0026quot;:\u0026quot;Hello, World!\u0026quot;} 传递了一个 name 参数，像这样 http://localhost:8080/greeting?name=User。你可能已经注意到 content 值的变化了：\n{\u0026quot;id\u0026quot;:2,\u0026quot;content\u0026quot;:\u0026quot;Hello, User!\u0026quot;} 这个变化意味了 @RequestParam 在 GreetingController 按照预期生效了。name 参数已经设置了，但是可以使用查询参数来明确覆盖。\n同时也注意到 id 从 1 变成 2。这证明你对同一个对象进行多次请求，counter 每次请求都会按照预期递增。\n总结 # 恭喜你学会使用 Spring 开发 Restful 应用了。\n","date":"2015-10-12","permalink":"/n3xtchen/2015/10/12/using-spring-boot-to-build-a-restful-web-server/","section":"时间线","summary":"这个教程我们将使用 Spring 创建一个 “Hello World” 的 Restful 服务。","title":"使用 Spring Boot: 构建 Restful 服务"},{"content":"","date":"2015-10-10","permalink":"/n3xtchen/tags/gradle/","section":"标签","summary":"","title":"gradle"},{"content":"这个指南将引导你使用 Gradle 创建一个简单的 Java 项目。\n成果 # 你讲创建一个简单的应用，然后使用 Gradle 构建它。\n要求 # 需要 15 分钟 使用你常用的文本编辑器或者 IDE JDK 6 以上（我的机子使用的 JDK 8） 配置你的项目 # 首先，你需要配置使用 Gradle 构建的 Java 项目。为了注意力集中在 Gradle 上，我们现在创建一个尽可能简单的项目。\n创建目录结构 # 在你的项目目录下，创建如下所示子目录结构；比如，你可以在 *nix 系统键入命令：\n$ mkdir -p src/main/java/hello $ tree -L 5 . └── src └── main └── java └── hello 4 directories, 0 files 你可以在 src/main/java/hello 中创建任何你想要的 Java 类。为了简化和剩下步骤的一致性， Spring 推荐你创建两个类：HelloWorld.java 和 Greeter.java。\n$ cat src/main/java/hello/HelloWorld.java package hello; public class HelloWorld { public static void main(String[] args) { Greeter greeter = new Greeter(); System.out.println(greeter.sayHello()); } } cat src/main/java/hello/Greeter.java package hello; public class Greeter { public String sayHello() { return \u0026quot;Hello world!\u0026quot;; } } 安装 Gradle # 现在，你已经有一个可以构建的项目了，现在你可以安装 Gradle。\n由于我的系统是 Ubuntu 14.04 ，这里提供 Linux 的安装方法。\n安装 Java # 如果你使用也是 linux，Java 还未安装的话，可以尝试下面安装方法：\n$ add-apt-repository ppa:webupd8team/java $ apt-get update $ apt-get install oracle-java8-installer 开始安装 Gradle # $ add-apt-repository ppa:cwchien/gradle $ apt-get update $ apt-get install gradle-2.7 测试 Gradle 是否安装成功 # $ gradle :help Welcome to Gradle 2.7. To run a build, run gradle \u0026lt;task\u0026gt; ... To see a list of available tasks, run gradle tasks To see a list of command-line options, run gradle --help To see more detail about a task, run gradle help --task \u0026lt;task\u0026gt; BUILD SUCCESSFUL Total time: 2.696 secs This build could be faster, please consider using the Gradle Daemon: https://docs.gradle.org/2.7/userguide/gradle_daemon.html 如果看到这样的信息，说明 Gradle 安装成功，可以进行下一步。\n看看 Gradle 能干什么 # 既然 Gradle 已经安装好了，现在看看它能做什么。在你创建一个 build.gradle 之前，你可以询问下 Gradle 有哪些任务可用：\n$ gradle tasks :tasks ------------------------------------------------------------ All tasks runnable from root project ------------------------------------------------------------ Build Setup tasks ----------------- init - Initializes a new Gradle build. [incubating] wrapper - Generates Gradle wrapper files. [incubating] Help tasks ---------- components - Displays the components produced by root project 'n3xt-gradle'. [incubating] dependencies - Displays all dependencies declared in root project 'n3xt-gradle'. dependencyInsight - Displays the insight into a specific dependency in root project 'n3xt-gradle'. help - Displays a help message. model - Displays the configuration model of root project 'n3xt-gradle'. [incubating] projects - Displays the sub-projects of root project 'n3xt-gradle'. properties - Displays the properties of root project 'n3xt-gradle'. tasks - Displays the tasks runnable from root project 'n3xt-gradle'. To see all tasks and more detail, run gradle tasks --all To see more detail about a task, run gradle help --task \u0026lt;task\u0026gt; BUILD SUCCESSFUL Total time: 4.117 secs This build could be faster, please consider using the Gradle Daemon: https://docs.gradle.org/2.7/userguide/gradle_daemon.html 虽然这些任务都是可用的，但是没有项目构建配置，他们不能提供足够的价值。随着你对 build.gradle 的充实，一些任务将要更有用，因此你是不是的需要运行下 task 检查下可用的任务。\n说起添加插件，下一步你需要添加插件来启用 Java 基础构建功能。\n构建 Java 代码 # 从简单的开始，创建一个只有一行的 build.gradle 文件：\n$ cat build.gradle apply plugin: 'java' 这样的一行将带了强大的特性，再次运行 gradle task 看看，你将看到一些新的任务加到列表中，包括构建项目，创建 JavaDoc，以及运行测试。\n你将经常使用 gradle build 来构建项目。这个任务编译，测试和打包成 JAR 包。你可以试着运行下： $ gradle build\n过了几秒，BUILD SUCCESSFUL 说明构建已经完成。\n看看构建后的结果，看看 build 目录。\nbuild |-- classes | `-- main | `-- hello | |-- Greeter.class | `-- HelloWorld.class |-- dependency-cache |-- libs | `-- n3xt-gradle.jar `-- tmp |-- compileJava `-- jar `-- MANIFEST.MF 8 directories, 5 files 在那里，你将找到一些文件夹，包括下面三个常见目录：\nclasses: 编译后的 .class 文件 reports: 构建产生的报道（例如测试报告） libs: 打包后的项目库（一般是 JAR 或者 WAR 文件） 这时在 classes 目录，你应该可以找到 HelloWorld.class 和 Greeter.class。\n现在，你的项目还有任何库依赖，因此没有 dependency_cache 目录。\nreports 目录中应该包含一个单元测试的报告。因为这个项目还没有编写人和单元测试，报告也是没有什么有趣的内容。\nlibs 目录中应该包含一个 JAR 包，和你的项目目录同名。后续，你将会学学会如何制定包名和它的版本。\n声明依赖 # 这个简单的 Hello World 例子完全是自给自足的，不依赖任何额外的苦。然而，大部分应用都来外部的库来处理通用或者复杂的功能。\n例如，想要在显示 “Hello World!” 同时，你想要答应当前的时间。你当然可以使用原生的时间处理功能，但是为了让例子更有趣些，我们将使用 Joda Time 库。\n首先，修改我们的 HelloWord.java：\npackage hello; import org.joda.time.LocalTime; public class HelloWorld { public static void main(String[] args) { LocalTime currentTime = new LocalTime(); System.out.println(\u0026quot;The current local time is: \u0026quot; + currentTime); Greeter greeter = new Greeter(); System.out.println(greeter.sayHello()); } } 这里 HelloWorld 使用 Joda Time 类来获取和打印当前时间。\n如果你现在使用 gradle build 命令来编译项目，编译器将会失败，因为你还没有声明 Joda Time 作为编译的依赖。\n对于初学者，你可能需要添加第三方源。\nrepositories { mavenCentral() } repositories 指明了构建可以通过 Maven Central 源来解决它的依赖问题。Gradle 重度依赖由 Maven 构建工具提供的很多惯例以及功能，包括使用 Maven Central 作为库依赖的源。\n现在，你已经准备好第三方库的源；然后让我们一起声明一些东西。\nsourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile \u0026quot;joda-time:joda-time:2.2\u0026quot; } 在 repositories 中，你为 Joda Time 声明一个独立依赖，并且明确的制定你需要的是 2.2 版本。\n关于依赖，你另外需要注意的是这个指定的是编译依赖，说明它只在编译时可用（如果你想要构建一个 War 文件的话，包含在 /Web-INF/libs 目录中的）。另一种只要的依赖类型有：\nprividedCompile：在编译代码时要求的依赖，但是它也将提供在 runtime 时由容器运行需要的依赖（例如，Java Servlet API）。 testCompile：在编译和运行测试时的依赖，但是不包括在构建和运行 runtime 代码中。 最后，让我们手动指定 JAR 包名。\njar { baseName = 'n3xt-gradle' version = '0.1.0' } jar 指定 JAR 将来将如何命名。这个例子中，它将是 n3xt-gradle-0.1.0.jar。\n现在，如果你运行 gradle build，Gradle 应该已经从 Maven Central 源中解决了 Joda Time 的依赖问题，并且构建成功。\n使用 Gradle Wrapper 构建你的项目 # Gradle Wrapper 是使用 Gradle 构建工具的最佳方式。它包含 Windows 批处理脚本以及 OS X 和 Linux 的脚本。这些脚本允许你不用预先在你的自己上安装 Gradle 前提下运行 Gradle 构建项目。为了使用它，在你的 build.gradle 底部添加如下代码。\ntask wrapper(type: Wrapper) { gradleVersion = '2.7' } 运行如下命令，下载和初始化 wrapper 脚本：\n$ gradle wrapper 这个任务完成后，你将看到一些新文件。由两个脚本在你的项目根目录中，同时 wrapper jar 和 properties 文件已经添加到新的 gradle/wrapper 目录中。\n. ... |-- gradle | `-- wrapper | |-- gradle-wrapper.jar | `-- gradle-wrapper.properties |-- gradlew `-- gradlew.bat Gradle Wrapper 现在已经在你的项目可用了。把它添加到你的版本控制系统中；这样只用使用你的项目代码，那你们的编译环境将都相同。由于安装了同一版本的 Gradle ，所以可以被完全相同的方法使用。和之前一样，运行构建命令：\n$ ./gradlew build 第一次使用指定版本的 Gradle Wrapper 时，它将下载和缓存 Gradle 二进制代码。Gradle Wrapper 文件就是为了版本控制而设计的，因此任何人都不用实现安装和配置 Gradle。\n这个步骤，你已经构建好你的代码了。JAR 文件包含了两个指定的类文件（Greeter 和 HelloWorld）；我们快速预览下。\n$ jar tvf build/libs/n3xt-gradle-0.1.0.jar 0 Sun Oct 11 13:38:44 UTC 2015 META-INF/ 25 Fri Oct 09 18:22:50 UTC 2015 META-INF/MANIFEST.MF 0 Fri Oct 09 18:22:50 UTC 2015 hello/ 988 Sun Oct 11 13:38:44 UTC 2015 hello/HelloWorld.class 369 Sun Oct 11 13:38:44 UTC 2015 hello/Greeter.class\n类文件已经被打包到一起了。有一个需要特别注意，虽然你已经声明了 joda-time 作为依赖，这个库病不包含在这里。并且 JAR 文件也不可执行。\n为了使这个代码可执行，我们使用 Gradle 的 application 插件。把它添加到你的 build.gradle 文件中。\napply plugin: 'application' mainClassName = 'hello.HelloWorld' 然后运行你的 app!\n$ ./gradlew run :compileJava UP-TO-DATE :processResources UP-TO-DATE :classes UP-TO-DATE :run The current local time is: 14:03:07.860 Hello world! BUILD SUCCESSFUL Total time: 7.375 secs 打包依赖需要更多想法。例如，如果你构建一个 WAR 文件，一个与第三方依赖紧密关联的通用格式，我们应该使用 WAR plugin。如果你使用 Spring Boot，想要可执行的 JAR 文件，spring-boot-gradle-plugin 将会很方便。这个阶段，Gradle 对你的系统没有足够的了解，很难做出选择。但是，现在你应该足有使用 Gradle 了。\n把这个指南中所有东西都整合到一块，这里是一个完成 build.gradle 文件：\napply plugin: 'java' apply plugin: 'application' mainClassName = 'hello.HelloWorld' // tag::repositories[] repositories { mavenCentral() } // end::repositories[] // tag::jar[] jar { baseName = 'gs-gradle' version = '0.1.0' } // end::jar[] // tag::dependencies[] sourceCompatibility = 1.8 targetCompatibility = 1.8 dependencies { compile \u0026quot;joda-time:joda-time:2.2\u0026quot; } // end::dependencies[] // tag::wrapper[] task wrapper(type: Wrapper) { gradleVersion = '2.3' } // end::wrapper[] 注意：这里有很多的 start/end 注释。这个只是为了解释，你不需要把它放到你的生产构建文件中。\n总结 # 恭喜你！你已经可以为你的 Java 项目创建一个简单高效的 Gradle 构建文件。\n参考：\n* gradle 指南 * 灵活强大的构建系统Gradle\n","date":"2015-10-10","permalink":"/n3xtchen/2015/10/10/build-java-project-with-gradle/","section":"时间线","summary":"这个指南将引导你使用 Gradle 创建一个简单的 Java 项目。","title":"使用 Gradle 创建 Java 项目"},{"content":" 译自： Understanding Nginx HTTP Proxying, Load Balancing, Buffering, and Caching\n介绍 # 这里，我们将介绍 Nginx 的 Http 代理功能（它允许 Nignx 传递请求到后端服务器，进行后续处理）。Nginx 经常被设置成反向代理帮助横向拓展设施提升负载能力或者传递请求给其他服务器。\n接下来，我们将讨论如何使用 Nginx 的负载均衡功能来横向拓展（scale out）服务器。我们同样也探讨使用缓存来提升代理的性能。\n常规的代理信息（重点校对） # 如果你过去只是在简单场景下使用 Web 服务器，单台服务器配置，那你可能会怀疑为什么需要代理请求。\n使用代理的其中一个理由是横向拓展提升你基础设施。Nignx 本来被用来处理并发请求，使它成为客户端接触点的理想选择。这个服务器可以传递请求给任何数量的后端服务器，来处理处理大量任务，达到夸设备拓展负载的目的。这个设计同样也帮助你更容易的增加服务器或者下线需要维护的服务器。\n另外一个例子就是当使用应用服务器，该应用本身不能直接处理从客户端传递过来请求时，代理服务器同样有用。很多框架（包括 Web 服务器）不如专门设计成高性能服务（如 Nignx）健壮。把 Nginx 放在这些服务之前，可以提升用户体验和安全性。\nNigix 通过接收一个请求，把它转发传递给其他服务器完成来完成代理。请求的结果会传递回 Nginx，展示给客户端。实例中的其他服务器可以是远端机器，本地服务，甚至是由 Nginx 定义的其他虚拟服务。Nginx 代理处理请求的服务器，我们称之为upstream（上游）服务。\nNginx 可以代理使用 http(s), FastCGI, SCGI 和 uwsgi 的请求，或者为每种代理制定不同指令进行多级缓存协议。Nginx 实例负责传递请求，并把各个信息组件揉成一个 upstream 可理解的格式。\n解构简单的 HTTP Proxy Pass # 最简单的代理类型莫过于把一个请求导向到单一可使用 http 协议通信的服务器。这种代理类型我们统称为 proxy pass，由 proxy_pass 同名 Nginx 指令处理。\nproxy_pass 指令主要在地址（Location）语境中被看到。它同样在 Location 和 limit_except 语境下的 if 语法块中有效。当一个请求匹配到一个包含 proxy_pass 的 Location时，这个请求将会被指令转发（Forward）到这个链接去。\n让我们看一个例子：\n# server context location /match/here { proxy_pass http://example.com; } . . . 在上面代码片段中，proxy_pass 定义中的服务器没有提供 URI。这个模式的定义，请求的 URI 会原封不动地直接传递给 upstream 服务器。\n例如，当一个匹配 /match/here/please 的请求被这个 block 处理，这个请求将会以 http://example.com/match/here/please 的形式把 URI 发送给 example.com 服务器。\n让我们一起看看另外一个场景：\n# server context location /match/here { proxy_pass http://example.com/new/prefix; } . . . 上述例子中，代理服务器在尾部定义了 URI 部分。当 URI 放到 proxy_pass 定义中，请求中匹配这个 Location 的定义的部分在传递的过程中将会被这个 URI 替换掉。\n例如，一个匹配 /match/here/please 的请求将会以 http://example.com/new/prefix/please 的形式发送给 upsream 服务器。/match/here 将会替换成 /new/prefix。这一点很重要，必须记住。\n有时，这样的替换是不可完成的。这时，定义在 proxy_pass 的尾部的URI 会被忽略，Nginx 直接把来自客户端或被其他 Nginx 的指令修改的 URI 传递给 upstream 服务器。\n例如，使用正则表达式匹配 Location 时，Nginx 不能决定 URI 的哪一个部分匹配这个表达式，于是它直接发送客户端请求的原始 URI。还有另一个例子，当一个 rewrite 指令在同一个地址中使用，会导致客户端的 URI 被重写，但是仍然在同一个 block 下处理，这时，重写的 URI 会被传递。\n理解 Nginx 处理 Headers 的方式 # 有一件事情可能现在不会马上明白，如果你希望 upstream 服务合理地处理请求，那仅仅传递 URI是不够的。来自于 Nginx 请求不同于直接来源于客户端的请求。这里的差异最大的一部分就是请求的 Headers 信息。\n当 Nginx 代理一个请求，它会自动对 Headers 信息做一些调整：\nNginx 去除任何空的 Headers。传递空值到其他服务器是没有意义的；它只会让请求变得臃肿。 Nginx 默认把包含下划线的 Headers 信息视为无效，会被移除。如果你希望把这样的信息解释为有效，你可以把 underscores_in_headers 执行设置成 on，否则你的头信息将永远不会把他发送给后端服务器。 Host 会被重写成由 $proxy_host 定义的值。它可以是 IP （或者名称）和端口，直接由 proxy_pass 指令定义的。 头信息 Connection 改成 close。这个 Header 用在关于两个服务器创建特定链接的信号信号。在这个实例中，Nginx 把它设置成 close 来指定一旦原始请求被响应，upstream 服务的这个连接将被关闭。upstream 服务器不应该期待这个连接被持久化。 从第一点看来，我们可以确定任何你不想要传递的头信息应该被设置成空字符串。带空值的头信息会被完全删除掉。\n接下来一点用来设置如果你的后端应用想要接受非标准的头，你应该确保它们不应该带下划线。如果你需要的头信息使用下划线，你可以在你的配置后面，把 underscores_in_headers 设置成 on（在 http 上下文或者为这个 IP/端口组合声明的默认服务器上下文红中有效）。如果你不想这么做，Nginx 将会把这个头标记为无效，并在传递给 upstream 服务器之前把它丢弃。\n头信息 Host 在大部分代理场景中起着重要作用。和之前讲的一样，默认，它会被设置成 $proxy_host 的值，一个由 proxy_pass 定义的包含域名或者 IP 以及端口的值。这样的默认设定是为了让 Nginx 确保 upsteam 服务器可以响应的唯一的地址（它直接从连接信息取出的）。\nHost 常见的值如下：\n$proxy_host：它把 Host 头设置成从 proxy_pass 定义的域名或IP加上端口的组合。从 Nginx 的角度看，它是默认以及安全的，但是经常不是被代理服务器需要的来正确处理请求的值。 $http_host：它把 Host 头设置成从和直接从客户端请求的一样。这个头由客户端发送，可以被 Nginx 使用。这个变量名前缀是 $http_，后面跟着头信息的名称，以小写命名，任何斜杠都会被下划线替换。虽然 $http_host 在大部分情况可用的，但是当客户端请求没有有效的 Host 信息的时候，会导致传输失败。 $host：这个是偏好是指：可以是来自请求的主机名，请求中的 Host 头信息或者匹配请求的服务器名。 在大部分情况，你将会把 Host 头信息设置成 $host 变量。它是最灵活的，经常为被代理的服务器提供尽可能精确的 Host 头信息。\n设置或者重置 Header # 为了适配代理连接，我们使用 proxy_set_header 指令。例如，为了改变我们之前讨论的 Host 头信息，并增加一些其它的和代理请求一样的 Header，我们可以这么做：\n# server context location /match/here { proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://example.com/new/prefix; } . . . 上述请求把 Host 头设置成 $host 变量，它讲包含请求的原始 host。X-Forwarded-Proto 头信息提供了关于院士客户端请求模式的被代理服务器信息（决定 http 还是 https 请求）。\nX-Real-IP 被设置成客户端的 IP 地址，以便代理服务器做判定或者记录基于该信息的日志。X-Forwarded-For 头信息是一个包含整个代理过程经过的所有服务器 IP 地址的列表。在上述例子中，我们把它设置成 $proxy_add_x_forwarded_for 变量。这个变量包含了从客户端获取的 X-Forwarded-For 头信息并把 Nginx 服务器的 IP 添加到最后。\n当然，我们会把 proxy_set_header 指令移到服务器或者 http 上下文的外部，允许它同时在多个 Location 生效：\n# server context proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_Header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location /match/here { proxy_pass http://example.com/new/prefix; } location /different/match { proxy_pass http://example.com; } 为负载均衡代理服务器定义 Upstream 上下文 # 在上一个例子中，我们演示了如何实现为了一个单一后端服务器做简单的 Http 代理。Nginx 帮助我们很容易通过指定一个后端服务器集群池子来扩展这个配置。\n我们使用 upstream 指令来定义服务器群的池子（pool）。这个配置假设这个服务器列表中的每台机子都可以处理来自客户端的请求。它允许我们轻轻松松横向扩展我们的基础设施。upstream 指令必须在 Nginx 的 http 上下文中设置。\n让我们一起看个简单的例子：\n# http context upstream backend_hosts { server host1.example.com; server host2.example.com; server host3.example.com; } server { listen 80; server_name example.com; location /proxy-me { proxy_pass http://backend_hosts; } } 上述例子，我们设置一个叫做 backend_hosts 的 upstream 上下文。一旦定义了，这个名称可以直接在 proxy_pass 中使用，就和常规的域名一样。如你所见，在我们的服务器块内，所有指向 example.com/proxy-me/\u0026hellip; 的请求都会被传递到我们定义的池子中。在那个池子内，会根据配置算法选取一个服务器。默认，它只是一个简单的循环选择处理（每一个请求都会按顺序传递给不同的服务器）。\n改变 Upstream 均衡算法 # 你可以通过指令或者标志修改 upstream 池子的均衡算法：\n（轮询 Round Robin）：默认的负载均衡算法，如果没有其它算法被指定的话，它会被使用。upstream 上下文定义的每一个服务器都会按顺序接受请求。 最少连接 least_conn: 指定新的连接永远应该传递给拥有最少连接的后端服务器。这个算法在后端连接有时候需要持久化的情况下将很有效。 ip 哈希 ip_hash：这种均衡算法基于客户端的 IP 分发到不同的服务器。把客户端 IP的前三位八进制数作为键值来决定服务器处理哪个请求。这样结果是，客户端每次由同一个台服务器服务，它能保证回话的一致性。 哈希 hash：这类均衡算法主要运用于缓存代理。服务器是给予提供的哈希值来分隔的。它可以是文本，变量或者文本变量组合。 修改均衡算法，应该像下面那样：\n# http context upstream backend_hosts { least_conn; server host1.example.com; server host2.example.com; server host3.example.com; } . . . 上述例子中，最少连接数的服务器将会优先选择。ip_hash 指令也可以用同样的方式设置，来获取同样数量的会话粘性。\n至于 hash 方法，你应该提供要哈希的键。可以是任何你想要的：\n# http context upstream backend_hosts { hash $remote_addr$remote_port consistent; server host1.example.com; server host2.example.com; server host3.example.com; } . . . 上述的例子，请求分发是基于客户端的 ip 和端口。我们也可以添加另外的参数 consistent，它实现了 Ketama 一致性 Hash 算法。基本上，它意味着如果你的 upstream 服务器改变了， 保证对 cache 的最小印象。\n设置服务器权重 # 在后端服务器声明中，每一台的服务器默认是权重平分的。它假定每一台服务器都能也都应该处理同一量级的负载（考虑到负载均衡算法的影响）。然而，你也可以为服务器设置其它的权重。\n# http context upstream backend_hosts { server host1.example.com weight=3; server host2.example.com; server host3.example.com; } . . . 上述例子中，host1.example.com 可以比其它服务器多接受 2 倍的流量。默认，每一台服务器的权重都是 1。\n使用 Buffer 缓解后端服务器的负载 # 对于大部分使用代理的用户最关心的一件事情就是添加一台服务器对性能的影响。大部分场景下，利用 Nginx 的缓冲和缓存能力，可以大大地减轻负担。\n当代理到其它服务器是，两个不同连接的速度影响客户端的体验：\n从客户端到代理服务器的连接 从代理服务器到后端服务器的连接 Nginx 可以根据你想要优化哪一个连接来调整它的行为。\n没有缓冲（buffer），数据将会直接从代理服务器传输到客户端。如果客户端的速度足够快（假设），buffer 可以关掉让数据尽可能快速地到达。如果使用 buffer，Nginx 代理服务器将会临时存储后端响应，然后慢慢把数据喂给客户端。如果客户端很慢，它可以让 Nginx 提前关闭后端服务器的连接。它可以控制在哪一步处理分发数据给客户端。\nNginx 默认的缓冲设计是客户端有这千差万别的速度。我们可以使用如下指令来调整缓冲速度。你可以把 buffer 设置防盗 http，server 或者 location 上下文中。必须记住指令为每一个请求配置的大小，因此如果客户端的请求很多，把值调的过大，会很影响性能：\nporxy_buffering：这个指令控制所在上下文或者子上下文的buffer是否打开。默认是 on。 proxy_buffers：这个指令控制 buffer 的数量（第一个参数）和大小（第二个参数）。默认是 8 个 buffer，每个 buffer 大小是 1 个内存页（4k 或 8k）。增加 buffer 的数量可以缓冲更多的信息。 proxy_buffer_size：是来自后端服务器响应信息的一部分，它包含头信息，从响应的部分分离出来。这个指令设置响应部分的缓冲。默认，它和 proxy_buffers 一样，但是因为它仅用于头信息，所有它市场被设置成更低的值。 proxy_busy_buffer_size：这个指令设置忙时 buffer 的最大值。一个客户端一次只能从一个 buffer 读取数据的同时，buffer 会被放到队列中，等待发送到客户端。这个指令控制在这个状态下 buffer 的空间大小 proxy_max_temp_file_size：这个是 Nginx 一次可以写临时文件的最大数据量，当代理服务器的响应太大超出配置的 buffer 的时候。 proxy_temp_path：当代理服务器的响应太大超出配置的 buffer 的时候，Nginx 写临时文件的路径。 正如你看到的，Nginx 提供了很少指令来调整缓冲行为。大部分时间，你不需要关心大部分指令，但是它们中的一些会很有用。可能最有用的就是 proxy_buffer 和 proxy_buffer_size 这两个指令。\n下面这个例子中睁开每个 upstream 可用代理 buffer 数，减少存储头信息的 buffer 数：\n# server context proxy_buffering on; proxy_buffer_size 1k; proxy_buffers 24 4k; proxy_busy_buffers_size 8k; proxy_max_temp_file_size 2048m; proxy_temp_file_write_size 32k; location / { proxy_pass http://example.com; } 相反，如果你的客户端足够快到你可以直接传输数据，你就可以完全关掉 buffer。实际上，即使 upstream 比客户端快很多，Nginx 还是会使用 buffer 的，但是它会直接清空到客户端的数据，不会让它进入到池子。如果客户端很慢，这会导致 upstream 连接会一直开到客户端赶上为止。当缓冲被设置为 off 的时候，只有 proxy_buffer_size 指令定义的 buffer 会被使用。\n# server context proxy_buffering off; proxy_buffer_size 4k; location / { proxy_pass http://example.com; } ","date":"2015-10-01","permalink":"/n3xtchen/2015/10/01/nginx-port-forwording/","section":"时间线","summary":"译自： Understanding Nginx HTTP Proxying, Load Balancing, Buffering, and Caching","title":"Nginx-端口转发(反向代理)"},{"content":"","date":"2015-10-01","permalink":"/n3xtchen/categories/nignx/","section":"分类页","summary":"","title":"nignx"},{"content":"","date":"2015-08-13","permalink":"/n3xtchen/tags/mysql/","section":"标签","summary":"","title":"mysql"},{"content":" 译自 PostGrest Introduction\n我目前在进行一个项目， PostgREST，它读取你 PostgreSQL 数据库，自动创建“一个简洁的，更标准兼容，更快的 API“。\n安装 # 二进制文件在 OS X 和 Linux 下可用，你还需要安装好的 PostgreSQL 。接下来，让我们创建一个简单的数据库，并使用一些数据丰富它。 Sqitch 是一个管理数据库变更的好工具，我们将在后面的使用到它。\n$ wget https://github.com/begriffs/postgrest/releases/download/v0.2.10.0/postgrest-0.2.10.0-osx.tar.xz $ tar xvfJ postgrest-0.2.10.0-osx.tar.xz x postgrest-0.2.10.0 $ mv postgrest-0.2.10.0 postgrest $ chmod +x postgrest 工作原理 # PostgREST 允许你建立一个多版本的 API 接口，它假定你将每一个版本存储在 schema 中，因此版本 1 就是存储在 schema “1” 中，以此类推。\n显然，大部分产品数据库不会有一个名叫 “1” 的 schema，因此我们可以在 schema “1” 中创建视图把数据暴露给 PostgREST。我也强烈推荐你也在新的项目中使用这套规范，一个 REST 资源没必要映射表的每一行，使用视图从我们的存储格式转化成我们的展示层。\n创建一个数据 # （首先，让我们添加一个有用的 git 别名，我们将在后续中使用，可选）\n$ git config --global alias.add-commit '!git add -A \u0026amp;\u0026amp; git commit' 接下来，创建一个简单的数据库，使用一些数据丰富它。\n$ createdb goodfilm $ git init . Initialized empty Git repository in /Users/ichexw/Dev/pgsql/restapi/.git/ $ sqitch init goodfilm Created sqitch.conf Created sqitch.plan Created deploy/ Created revert/ Created verify/ $ sqitch config core.engine pg $ sqitch target add production db:pg://localhost:5432/goodfilm $ sqitch engine add pg $ sqitch engine set-target pg production $ sqitch add appschmea -n 'Add schema for good film object' Created deploy/appschmea.sql Created revert/appschmea.sql Created verify/appschmea.sql deploy/appschmea.sql\nBEGIN; create schema film; COMMIT; revert/appschmea.sql\nBEGIN; drop schema film; COMMIT; 部署你的变更\n$ sqitch deploy Adding registry tables to production Deploying changes to production + appschmea .. ok 如果部署成功，把变更提交到版本控制中\n$ git add-commit -m \u0026quot;Adding goodfilm schema\u0026quot; [master (root-commit) d2d6c70] Adding goodfilm schema 6 files changed, 41 insertions(+) create mode 100644 deploy/appschmea.sql create mode 100644 postgrest-0.2.10.0-osx.tar.xz create mode 100644 revert/appschmea.sql create mode 100644 sqitch.conf create mode 100644 sqitch.plan create mode 100644 verify/appschmea.sql 现在重复上面的步骤，添加 goodfilm 表（一般你会吧这个分到多个文件中，但是为了简洁，我们将使用一个文件）。\n$ sqitch add film -n \u0026quot;Add the goodfilm tables\u0026quot; Created deploy/film.sql Created revert/film.sql Created verify/film.sql Added \u0026quot;film\u0026quot; to sqitch.plan deploy/film.sql\nBEGIN; CREATE TABLE film.director ( name text NOT NULL PRIMARY KEY ); CREATE TABLE film.film ( id serial PRIMARY KEY, title text NOT NULL, year date NOT NULL, director text, rating real NOT NULL DEFAULT 0, language text NOT NULL, CONSTRAINT film_director_fkey FOREIGN KEY (director) REFERENCES film.director (name) MATCH SIMPLE ON UPDATE NO ACTION ON DELETE NO ACTION ); CREATE TABLE film.festival ( name text NOT NULL PRIMARY KEY ); CREATE TABLE film.competition ( id serial PRIMARY KEY, name text NOT NULL, festival text NOT NULL, year date NOT NULL, CONSTRAINT comp_festival_fkey FOREIGN KEY (festival) REFERENCES film.festival (name) MATCH SIMPLE ON UPDATE NO ACTION ON DELETE NO ACTION ); CREATE TABLE film.nominations ( id serial PRIMARY KEY, competition integer NOT NULL, film integer NOT NULL, won boolean NOT NULL DEFAULT true, CONSTRAINT nomination_competition_fkey FOREIGN KEY (competition) REFERENCES film.competition (id) MATCH SIMPLE ON UPDATE NO ACTION ON DELETE NO ACTION, CONSTRAINT nomination_film_fkey FOREIGN KEY (film) REFERENCES film.film (id) MATCH SIMPLE ON UPDATE NO ACTION ON DELETE NO ACTION ); COMMIT; revert/film.sql\nBEGIN; DROP TABLE film.director CASCADE; DROP TABLE film.film CASCADE; DROP TABLE film.festival CASCADE; DROP TABLE film.competition CASCADE; DROP TABLE film.nominations CASCADE; COMMIT; $ sqitch deploy Deploying changes to production + film .. ok $ git add-commit -m \u0026quot;Adding film table\u0026quot; [master 1122a28] adding film table 4 files changed, 76 insertions(+) create mode 100644 deploy/film.sql create mode 100644 revert/film.sql create mode 100644 verify/film.sql 好的，现在让我们添加一些数据看看。我们将使用威尼斯金狮奖和金棕榈奖最近20年被提名的电影。\n$ git clone https://gist.github.com/7d07a5cd840734342d35.git Cloning into '7d07a5cd840734342d35'... remote: Counting objects: 6, done. remote: Compressing objects: 100% (4/4), done. remote: Total 6 (delta 1), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (6/6), done. Checking connectivity... done. $ psql -d goodfilm \u0026lt; d07a5cd840734342d35/insert_data.sql 现在，让我们查询下数据\n$ psql -d goodfilm goodfilm=# select title, rating from film.film limit 5; title | rating ---------------------+-------- Chuang ru zhe | 6.2 The Look of Silence | 8.3 Fires on the Plain | 5.8 Far from Men | 7.5 Good Kill | 6.1 (5 rows) goodfilm=# select * from film.nominations limit 5; id | competition | film | won ----+-------------+------+----- 1 | 1 | 1 | f 2 | 1 | 2 | f 3 | 1 | 3 | f 4 | 1 | 4 | f 5 | 1 | 5 | f (5 rows) 让我们查询下提名前五的电影\ngoodfilm=# SELECT substring(f.title from 1 for 20) as title, c.name, f.rating from film.nominations as n LEFT JOIN film.film as f ON f.id=n.film LEFT JOIN film.competition as c ON c.id=n.competition ORDER BY f.rating DESC limit 5; title | name | rating ----------------------+-------------+-------- Winter Sleep | Palme d'Or | 8.5 Mommy | Palme d'Or | 8.3 The Look of Silence | Golden Lion | 8.3 Birdman: Or (The Une | Golden Lion | 8 Sivas | Golden Lion | 7.7 (5 rows) 创建一个 API # 现在我们将会使用 PostgREST 来创建一个 API。我们将暴露是三个资源：\nFilm：电影列表 Festival：包含每年赛事和提名 Director：包含这些电影的导演信息 首先，我们需要添加一个新的 schema\n$ sqitch add v1schema -m 'adding API v1 schema' Created deploy/v1schema.sql Created revert/v1schema.sql Created verify/v1schema.sql Added \u0026quot;v1schema\u0026quot; to sqitch.plan deploy/v1schema.sql\nBEGIN; create schema \u0026quot;1\u0026quot;; COMMIT; revert/v1schema.sql\nBEGIN; drop schema \u0026quot;1\u0026quot;; COMMIT; 提交它，部署它并把文件加到我们的 API\n$ git add-commit -m \u0026quot;Adding API schema\u0026quot; [master 68f5b73] Adding API schema 6 files changed, 25 insertions(+) create mode 160000 7d07a5cd840734342d35 create mode 100644 deploy/v1schema.sql create mode 100644 postgrest-0.2.10.0-osx.tar.xz.1 create mode 100644 revert/v1schema.sql create mode 100644 verify/v1schema.sql $ sqitch deploy Deploying changes to production + v1schema .. ok $ sqitch add v1views -m \u0026quot;Adding API v1 views\u0026quot; Created deploy/v1views.sql Created revert/v1views.sql Created verify/v1views.sql Added \u0026quot;v1views\u0026quot; to sqitch.plan 现在我们把视图添加我们想要暴露的 API 中。\ndeploy/v1views.sql\nBEGIN; create or replace view \u0026quot;1\u0026quot;.film as select title, film.year, director, rating, language, comp.name as competition from film.film left join film.nominations as n on film.id = n.film left join film.competition as comp on n.competition = comp.id; create or replace view \u0026quot;1\u0026quot;.festival as select comp.festival, comp.name as competition, comp.year, film.title, film.director, film.rating from film.nominations as noms left join film.film as film on noms.film = film.id left join film.competition as comp on noms.competition = comp.id order by comp.year desc, comp.festival, competition; create or replace view \u0026quot;1\u0026quot;.director as select d.name, f.title, f.year, f.rating from film.director as d left join film.film as f on f.director = d.name; COMMIT; revert/v1views.sql\nBEGIN; drop view \u0026quot;1\u0026quot;.film; drop view \u0026quot;1\u0026quot;.director; drop view \u0026quot;1\u0026quot;.festival; COMMIT; 现在我们部署它，然后运行 PostGREST 看看\n$ sqitch deploy Deploying changes to production + v1views .. ok $ postgrest --db-host localhost --db-port 5432 --db-name goodfilm --db-pool 200 --anonymous $USER --port 3000 --db-user $USER WARNING, running in insecure mode, auth will be in plaintext WARNING, running in insecure mode, JWT secret is the default value Listening on port 3000 现在看看结果\n$ curl -s http://localhost:3000/ | python -m json.tool [ { \u0026quot;insertable\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;director\u0026quot;, \u0026quot;schema\u0026quot;: \u0026quot;1\u0026quot; }, { \u0026quot;insertable\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;festival\u0026quot;, \u0026quot;schema\u0026quot;: \u0026quot;1\u0026quot; }, { \u0026quot;insertable\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;film\u0026quot;, \u0026quot;schema\u0026quot;: \u0026quot;1\u0026quot; } ] 我们想要查看评分超过 8 的电影\n$ curl -s \u0026quot;http://localhost:3000/festival?year=gte.2014-01-01\u0026amp;rating=gte.8\u0026quot; | python -m json.tool [ { \u0026quot;competition\u0026quot;: \u0026quot;Palme d'Or\u0026quot;, \u0026quot;director\u0026quot;: \u0026quot;Xavier Dolan\u0026quot;, \u0026quot;festival\u0026quot;: \u0026quot;Cannes Film Festival\u0026quot;, \u0026quot;rating\u0026quot;: 8.3, \u0026quot;title\u0026quot;: \u0026quot;Mommy\u0026quot;, \u0026quot;year\u0026quot;: \u0026quot;2014-01-01\u0026quot; }, { \u0026quot;competition\u0026quot;: \u0026quot;Palme d'Or\u0026quot;, \u0026quot;director\u0026quot;: \u0026quot;Nuri Bilge Ceylan\u0026quot;, \u0026quot;festival\u0026quot;: \u0026quot;Cannes Film Festival\u0026quot;, \u0026quot;rating\u0026quot;: 8.5, \u0026quot;title\u0026quot;: \u0026quot;Winter Sleep\u0026quot;, \u0026quot;year\u0026quot;: \u0026quot;2014-01-01\u0026quot; }, { \u0026quot;competition\u0026quot;: \u0026quot;Golden Lion\u0026quot;, \u0026quot;director\u0026quot;: \u0026quot;Joshua Oppenheimer\u0026quot;, \u0026quot;festival\u0026quot;: \u0026quot;Venice Film Festival\u0026quot;, \u0026quot;rating\u0026quot;: 8.3, \u0026quot;title\u0026quot;: \u0026quot;The Look of Silence\u0026quot;, \u0026quot;year\u0026quot;: \u0026quot;2014-01-01\u0026quot; }, { \u0026quot;competition\u0026quot;: \u0026quot;Golden Lion\u0026quot;, \u0026quot;director\u0026quot;: \u0026quot;Alejandro Gonz\\u00e1lez I\\u00f1\\u00e1rritu\u0026quot;, \u0026quot;festival\u0026quot;: \u0026quot;Venice Film Festival\u0026quot;, \u0026quot;rating\u0026quot;: 8, \u0026quot;title\u0026quot;: \u0026quot;Birdman: Or (The Unexpected Virtue of Ignorance)\u0026quot;, \u0026quot;year\u0026quot;: \u0026quot;2014-01-01\u0026quot; } ] 太棒了，一些加到我们查看列表。我是日本电影超级粉丝，因此看看去年提名的日本电影。\n$ curl -s \u0026quot;http://localhost:3000/film?year=gte.2014-01-01\u0026amp;language=eq.Japanese\u0026quot; | python -m json.tool [ { \u0026quot;competition\u0026quot;: \u0026quot;Golden Lion\u0026quot;, \u0026quot;director\u0026quot;: \u0026quot;Shin'ya Tsukamoto\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;Japanese\u0026quot;, \u0026quot;rating\u0026quot;: 5.8, \u0026quot;title\u0026quot;: \u0026quot;Fires on the Plain\u0026quot;, \u0026quot;year\u0026quot;: \u0026quot;2014-01-01\u0026quot; }, { \u0026quot;competition\u0026quot;: \u0026quot;Palme d'Or\u0026quot;, \u0026quot;director\u0026quot;: \u0026quot;Naomi Kawase\u0026quot;, \u0026quot;language\u0026quot;: \u0026quot;Japanese\u0026quot;, \u0026quot;rating\u0026quot;: 6.9, \u0026quot;title\u0026quot;: \u0026quot;Still the Water\u0026quot;, \u0026quot;year\u0026quot;: \u0026quot;2014-01-01\u0026quot; } ] 结论 # 我希望我已经展示了 PostgreSQL 如何结合 PostGrest 快速把数据暴露给其他应用或者 web 前端。\n我对这个项目寄以厚望，你现在有少了必须使用 NoSQL 数据存储你的数据的原因（因为自由的 REST API），你也没有理由放弃关系数据库的好处。\n","date":"2015-08-13","permalink":"/n3xtchen/2015/08/13/postgresql---postgrest/","section":"时间线","summary":"译自 PostGrest Introduction","title":"PostgreSQL - PostGrest 简介"},{"content":"pigdb=# WITH T(CateID, ID) AS (SELECT 1,1 UNION ALL SELECT 1,1 UNION ALL SELECT 1,1 UNION ALL SELECT 1,2) SELECT *, RANK() OVER(PARTITION BY CateID ORDER BY ID) , ROW_NUMBER() OVER(PARTITION BY CateID ORDER BY ID), DENSE_RANK() OVER(PARTITION BY CateID ORDER BY ID) FROM T; cateid | id | rank | row_number | dense_rank --------+----+------+------------+------------ 1 | 1 | 1 | 1 | 1 1 | 1 | 1 | 2 | 1 1 | 1 | 1 | 3 | 1 1 | 2 | 4 | 4 | 2 (4 rows) ","date":"2015-08-13","permalink":"/n3xtchen/2015/08/13/postgresql---row_num-rank-dense_rank/","section":"时间线","summary":"pigdb=# WITH T(CateID, ID) AS (SELECT 1,1 UNION ALL SELECT 1,1 UNION ALL SELECT 1,1 UNION ALL SELECT 1,2) SELECT *, RANK() OVER(PARTITION BY CateID ORDER BY ID) , ROW_NUMBER() OVER(PARTITION BY CateID ORDER BY ID), DENSE_RANK() OVER(PARTITION BY CateID ORDER BY ID) FROM T; cateid | id | rank | row_number | dense_rank --------+----+------+------------+------------ 1 | 1 | 1 | 1 | 1 1 | 1 | 1 | 2 | 1 1 | 1 | 1 | 3 | 1 1 | 2 | 4 | 4 | 2 (4 rows) ","title":"Postgresql - ROW_NUM(), RANK() 和 DENSE_RANK() 的区别"},{"content":" 测试数据： # +------+------+------------------+ | cate | item | note | +------+------+------------------+ | a | 2 | a的第二个值 | | a | 1 | a的第一个值 | | a | 3 | a的第三个值 | | b | 1 | b的第一个值 | | b | 3 | b的第三个值 | | b | 2 | b的第二个值 | | b | 4 | b的第四个值 | | b | 5 | b的第五个值 | +------+------+------------------+ 建表和准备数据 # 都是通用类型，所以 pgsql 和 mysql 的建表和插表的语句都一样：\nCREATE TABLE tbl ( cate varchar(10), item int, note varchar(20)); INSERT INTO tbl VALUES ('a', 2, 'a的第二个值'), ('a', 1, 'a的第一个值'), ('a', 3, 'a的第三个值'), ('b', 1, 'b的第一个值'), ('b', 3, 'b的第三个值'), ('b', 2, 'b的第二个值'), ('b', 4, 'b的第四个值'), ('b', 5, 'b的第五个值'); MySQL 的实现方式 # 取每个 cate item 最大/最小的那条记录 # mysql\u0026gt; SELECT a.* FROM tbl a WHERE item = ( -\u0026gt; SELECT MAX(item) FROM tbl WHERE cate = a.cate -\u0026gt; ) ORDER BY a.cate; +------+------+------------------+ | cate | item | note | +------+------+------------------+ | a | 3 | a的第三个值 | | b | 5 | b的第五个值 | +------+------+------------------+ 2 rows in set (0.01 sec) 按 cate 分组取最大/最小的两个(N个)item # mysql\u0026gt; SELECT a.* FROM tbl a WHERE EXISTS ( -\u0026gt; SELECT COUNT(*) FROM tbl -\u0026gt; WHERE cate = a.cate AND item \u0026lt; a.item HAVING COUNT(*) \u0026lt; 2 -\u0026gt; ) ORDER BY a.cate, a.item; +------+------+------------------+ | cate | item | note | +------+------+------------------+ | a | 1 | a的第一个值 | | a | 2 | a的第二个值 | | b | 1 | b的第一个值 | | b | 2 | b的第二个值 | +------+------+------------------+ 4 rows in set (0.00 sec) PostgreSQL 的实现 # 取每个 cate item 最大/最小的那条记录 # # 那个分组的最最值的那条记录 pigdb=# SELECT DISTINCT ON (cate) cate, item, note FROM T ORDER BY cate, item; cate | item | note ------+------+------------- a | 1 | a的第一个值 b | 1 | b的第一个值 (2 rows) # 那个分组的最大值的那条记录 pigdb=# SELECT DISTINCT ON (cate) cate, item, note FROM T ORDER BY cate, item DESC; cate | item | note ------+------+------------- a | 3 | a的第三个值 b | 5 | b的第五个值 按 cate 分组取最大/最小的两个(N个)item # pigdb=# SELECT * FROM ( SELECT T.*, (NTH_VALUE(ITEM, 3) OVER ( PARTITION BY cate ORDER BY item) ) fv FROM T )T1 WHERE fv is null; cate | item | note | fv ------+------+-------------+---- a | 1 | a的第一个值 | a | 2 | a的第二个值 | b | 1 | b的第一个值 | b | 2 | b的第二个值 | 引用\n* mysql分组取每组前几条记录(排名) 附group by与order by的研究 * Postgresql 窗口函数 * How to select the first row in a group: SELECT DISTINCT ON in PostgreSQL\n","date":"2015-08-13","permalink":"/n3xtchen/2015/08/13/pgsql-vs-mysql---get-nth-value-per-group/","section":"时间线","summary":"测试数据： # +------+------+------------------+ | cate | item | note | +------+------+------------------+ | a | 2 | a的第二个值 | | a | 1 | a的第一个值 | | a | 3 | a的第三个值 | | b | 1 | b的第一个值 | | b | 3 | b的第三个值 | | b | 2 | b的第二个值 | | b | 4 | b的第四个值 | | b | 5 | b的第五个值 | +------+------+------------------+ 建表和准备数据 # 都是通用类型，所以 pgsql 和 mysql 的建表和插表的语句都一样：","title":"PostgreSQL vs MySQL：取分组数据的前N条纪录"},{"content":"","date":"2015-08-13","permalink":"/n3xtchen/tags/restful/","section":"标签","summary":"","title":"restful"},{"content":" 环境要求 # postgresql 9.4 mysql 5.5 python2.7 with pip sqitch(将在后续配置 SQL 时，使用它) git(可选，将在后续配置 SQL 时，使用它) PGXN 安装 # $ pip install pgxnclient PGXN 命令 # $ pgxn help usage: pgxn [--help] [--version] COMMAND ... Interact with the PostgreSQL Extension Network (PGXN). optional arguments: --help 打印帮助信息并推出 --version 打印版本并退出 available commands: COMMAND 查看具体命令的用法 `pgxn help --all`. 内置命令如下: check 检查包 download 下载包，供手动安装时使用 help 帮助 info 查询包信息 install 下载并安装 load 载入到特定数据库 mirror 返回可用的下载镜像 search 搜索 uninstall 卸载 unload 卸载扩展] 安装 mysql_fdw # 首先我们先查看下 mysql_fdw 的包信息\n$ pgxn info mysql_fdw name: mysql_fdw abstract: MySQL FDW for PostgreSQL 9.3+ description: This extension implements a Foreign Data Wrapper for MySQL. It is supported on PostgreSQL 9.3 and above. maintainer: mysql_fdw@enterprisedb.com license: postgresql release_status: stable version: 2.0.1 date: 2015-02-04T12:45:26Z sha1: dae449a1b017335cf9b19fc769a44589b26f5a59 provides: mysql_fdw: 2.0.1 runtime: requires: PostgreSQL 9.3.0 然后开始安装：\n$ pgxn install mysql_fdw 如果报错，尝试下面的命令：\n$ USE_PGXS=1 pgxn install mysql_fdw 启用 mysql_fdw 模块 # 首先，要载入模块，有两种方法：\n使用 SQL：\n$ CREATE EXTENSION mysql_fdw; 使用 PGXN：\n我们先看下 pgxn 载入模块命令的用法\n$ pgxn help load usage: pgxn load [--help] [--mirror URL] [--verbose] [--yes] [--stable | --testing | --unstable] [-d DBNAME] [-h HOST] [-p PORT] [-U NAME] [--pg_config PROG] [--schema SCHEMA] SPEC [EXT [EXT ...]] load a distribution's extensions into a database positional arguments: SPEC 制定要载入的扩展（或列表） EXT 只能指定要载入的扩展，默认是全部 optional arguments: --help 打印帮助信息并推出 --stable 只接受稳定的版本（默认） --testing 也接受测试中的版本 --unstable 也接受不稳定的版本 --pg_config PROG pg_config 可执行命令 --schema SCHEMA 使用 SCHEMA 代替默认 global options: --mirror URL 需要交互的镜像 [默认: http://api.pgxn.org/] --verbose 打印更多信息 --yes 确认所有命令提问 database connections options: -d DBNAME, --dbname 库名 -h HOST, --host HOST 库主机 -p PORT, --port PORT 库端口 -U NAME, --username NAME\t用户名 ... 此时省略 现在，我们使用 pgxn load 命令载入模块\n$ pgxn load -d DBNAME -h HOST -p PORT -U NAME SEPC mysql_fdw 创建 mysql 外部表 # （首先，让我们添加一个有用的 git 别名，我们将在后续中使用，可选）\n$ git config --global alias.add-commit '!git add -A \u0026amp;\u0026amp; git commit' 接下来，创建一个测试数据库：\n$ createdb fdw_test $ git init . Initialized empty Git repository in /Users/ichexw/Dev/pgsql/restapi/.git/ $ sqitch init fwd_test Created sqitch.conf Created sqitch.plan Created deploy/ Created revert/ Created verify/ $ sqitch config core.engine pg $ sqitch target add production db:pg://localhost:5432/fdw_test $ sqitch engine add pg $ sqitch engine set-target pg production $ sqitch add appschmea -n 'Add schema for fdw_test object' Created deploy/appschmea.sql Created revert/appschmea.sql Created verify/appschmea.sql deploy/appschmea.sql\nBEGIN; CREATE EXTENSION mysql_fdw; create schema fwd_test; COMMIT; revert/appschmea.sql\nBEGIN; DROP EXTENSION mysql_fdw; drop schema fwd_test; COMMIT; 部署你的变更\nAdding registry tables to production Deploying changes to production + appschmea .. ok 如果部署成功，把变更提交到版本控制中\n$git add-commit -m \u0026quot;Adding fdw_test schema\u0026quot; [master (root-commit) be4eec4] Adding fdw_test schema 5 files changed, 43 insertions(+) create mode 100644 deploy/appschmea.sql create mode 100644 revert/appschmea.sql create mode 100644 sqitch.conf create mode 100644 sqitch.plan create mode 100644 verify/appschmea.sql 现在重复上面的步骤，把我们的 mysql 关联到当前数据库\n$ sqitch add mysql_server -n \u0026quot;Add the mysql fdw\u0026quot; Created deploy/mysql_server.sql Created revert/mysql_server.sql Created verify/mysql_server.sql Added \u0026quot;mysql_server\u0026quot; to sqitch.plan deploy/mysql_server.sql\nBEGIN; -- 创建服务对象，即 mysql 服务 CREATE SERVER mysql_server FOREIGN DATA WRAPPER mysql_fdw OPTIONS (host '127.0.0.1', port '3306'); -- 创建映射的用户，即 mysql 的用户 CREATE USER MAPPING FOR postgres SERVER mysql_server OPTIONS (username 'root', password ''); -- 创建外部封装数据，即要映射的 mysql 表 CREATE FOREIGN TABLE pg_fdw( id int, name text, created timestamp ) SERVER mysql_server OPTIONS (dbname 'test', table_name 'pg_fdw'); COMMIT; revert/mysql_server.sql\n-- Revert fwd_test:mysql_server from pg BEGIN; -- 注意注意先后顺序 DROP FOREIGN TABLE pg_fdw; DROP USER MAPPING FOR ichexw SERVER mysql_server; DROP SERVER mysql_server; COMMIT; 使用 mysql 外部表 # 在 mysql 服务器中操作 # mysql\u0026gt; INSERT INTO pg_fdw values (1, 'UPS', sysdate()); Query OK, 1 row affected (0.01 sec) mysql\u0026gt; INSERT INTO pg_fdw values (2, 'TV', sysdate()); Query OK, 1 row affected (0.01 sec) mysql\u0026gt; INSERT INTO pg_fdw values (3, 'Table', sysdate()); Query OK, 1 row affected (0.01 sec) mysql\u0026gt; select * from pg_fdw; +------+-------+---------------------+ | id | name | created | +------+-------+---------------------+ | 1 | UPS | 2015-08-14 12:22:23 | | 2 | TV | 2015-08-14 12:22:49 | | 3 | Table | 2015-08-14 12:23:04 | +------+-------+---------------------+ 3 rows in set (0.00 sec) 在 pgsql 中可以进行同样的操作 # fdw_test=# select * from pg_fdw fdw_test-# ; id | name | created ----+-------+--------------------- 1 | UPS | 2015-08-14 12:22:23 2 | TV | 2015-08-14 12:22:49 3 | Table | 2015-08-14 12:23:04 (3 rows) fdw_test=# DELETE FROM pg_fdw where id = 3; DELETE 1 fdw_test=# UPDATE pg_fdw set name = 'UPS_NEW' where id = 1; UPDATE 1\t我门看看 pgsql 是如何操作 mysql 表的 # fdw_test=# EXPLAIN ANALYZE VERBOSE SELECT id, name FROM pg_fdw WHERE name LIKE 'Table' limit 1; QUERY PLAN -------------------------------------------------------------------------------------------------------------------- Limit (cost=10.00..11.00 rows=1 width=36) (actual time=0.653..0.653 rows=1 loops=1) Output: id, name -\u0026gt; Foreign Scan on public.pg_fdw (cost=10.00..13.00 rows=3 width=36) (actual time=0.652..0.652 rows=1 loops=1) Output: id, name Local server startup cost: 10 Remote query: SELECT `id`, `name` FROM `test`.`pg_fdw` WHERE ((`name` like 'Table')) Planning time: 0.589 ms Execution time: 10.999 ms (8 rows) 结语 # 哈哈，是不是很方便，PostgreSQL 的 FDW 不仅仅支持 MySQL，还支持 Oracle，MSSQL 等等所有主流的数据库，这无形中给数据迁移，带来的极大方便，使用同一个数据库来操作不同的数据，也对多数据库应用带来了很大的便利，是不是很吸引人，快到碗里来吧！^_^\n","date":"2015-06-17","permalink":"/n3xtchen/postgresql/2015/06/17/postgresql-use-pgxn-install-mysql-fdw/","section":"时间线","summary":"环境要求 # postgresql 9.","title":"PostgreSQL - 使用 PGXN 安装 Mysql-fdw"},{"content":"","date":"2015-06-16","permalink":"/n3xtchen/tags/api/","section":"标签","summary":"","title":"api"},{"content":" 原文引用： Simplify: move code into database functions\n如果你是一个网页或者接口开发者，并且使用数据库，那这边适合你。\n我发现了一种不同寻常和有用的方式构建代码。这对我来说是非常的与众不同，因此我必须分享它。\n事情是怎样的呢？ # 大部分 Web 开发者－无论是依赖还是不依赖框架（如 Rails，Django，Laravel，Sinatra，Flask 或者是 Symfony）－都适用同样的模式工作：\n核心的数据库只是作为数据的存储 所有逻辑都在 Ruby/Python/PHP/Javascript 类中。 为什么这样不好呢？？ # 这样可能会导致一些潜在的问题：\n所有东西都必须通过 Ruby/Python/PHP/Javascript 类来实现－也包括 shell 脚本以及其他不属于网站的部分； 所有东西都不能直接访问数据；为了这么做，你必须要把逻辑定义在其他语言中； 数据被当作愚蠢的存储器，即使数据库足够智能，可以完成大部分的逻辑； 如果你增加业务逻辑到数据库中，那你需要同时变更代码和数据结构；如果它的规则也变化，那就需要修改更多的地方； 两个系统－数据和围绕它的代码－相互捆绑又彼此独立； 如果需要使用其他语言（比如从 Ruby 到 Javascript，或者 Python 到 Elixir），你就必须要重写所有的东西。 简单 VS 复杂 # 你应该去听听 Rich Hichkey 35 分钟令人惊叹的演讲 Simplicity Matters by Rich Hickey。\n对于这篇文章，与他的观点不谋而合：\n复杂 是客观存在。它意味着很多东西都捆绑在一起； 简单 也是客观存在的。它可以是一种原材料，复杂的对立面； 它门都和 简单 无关。你可以很容易的安装和自己绑定在一些非常复杂的东西上（如 ORM），有时候创建一个简单的东西却很困难； 类，模型以及方法（OOP）是一个不必要并发症； 信息是简单的，但是不要把它们隐藏在一个宏语言后面； 直接用数值工作：哈希数组； 如果一个 JSON 接口－一个哈希数组－作为最终接口，那就更有理由跳过抽象成，直接使用数值工作 为什么这个很触动我 # 我从 1997 年开始，一直都使用相同的数据库：同一个数据，值和 SQL 表。但是代码却标更了很多次。\n在 1997 年，我使用 Perl。1998 年，我转到 PHP。2004 年，我使用 Rails 重写。2007 年，我又转回道 PHP。在 2009 年，使用极简的 Ruby。2012 年，使用客户端的 Javascript。\n每一次，我都要围绕数据库重写所有的逻辑：如何添加一个用户信息到数据库，如果验证支票的有效性，以及如何标识一个订单是已付费的等等。\n但是 在这整个过程，我信任的 PostgreSQL 数据库是唯一不变的。\n因为大部分都是 数据逻辑，而不是业务逻辑，所以它应该在数据库中。\n因此，我把我的数据逻辑直接存储在 PostgreSQL 中，因为我还计划使用它很多年，但是持续计划使用编程做实验（TO－DO：Haskell，Elixir，Racket，Lua）。\n那应该是怎样的呢？ # Web 开发者已经把数据库作为愚蠢的存储，但是实际上它已经很智能了。\n在数据库中就能简单德实现所有的逻辑。\n但是把他捆绑在外部的代码就会变的很复杂。\n一旦你把所有的逻辑都放到数据库中，那外部的代码就消失了！\n然后数据库是自包含的，不用绑定任何东西。\n你的外部接口可以很容易转到 Javascript，Haskell，Elixir 或者其他任何东西，因为你的核心逻辑都都在数据库中。\n那怎么做呢？ # 表约束 # 最简单的就是从约束（Constraints）开始：\nCREATE TABLE people ( id serial primary key, name text NOT NULL CONSTRAINT no_name CHECK (LENGTH(name) \u0026gt; 0), email text UNIQUE CONSTRAINT valid_email CHECK (email ~ '\\A\\S+@\\S+\\Z') ); CREATE TABLE tags ( person_id integer NOT NULL REDERENCES people(id) ON DELETE CASCADE, tag varchar(16) CONSTRAINT tag_format CHECK (statkey ~ '\\A[a-z0-9._-]+\\Z') ); 在这里定义了数据的有效性验证。\npeople 这张表中，name 不能为空，email 必须比符合规范（包含 @ 和 . ，不能包含空格）。tags.person_id 必须存在 people 中，但是如果 people 中删除了，tags 表中对应的也会删除；另外 tag 必须要符合规范，只能是小写字母，数字，点，下划线或者破折号。\n触发器（Triggers） # 如果一些操作必须在你修改数据的之前或者之后触发，那就要使用 trigger：\nCREATE FUNCTION clean() RETURNS TRIGGER AS $$ BEGIN NEW.name = btrim(regexp_replace(NEW.name, '\\s+', ' ', 'g')); NEW.email = lower(regexp_replace(NEW.email, '\\s', '', 'g')); END; $$ LANGUAGE plpgsql; CREATE TRIGGER clean BEFORE INSERT OR UPDATE OF name, email ON people FOR EACH ROW EXECUTE PROCEDURE clean(); 这个例子在我们添加数据到数据库之前格式化输入，以防有些人不小心把空格输到邮箱中，或者换行符到他的名字总。\n函数（Functions） # 编写一些可复用的函数，来代替反复使用的代码。\nCREATE FUNCTION get_person(a_name text, a_email text) RETURNS SETOF people AS $$ BEGIN IF NOT EXISTS (SELECT 1 FROM people WHERE email = a_email) THEN RETURN QUERY INSERT INTO people (name, email) VALUES (a_name, a_email) RETURNING people.*; ELSE RETURN QUERY SELECT * FROM people WHERE email = a_email; END IF; END; $$ LANGUAGE plpgsql; 这是我经常食用的：给定某人的用户名和邮箱，如果他不在我们的数据库中，添加他。然后，从数据库中返回这个用户的信息。\n用于 JSON 的视图 # 替代外部代码把你的数据转化成 JSON，你可以 直接在数据库中创建 JSON。\n在这里，使用 视图来作为 JSON 的结构模版。在视图中，使用 json_agg 来嵌入值。\nCREATE VIEW person_view AS SELECT *, (SELECT json_agg(t) AS tags FROM (SELECT tag FROM tags WHERE person_id=people.id) t) FROM people; 这个将在下面 API 的函数中用到。\nAPI 函数 # 外部代码只能通过这些函数访问数据库。\n他们都只返回 JSON。\nCREATE FUNCTION update_password(p_id integer, nu_pass text, OUT js json) AS $$ BEGIN UPDATE people SET password=crypt(nu_pass, gen_salt('bf', 8)) WHERE id = p_id; js := row_to_json(r) FROM (SELECT * FROM person_view WHERE id = p_id) r; END; $$ LANGUAGE plpgsql; CREATE FUNCTION people_with_tag(a_tag text, OUT js json) AS $$ BEGIN js := json_agg(r) FROM (SELECT * FROM person_view WHERE id IN (SELECT person_id FROM tags WHERE tag = a_tag)) r; END; $$ LANGUAGE plpgsql; 任何和数据相关的操作，你都可以使用 PostgreSQL 内置的 存储过程语言。\nPL/pgSQL 不是一个最简洁的语言，但是为了简化数据库操作，还是值得的。\n如果你喜欢 Javascript，你可能需要 PLv8，这有个 关于它好的分享。\n现在，如果你需要一个 REST API： # require 'pg' require 'sinatra' DB = PG::Connection.new(dbconfig) def qry(sql, params=[]) @res = DB.exec_params('SELECT js FROM ' + sql, params) end after do content_type 'application/json' body @res[0]['js'] end get '/people' do qry('get_people()') end get %r{/people/([0-9]+)} do |id| qry('get_person($1)', [id]) end put %r{/people/([0-9]+)} do |id| qry('update_password($1, $2)', [id, params[:password]]) end get '/people/tagged' do qry('people_with_tag($1)', [params[:tag]]) end 或者你需要一个客户端库： # require 'pg' require 'json' DB = PG::Connection.new(dbconfig) def js(func, params=[]) res = DB.exec_params('SELECT js FROM ' + func, params) JSON.parse(res[0]['js']) end def people js('get_people()') end def person(id) js('get_person($1)', [id]) end def update_password(id, newpass) js('update_password($1, $2)', [id, newpass]) end def people_tagged(tag) js('people_with_tag($1)', [tag]) end 就到这里！ # 无论是 REST API 还是客户端库，你所需要做的就是传递参数给数据库函数，返回 JSON。\n我并不打算说服所有人都这么做。但是我只希望它能对你有用处，或者至少听起来有趣。\n","date":"2015-06-16","permalink":"/n3xtchen/2015/06/16/pgsql-move-code-into-database-functions/","section":"时间线","summary":"原文引用： Simplify: move code into database functions","title":"简化：将代码移到数据库函数中"},{"content":"思考一下这个程序的输出结果\n\u0026lt;?php $languages = array('php','python','scala'); foreach ($languages as \u0026amp;$lang) $lang = strtoupper($lang); foreach ($languages as $lang) { // notice NO reference here! echo $lang.\u0026quot;\\n\u0026quot;; } ?\u0026gt; 答案是：\nPHP PYTHON PYTHON 哈哈，不错的一道面试题目！这里很明显，我想讲的就是副作用。\n作为 FP 的鼓吹者，我今天从黑 foreach 开始讲起（Sorry，foreach ^_^）。\n\u0026lt;?php $languages = array('php','python','scala'); array_map(function (\u0026amp;$lang) { $lang = strtoupper($lang); }, $languages); foreach ($languages as $lang) { echo $lang.\u0026quot;\\n\u0026quot;; } ?\u0026gt; 使用 map 的结果是才是你预期的，因为 array_map 遵循函数式编程的特性，无副作用！如果想知道map 算法的实现，参考 使用 python - 实现 Map，Filter 以及 Reduce，其中揽括了 MapReduce 和 Filter 的递归实现！\n所有的循环都可以通过递归来实现。\n函数式编程特点－没有\u0026quot;副作用\u0026quot; # 所谓\u0026quot;副作用\u0026quot;（side effect），指的是函数内部与外部互动（最典型的情况，就是修改全局变量的值），产生运算以外的其他结果。函数式编程强调没有\u0026quot;副作用\u0026quot;，意味着函数要保持独立，所有功能就是返回一个新的值，没有其他行为，尤其是不得修改外部变量的值。\n上面的那个例子是违反函数编程特性，虽然避免的副作用，但是它修改了变量的状态，应该这样：\n\u0026lt;?php $languages = array_map(function ($lang) { $lang = strtoupper($lang); }, array('php','python','scala')); foreach ($languages as $lang) { echo $lang.\u0026quot;\\n\u0026quot;; } ?\u0026gt; 函数式编程特点－不修改状态 # 上一点已经提到，函数式编程只是返回新的值，不修改系统变量。因此，不修改变量，也是它的一个重要特点。 在其他类型的语言中，变量往往用来保存\u0026quot;状态\u0026quot;（state）。不修改变量，意味着状态不能保存在变量中。函数式编程使用参数保存状态，最好的例子就是递归。下面的代码是一个将字符串逆序排列的函数，它演示了不同的参数如何决定了运算所处的\u0026quot;状态\u0026quot;。\n看到第二个例子中的 array_map 的第一个参数，他就是传说中的匿名函数，顾名思义，就是声明一个没有名称的函数，可能一些同学会比较陌生。我们稍微介绍下：\n匿名函数（Lambda） # 在计算机编程中，匿名函数（英语：anonymous function）是指一类无需定义标识符（函数名）的函数或子程序，普遍存在于多种编程语言中。（来源于 Wikipedia）\n先从简单的开始，先来看看匿名函数。\n来看个简单的例子：\n\u0026lt;?php # hi_lambda.php $hi_lambda = function ($name) { echo \u0026quot;Hello, $name!\u0026quot;; }; $hi_lambda('n3xtchen');\t# 打印 Hello, n3xtchen! ?\u0026gt; 不过和其他语言的匿名函数相比，可能就会觉得 PHP 太丑了，下面 javascript 的实现\n(function(name){ console.log('Hello,'+name+'!'); })('n3xtchen'); // $ node hi_lambda.js // Hello,n3xtchen! 现在，看一下匿名函数在类中的使用：\n\u0026lt;?php class LambdaClass { public $value = '2015'; public function foo() { echo \u0026quot;foo is called\\n\u0026quot;; } public function run() { $access_attr = function () { ＃ 类中的匿名函数可以访问类本身 return $this-\u0026gt;value; }; $res = $access_attr(); echo \u0026quot;$res\\n\u0026quot;; $this-\u0026gt;foo(); } } $test = new LambdaClass(); $test-\u0026gt;run(); # 打印: # 2015 # foo is called\t?\u0026gt; 注意：\n在类内定义的匿名函数式可以用 $this 来访问类的成员变量和方法；但是只有 PHP5.4 以后的版本才支持。\n这样子真的方便了很多，我们公司现在使用的 5.3，想在匿名函数中调用本类，还用使用 use，可蛋疼了。\n讲完了匿名函数就不得不讲讲闭包了\n闭包（Closure） # 在计算机科学中，闭包（Closure）是词法闭包（Lexical Closure）的简称，是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，即使已经离开了创造它的环境也不例外。所以，有另一种说法认为闭包是由函数和与其相关的引用环境组合而成的实体。闭包在运行时可以有多个实例，不同的引用环境和相同的函数组合可以产生不同的实例。(来自维基)\n先来看一个例子\n\u0026lt;?php $bind = 3; $closure = function ($arg) use ($bind) { return $arg + $bind; }; var_dump($closure(4)); ?\u0026gt; 如上面的例子，我们用关键字 use 来捆绑变量。PHP 中的捆绑默认是前期绑定(early binding)。这意味着匿名函数接受到的值是函数定义时该变量的值。我们也可以用引用来传递变量，并以此来实现后期绑定(late binding)。看看下面的例子:\n\u0026lt;?php $time = \u0026quot;morning!\\n\u0026quot;; $late_binding = function() use (\u0026amp;$time) { echo \u0026quot;good $time\u0026quot;; }; $func(); $time = \u0026quot;afternoon!\\n\u0026quot;; $func(); ?\u0026gt; PHP.NET 中匿名函数定义 # 中文：匿名函数（Anonymous functions），也叫闭包函数（Closures），允许 临时创建一个没有指定名称的函数。最经常用作回调函数（callback）参数的值。当然，也有其它应用的情况。\n之所以引用维基，是因为本人认为 PHP 官方的描述是有误，匿名函数和闭包属于不同概念，而维基说的描述更为准确，闭包是由函数和与其相关的引用环境组合而成的实体。\n函数式编程特点－函数是\u0026quot;第一等公民\u0026quot; # 所谓\u0026quot;第一等公民\u0026quot;（first class），指的是函数与其他数据类型一样，处于平等地位，可以赋值给其他变量，也可以作为参数，传入另一个函数，或者作为别的函数的返回值。\n函数式编程特点－高阶函数（Higher Order Function） # 高阶函数：所谓高阶函数就是函数当参数，把传入的函数做一个封装，然后返回这个封装函数。现象上就是函数传进传出，就像面向对象编程，对象满天飞一样。\n在函数中把 匿名函数返回，并且调用它:\n\u0026lt;?php function return_func() { return function () { echo \u0026quot;返回一个函数！\u0026quot;; }; } $get_return_func = return_func(); $get_return_func(); ?\u0026gt; 把匿名函数 当做参数传递，并且调用它:\nfunction callFunc( $func ) { $func('some string'); } $printStrFunc = function ($str) { echo $str; }; $printStrFunc(); 函数式编程特点－偏函数应用 # 偏函数应用 指的是固化函数的一个或一些参数，从而产生一个新的函数。\n\u0026lt;?php function log($level, $message) { echo \u0026quot;$level : $message\u0026quot;; } log(\u0026quot;Warning\u0026quot;, \u0026quot;this is one warning message\u0026quot;); log(\u0026quot;Error\u0026quot;, \u0026quot;this is one error message\u0026quot;); ?\u0026gt; 使用和匿名函数和偏函数应用改写下：\n\u0026lt;?php $logWarning = function ($message) { log(\u0026quot;Warning\u0026quot;, $message); }; $logError = function ($message) { log(\u0026quot;Error\u0026quot;, $message); } $logWarning(\u0026quot;this is one warning message\u0026quot;); $logError(\u0026quot;this is one error message\u0026quot;); ?\u0026gt; 虽然现在粗看，没有任何好处，反而代码更多了。这里说说偏函数应用的两个好处：\n当你多次调用一个函数的时候，发现很多参数都是固定的，那你可以考虑使用偏函数； 你会不会发现固定参数之后，给新函数一个更可读的名称，让你更容易维护他； 现在看一个例子：\n函数式编程特点－函数加里化 # 函数加里化（Currying） 指的是将一个具有多个参数的函数，转换成能够通过一系列的函数链式调用，其中每一个函数都只有一个参数。\nfunction sum3($x, $y, $z) { return $x + $y + $z; } function curried_sum3($x) { return function ($y) use ($x) { return function ($z) use ($x, $y) { return sum3 ($x, $y, $z); }; }; } $f1 = curried_sum3(1); $f2 = $f1(2); $result = $f2(3); 偏函数和Currying有什么用？主要就是从能一个通用函数得到更特定的函数。有一些编程经验的，一定都手工写过偏函数应用吧。 Currying提供了另外一种实现方式。这种方式在函数式编程中更常见。函数式编程思想，不仅在Lisp这样的函数式编程语言中，在更多的语言中也得到了实现和发展，像Python，Javascript乃至C#这样的命令式语言(imperative language)。所以有机会不妨考虑下使用Currying，能否更好地解决问题。\n递归和尾递归 # 在计算机科学里，尾调用是指一个函数里的最后一个动作是一个函数调用的情形：即这个调用的返回值直接被当前函数返回的情形。这种情形下称该调用位置为尾位置。若这个函数在尾位置调用本身（或是一个尾调用本身的其他函数等等），则称这种情况为尾递归，是递归的一种特殊情形。尾调用不一定是递归调用，但是尾递归特别有用，也比较容易实现。\nfunction factorial($n) { if($n == 0) { return 1; } return factorial($n-1) * $n; } var_dump(factorial(100)); 即便代码能正常运行，只要我们不断增大参数，程序迟早会报错：\nFatal error: Allowed memory size of … bytes exhausted 为什么呢？简单点说就是递归造成了栈溢出。按照之前的思路，我们可以试下用尾递归来消除递归对栈的影响，提高程序的效率。\nfunction factorial($n, $acc) { if($n == 0) { return $acc; } return factorial($n-1, $acc * $n); } var_dump(factorial(100, 1)); XDebug同样报错，并且程序的执行时间并没有明显变化。\nFatal error: Maximum function nesting level of '100' reached, aborting! PHP如何消除递归\nfunction factorial($n, $accumulator = 1) { if ($n == 0) { return $accumulator; } return function() use($n, $accumulator) { return factorial($n - 1, $accumulator * $n); }; } function trampoline($callback, $params) { $result = call_user_func_array($callback, $params); while (is_callable($result)) { $result = $result(); } return $result; } var_dump(trampoline('factorial', array(100))); 现在XDebug不再警报效率问题了。\n注意到trampoline()函数没？简单点说就是利用高阶函数消除递归。\n还有很多别的方法可以用来规避递归引起的栈溢出问题，比如说Python中可以通过装饰器和异常来消灭尾调用，让人有一种别有洞天的感觉。\n参考：\nPHP中的函数式编程 DrupalCon Review: Functional Programming 阮一峰的网络日志-函数式编程初 Functional Progamming With Python CoolShell-函数式编程 匿名函数和闭包 深入理解PHP之匿名函数 PHP闭包（Closure）初探 谈PHP 闭包特性在实际应用中的问题(GOOD) 偏函数应用和函数加里化 Currying vs. Partial Application(GOOD) Currying in PHP(GOOD) Request for Comments: Currying matteosister/php-curry 偏函数应用(Partial Application）和函数柯里化(Currying) 递归和尾递归 漫谈递归：PHP里的尾递归及其优化(GOOD) 尾调用 说说尾递归 ","date":"2015-04-26","permalink":"/n3xtchen/2015/04/26/php---functional-programming/","section":"时间线","summary":"思考一下这个程序的输出结果","title":"PHP：浅谈函数式编程（一）"},{"content":"这张帖子讨论 Python 的下划线的使用，讲解下它的大部分使用场景。\n当个下划线（_） # 有三个典型的场景：\n在解释器中：_ 指向交互解释器绘画的最后一次执行语句的结果。最早是 CPython 的标准，后续其他的也跟进了。\n\u0026gt;\u0026gt;\u0026gt; _ Traceback (most recent call last): File \u0026quot;\u0026lt;stdin\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; NameError: name '_' is not defined \u0026gt;\u0026gt;\u0026gt; 42 \u0026gt;\u0026gt;\u0026gt; _ 42 \u0026gt;\u0026gt;\u0026gt; 'alright!' if _ else ':(' 'alright!' \u0026gt;\u0026gt;\u0026gt; _ 'alright!' 作为一个名称：跟前一个例子优点相关。_ 作为一个占位符。它允许下一个人读你代码的时候知道，按惯例，它只是被分配，但是不使用它。例如，你也许不在乎循环计数器的实际值：\nn = 42 for _ in range(n): do_something() i18n: _ 也可以作为一个函数来使用。这个场景下，它经常作为多国语言和本地语言的翻译查询。这个实际上源于 C 语言的惯例。例如，你看到 Django 中的翻译：\nfrom django.utils.translation import ugettext as _ from django.http import HttpResponse def my_view(request): output = _(\u0026quot;Welcome to my site.\u0026quot;) return HttpResponse(output) 在名称之前的单下划线（例如 _varname） # 一个名称前置下划线用来指明它被程序员私有化。它是一种惯例，下一个人使用（或者你自己）的时候，知道前置下划线的名称只供内部使用。看看，Python 文档中是怎么定义的：\n一个名称前置了下划线（如 _spam）应该被当作 **API** 中非公共的部分（无论他是函数，方法或者属性）。它都应该考虑一个具体的实现来改变。 参考: (Underscores in Python)[http://shahriar.svbtle.com/underscores-in-python]\n","date":"2015-04-22","permalink":"/n3xtchen/2015/04/22/python---underscore/","section":"时间线","summary":"这张帖子讨论 Python 的下划线的使用，讲解下它的大部分使用场景。","title":"Python - 下划线（_）"},{"content":"","date":"2015-04-20","permalink":"/n3xtchen/tags/functional/","section":"标签","summary":"","title":"functional"},{"content":"偏函数应用（Partial Function Application）听起来名字就很吸引人；它的作用是在函数调用前，预先固定参数的方法。它的机制有点粗糙，学术的解释有点古板，但是它很有用。如果你的函数需要 x 和 y 两个参数，实现把 x 参数固定了，后续调用只需要传入 y 即可，来看一个例子：\n# -*- coding: utf-8 -*- import functools def adder(x, y): return x + y # 执行它 assert adder(1, 1) == 2 assert adder(5, 5) == 10 assert adder(6, 2) == 8 # 把 y 的参数固定 add_five = functools.partial(adder, y=5) # 现在 加上 5 # x =1, y =5 assert add_five(1) == 6 # x =5, y =5 assert add_five(5) == 10 # x =2, y =5 assert add_five(2) == 7 坦白说，这个例子并没什么用处，但也不是一无是处。至少它很好展示了偏函数应用的用法。但是并没有告诉你为什么要用它，或者哪里能使用它。当你开始使用函数开始编写程序的时候，你很难从这个例子来想象他的使用场景。\n这就是我为什么写这篇博客原因。我想要尝试不一样的方式。因为其他人已经把偏函数应用的实现机制都写烂了，我将跳过这个部分，旨在阐述它的应用场景。我们通过案例学习；每个案例的讲解都会通过逐步重构来有效应用偏函数。\n案例1 - 重构特定领域表达式 # 我经常使用编写任务匹配的应用；如追踪 URL，查找制定循环，在日志文件中寻找模式。这类的任务，会慢慢变的臃肿，最终会失去控制。\n导致这个问题的罪魁祸首就是正则表达式。没有任何的警惕，给予足够的时间，和不断增多的 “呃，你还能找到。。。” 方式请求，它很快就让你陷入不可维护代码的深坑。比如：\nfor text in lines: if re.search(‘[a-zA-Z]\\=’, text): some_action(text) elif re.search(‘[a-zA-Z]\\s\\=’, text): some_other_action(text) else: some_default_action() 当我们需要快速实现的时候，这样看起来没什么问题。然后，几周之后再来看这段代码，我已经看不懂这代码中正则表达式的含义了。因此，是时候开始重构了。第一步就是拨出不合理的部分，利用良好命名的函数来替换我们的正则表达式：\ndef is_grouped_together(text): return re.search(\u0026quot;[a-zA-Z]\\s\\=\u0026quot;, text) def is_spaced_apart(text): return re.search(“[a-zA-Z]\\s\\=”, text) def and_so_on(text): return re.search(“pattern_188364625\u0026quot;, text) for text in lines: if is_grouped_together(text): some_action(text) elif is_spaced_apart(text): some_other_action(text) else: some_default_action() 这个看起来好多了，对于我来说。实际上，在每个模块里只做了一件事情。然而，这个实际包含了无数的搜索路由；因此，在完成这次重构的之前，需要对函数进行微妙的调整。我们重构他的时候只有一到两个帮助函数，一旦你拥有一堆的函数，那看起来就没现在这么好了。\n这个问题的核心是所有的这些小函数只是为我的正则表达式提供一个可读的名称而已，但是目前的实现使得当前模块变的杂乱。实际的工作都在 re.search 上。我的所想要的是基于领域的版本。幸运的是，偏函数应用帮我做到这个：\ndef my_search_method(): is_spaced_apart = partial(re.search, '[a-zA-Z]\\s\\=') is_grouped_together = partial(re.search, '[a-zA-Z]\\=') ... for text in lines: if is_grouped_together(text): some_action(text) elif is_spaced_apart(text): some_other_action(text) else: some_default_action() 现在我们得到一个很好的描述和可读性的代码。我们使用偏函数应用的预填充来处理 re.search，使我们获得了一个针对我们领域具有描述性的控制结构，由于他们的定义很密集，这两个方法都是两个查询函数，我把它们封装到一个函数体内，避免污染当前模块的命名空间。\n案例2 - 使用偏函数构建伪对象继承 # 关于偏函数应用简洁代码的最好方法之一，就是创建一个简单的伪对象继承结构，但是不附带明确的子类模版。当你有一个对象，需要通过不同的参数来定制它的行为，那偏函数应用就变得很有用。\n先看一段丑陋的代码：\ndef do_complicated_thing(request, slug): if not request.is_ajax() or not request.method == 'POST': return HttpResponse(json.dumps({'error': 'Invalid Request'}, content_type=\u0026quot;application/json\u0026quot;, status=400) if not _has_required_params(request): return HttpResponse( json.dumps({'error': 'Missing required param(s): {0}'.format(_get_missing(request)), content_type=\u0026quot;application/json\u0026quot;, status=400) ) try: _object = Object.objects.get(slug=slug) except Object.DoesNotExist: return HttpResponse(json.dumps({'error': 'No Object matching x found'}, content_type=\u0026quot;application/json\u0026quot;, status=400) else: result = do_a_bunch_of_stuff(_object) if result: HttpResponse(json.dumps({'success': 'successfully did thing!'}, content_type=\u0026quot;application/json\u0026quot;, status=200) else: return HttpResponse(json.dumps({'error': 'Bugger!'}, content_type=\u0026quot;application/json\u0026quot;, status=400) 这个是我用来处理一些异步请求的代码。虽然看过去不是非常的可怕，但是真的厌恶其中一些东西。首先，每一个 HttpResponse 实例都使用同一个 content_type。接着，我们的实际响应数据都要通过 json.dumps() 来处理。最后，大部分的 status_code 都是相同的，只有一个例外。总的来说，最大的问题时，很多东西出现在密度这么大的代码中，只有很小的一部分与代码的实际意图相关。它隐藏在整个执行流程里。\n因此，带着这些问题，我又开始重构。我想要一个结构，让我们通过关联上下文的方式展示我的当前任务。简而言之，就是我想要 HttpResponse 映射流程中的实际操作。我想用它来描述 Json 响应。\n第一步－没有 partial 的偏函数应用 # JsonResponse = lambda content, *args, **kwargs: HttpResponse( json.dumps(content), content_type=\u0026quot;application/json\u0026quot;, *args, **kwargs ) 这一步有点不同，我们没有使用 functools.partial 来实现偏函数应用。是因为我们需要用 json.dumps() 处理 content，而 functools.partial 不好实现。不过，原理还是相同的。我们都是进行参数预填充。\n重构 # def do_complicated_thing(request, slug): if not request.is_ajax() or not request.method == 'POST': return JsonResponse({'error': 'Invalid Request'}, status=400) if not _has_required_params(request): return JsonResponse({'error': 'Missing required param(s): {0}'.format(_get_missing(request)), status=400) try: _object = Object.objects.get(slug=slug) except Object.DoesNotExist: return JsonResponse({'error': 'No Object matching x found'}, status=400) else: result = do_a_bunch_of_stuff(_object) if result: JsonResponse({'success': 'successfully did thing!'}, status=200) else: return JsonResponse({'error': 'Bugger!'}, status=400) 简洁多了吧！我使用新创建的 JsonResponse 替代所有的 HttpResponse。现在我们不仅有很好描述的调用格式，还多亏了把 json.dumps() 抽离出来，我们现在可以传递一个字典作为参数，使得接口更加的简洁。虽然已经很好了，但是我们可以继续进一步更有趣的优化。现在我们可以通过纯函数式的实现，创建小型的伪类继承关系！让我们编写一些可能产生的响应类型。\nJsonResponse = lambda content, *args, **kwargs: HttpResponse( json.dumps(content), content_type=\u0026quot;application/json\u0026quot;, *args, **kwargs ) JsonOKResponse = functools.partial(JsonResponse, status=200) JsonCreatedResponse = functools.partial(JsonResponse, status=201) JsonBadRequestResponse = functools.partial(JsonResponse, status=400) JsonNotAllowedResponse = functools.partial(JsonResponse, status=405) 注意，这些偏函数建立在我们之前的基础上。他是一个偏函数应用的偏函数应用。下面是我们的伪类继承关系：\nHttpResponse | |-- JsonResponse | | - JsonBadRequestResponse | - JsonOKResponse | - JsonCreatedResponse | - JsonOKResponse 最后的重构 # def do_complicated_thing(request, slug): if not request.is_ajax() or not request.method == 'POST': return JsonBadRequestResponse({'error': 'Invalid Request'}) if not _has_required_params(request): return JsonBadRequestResponse({'error': 'Missing required param(s): {0}'.format(_get_missing(request))) try: _object = Object.objects.get(slug=slug) except Object.DoesNotExist: return JsonBadRequestResponse({'error': 'No Object matching x found'}) else: result = do_a_bunch_of_stuff(_object) if result: JsonOKResponse({'success': 'successfully did thing!'}) else: return JsonBadRequestResponse({'error': 'Bugger!'}) 就这样了。一组简单的描述工具都内置了通过偏应用程序编写的基本对象。\n结语 # 偏函数应用；奇特的名字；简单的想法。作为函数式编程最重要的概念之一，偏函数应用通过固定参数的方式使得代码更加可读，调用更加简便了。永远告别冗长的参数表。当你一次又一次的使用它的时候，即使很简单绑定参数的方式也会带来很大的可读性差异。\n参考： Cleaner Code Through Partial Function Application\n","date":"2015-04-20","permalink":"/n3xtchen/2015/04/20/python-clean-code-through-partial-function-application/","section":"时间线","summary":"偏函数应用（Partial Function Application）听起来名字就很吸引人；它的作用是在函数调用前，预先固定参数的方法。它的机制有点粗糙，学术的解释有点古板，但是它很有用。如果你的函数需要 x 和 y 两个参数，实现把 x 参数固定了，后续调用只需要传入 y 即可，来看一个例子：","title":"Python - 偏函数应用让你的代码看起来更简洁"},{"content":"","date":"2015-04-17","permalink":"/n3xtchen/tags/atdd/","section":"标签","summary":"","title":"atdd"},{"content":"","date":"2015-04-17","permalink":"/n3xtchen/categories/cucumber/","section":"分类页","summary":"","title":"Cucumber"},{"content":"经过一段时间的学习，对 Cucumber 有了初步的了解，它所包含的理念深深的打动了我。作为 TDD 的拥护者，已经长期被需求折磨的开发来讲，希望借助 Cucumber 的推广：\n有效地指导我们的开发，提高效率，降低需求误解的带来的成本，避免垃圾需求的实现污染代码库； 帮助需求方更好地整理思路，更准确的表述需求，以及提供更清晰的用例； 好话不多说，我们现在通过简单的流程初步认识 Cucumber。\n首先，假设一个案例（来自**《The Cucumber Book》**）， 我们的目的为了编写一个计算器程序。\n我们有个非常棒的想法（^_^），这个计算器将来有一天可能会成一个云服务，运行在每个手机，PC 以及浏览器上，帮助人们进行各种各样的计算。我们是务实的善根，因此，我们的第一个版本尽可能的简单。第一版本的程序是一个命令行工具，有 Ruby 实现；它接受输入，通过命令行输出结果。\n例如，如果输入一个文件，如下：\n2+2 它的输出应该是 4；\n类似的，如果输入的文件是：\n100/2 那它的输出就应该是 50。\n创建一个功能（Feature） # Cucumber 测试通过功能来分组。\n我们首先要做的是创建一个目录来保存我们的程序\nλ MacBook-Pro cucumber → mkdir calculator λ MacBook-Pro cucumber → cd calculator 我们将通过 Cucumber 来指导我们的开发，因此我们先从运行 cucumber 命令开始：\nλ MacBook-Pro calculator → cucumber No such file or directory @ rb_sysopen - features. Please create a features directory to get started. (Errno::ENOENT) 因为我们没有提供任何参数给 cucumber 命令；因此，按惯例，我们使用 features 目录来保存我们的测试文件。由于不存在这个目录，就像命令提醒我们那样；那我们就按照它的指示继续：\nλ MacBook-Pro calculator → mkdir features λ MacBook-Pro calculator → cucumber 0 scenarios 0 steps 0m0.000s 现在没有报错了。讲解一些概念，每一个 Cucumber 测试就是一个 scenarios，以及每一个 scenarios 都包含多个 step。这个输出告诉我们，Cucumber 扫描了 features 目录，但是没有找到任何的 scenarios 可以执行。\n根据我们的用户调查，所有的运算操作中加法占据了 67%，因此我们首先要支持他。使用你最喜欢的编辑器，创建一个叫做 features/adding.feature 的文本文件，内容如下：\nFeature: Adding Scenario: Add two numbers Given the input \u0026quot;2+2\u0026quot; When the calculator is run Then the output should be \u0026quot;4\u0026quot; 这个功能文件包含了我们程序的第一个场景。关键字 Feature，Scenario，Given，When 和 Then 是结构，其他的东西都是文档。我们称这个结构为 Gherkin。\n保存好你的文件，在运行一次 cucumber：\nλ MacBook-Pro calculator → cucumber Feature: Adding Scenario: Add two numbers # features/adding.feature:2 Given the input \u0026quot;2+2\u0026quot; # features/adding.feature:3 When the calculator is run # features/adding.feature:5 Then the output should be \u0026quot;4\u0026quot; # features/adding.feature:6 1 scenario (1 undefined) 3 steps (3 undefined) 0m0.002s You can implement step definitions for undefined steps with these snippets: Given(/^the input \u0026quot;(.*?)\u0026quot;$/) do |arg1| pending # express the regexp above with the code you wish you had end When(/^the calculator is run$/) do pending # express the regexp above with the code you wish you had end Then(/^the output should be \u0026quot;(.*?)\u0026quot;$/) do |arg1| pending # express the regexp above with the code you wish you had end If you want snippets in a different programming language, just make sure a file with the appropriate file extension exists where cucumber looks for step definitions. Wow，突然这么多的输出，吓的一跳吧。首先，我们可以看到 Cucumber 找到了我们的 feature，并且尝试运行它；接下来的是给出了三个 Ruby 代码片段。那我们就遵照这个指示进行下一步了，把这三个代码片段黏贴到文件中。\n创建步骤（Step Definitions） # 现在，我们开始实现一些步骤，让我们的场景不再 undefined。不要在意它的含义，把它拷贝到文件中就是了；Cucumber 的惯例是把步骤定义存放在 features/step_definitions 下，现在我们要创建它：\nλ MacBook-Pro calculator → mkdir features/step_definitions 现在我们在这个目录下面创建一个 Ruby 文件，命名为 calculator_step.rb。Cucumber 不在乎步骤文件的命名，只要他是一个 Ruby 文件，但是好的名称还是很有必要的。现在我们打开它，代码片段复制进去：\n# features/step_definitions/calculator_step.rb Given(/^the input \u0026quot;(.*?)\u0026quot;$/) do |arg1| pending # express the regexp above with the code you wish you had end When(/^the calculator is run$/) do pending # express the regexp above with the code you wish you had end Then(/^the output should be \u0026quot;(.*?)\u0026quot;$/) do |arg1| pending # express the regexp above with the code you wish you had end 保存完文件，我们执行下 cucumber：\nλ MacBook-Pro calculator → cucumber Feature: Adding Scenario: Add two numbers # features/adding.feature:2 Given the input \u0026quot;2+2\u0026quot; # features/step_definitions/calculator_step.rb:1 TODO (Cucumber::Pending) ./features/step_definitions/calculator_step.rb:2:in `/^the input \u0026quot;(.*?)\u0026quot;$/' features/adding.feature:3:in `Given the input \u0026quot;2+2\u0026quot;' When the calculator is run # features/step_definitions/calculator_step.rb:5 Then the output should be \u0026quot;4\u0026quot; # features/step_definitions/calculator_step.rb:9 1 scenario (1 pending) 3 steps (2 skipped, 1 pending) 0m0.057s 我们顺利让场景从 undefined 晋级到 pending。这是个好消息，说明 Cucumber 在执行第一个步骤，但是碰到了 pending 标记。我们现在需要使用真正的实现来替换 pending 标记。\n注意 Cucumber 报道另外两个步骤被跳过；是因为当它遇到一个失败或者 pending 步骤的时候，Cucumber 将会停止执行这个场景，跳过剩下的步骤。\n实现我们第一个步骤（Step Definitions） # 我们已经决定我们的第一版本是一个计算器，接受命令行的参数作为输入，因此我们的第一步就是获取一个参数；我们现在开始定义：\n# features/step_definitions/calculator_step.rb Given(/^the input \u0026quot;(.*?)\u0026quot;$/) do |input| @input = input end 然后，\nλ MacBook-Pro calculator → cucumber Feature: Adding Scenario: Add two numbers # features/adding.feature:2 Given the input \u0026quot;2+2\u0026quot; # features/step_definitions/calculator_step.rb:1 When the calculator is run # features/step_definitions/calculator_step.rb:5 TODO (Cucumber::Pending) ./features/step_definitions/calculator_step.rb:6:in `/^the calculator is run$/' features/adding.feature:5:in `When the calculator is run' Then the output should be \u0026quot;4\u0026quot; # features/step_definitions/calculator_step.rb:9 1 scenario (1 pending) 3 steps (1 skipped, 1 pending, 1 passed) 0m0.031s 我们的第一步通过了！但是场景还是被标记为 pending，当然，是因为我们实现了一步，剩下的两个步骤还没有实现。不过我们取得了进展，不是吗？\n运行我们的程序 # 现在开始实现我们的第二步：\nWhen(/^the calculator is run$/) do @output = `ruby cacl.rb #{@input}` raise('Command failed') unless $?.success? end 然后,\nλ MacBook-Pro calculator → cucumber Feature: Adding Scenario: Add two numbers # features/adding.feature:2 Given the input \u0026quot;2+2\u0026quot; # features/step_definitions/calculator_step.rb:1 ruby: No such file or directory -- cacl.rb (LoadError) When the calculator is run # features/step_definitions/calculator_step.rb:5 Command failed (RuntimeError) ./features/step_definitions/calculator_step.rb:7:in `/^the calculator is run$/' features/adding.feature:5:in `When the calculator is run' Then the output should be \u0026quot;4\u0026quot; # features/step_definitions/calculator_step.rb:10 Failing Scenarios: cucumber features/adding.feature:2 # Scenario: Add two numbers 1 scenario (1 failed) 3 steps (1 failed, 1 skipped, 1 passed) 0m0.103s 这个步骤失败了，是因为没有 calc.rb 这个程序可以运行。\n添加断言 # 按照 cucumber 的指示，我们需要为我们的程序，创建一个 Ruby 文件。\n针对 Mac/Linux 用户在命令行中输入如下代码来创建这个文件：\nλ MacBook-Pro calculator → touch cacl.rb 如果是 Windows 用户，你的命令是：\n​C:\\\u0026gt; echo.\u0026gt;calc.rb​ 然后我们再次执行 cucumber\nλ MacBook-Pro calculator → cucumber Feature: Adding Scenario: Add two numbers # features/adding.feature:2 Given the input \u0026quot;2+2\u0026quot; # features/step_definitions/calculator_step.rb:1 When the calculator is run # features/step_definitions/calculator_step.rb:5 Then the output should be \u0026quot;4\u0026quot; # features/step_definitions/calculator_step.rb:10 TODO (Cucumber::Pending) ./features/step_definitions/calculator_step.rb:11:in `/^the output should be \u0026quot;(.*?)\u0026quot;$/' features/adding.feature:6:in `Then the output should be \u0026quot;4\u0026quot;' 1 scenario (1 pending) 3 steps (1 pending, 2 passed) 0m0.102s 现在我们给我们的最后一步添加断言：\nThen(/^the output should be \u0026quot;(.*?)\u0026quot;$/) do |arg1| expect(@output).to eq 4 end 我们使用 RSpec 断言来检查 feature 中指定的期待值是否与程序的实际输出一致:\nλ MacBook-Pro calculator → cucumber Feature: Adding Scenario: Add two numbers # features/adding.feature:2 Given the input \u0026quot;2+2\u0026quot; # features/step_definitions/calculator_step.rb:2 When the calculator is run # features/step_definitions/calculator_step.rb:6 Then the output should be \u0026quot;4\u0026quot; # features/step_definitions/calculator_step.rb:11 expected: 4 got: \u0026quot;\u0026quot; (compared using ==) (RSpec::Expectations::ExpectationNotMetError) ./features/step_definitions/calculator_step.rb:12:in `/^the output should be \u0026quot;(.*?)\u0026quot;$/' features/adding.feature:6:in `Then the output should be \u0026quot;4\u0026quot;' Failing Scenarios: cucumber features/adding.feature:2 # Scenario: Add two numbers 1 scenario (1 failed) 3 steps (1 failed, 2 passed) 0m0.080s 好极了。现在我们的测试终于能够由于合理的理由失败了：它运行我们的程序，检查输出，告诉我们应该如何正确输出。我们已经为这个版本做了很多工走：当我们回到这个代码的时候，cucumber 将告诉我们该做做什么来让我们的程序生效。如果我们所有的需求都可以利用失败的测试来验证，创建软件将会容易很多。\n通过测试 # 现在我们有了一个失败的 Cucumber 场景，是时候让我们的场景驱动我们解决问题。\n试试下面的代码：\n# cacl.rb print \u0026quot;4\u0026quot; 终于成功了\nλ MacBook-Pro calculator → cucumber Feature: Adding Scenario: Add two numbers # features/adding.feature:2 Given the input \u0026quot;2+2\u0026quot; # features/step_definitions/calculator_step.rb:2 When the calculator is run # features/step_definitions/calculator_step.rb:6 Then the output should be \u0026quot;4\u0026quot; # features/step_definitions/calculator_step.rb:11 1 scenario (1 passed) 3 steps (3 passed) 0m0.095s http://baya.github.io/2014/04/21/cucumber%E5%AE%9E%E6%88%98/\n","date":"2015-04-17","permalink":"/n3xtchen/2015/04/17/cucumber---intro/","section":"时间线","summary":"经过一段时间的学习，对 Cucumber 有了初步的了解，它所包含的理念深深的打动了我。作为 TDD 的拥护者，已经长期被需求折磨的开发来讲，希望借助 Cucumber 的推广：","title":"Cucumber - 入门"},{"content":"通常情况，常规的 SQL 事件（Event） 应该在普通行为的之前或之后被触发。这个行为可以是对插入的值的类型检查，可以是在插入前的格式化，或者是变更和删除数据之后对相关的表的数据修改。传统的处理方式是通过连接数据库的应用的编码层来做，而不是数据库软件本身。\n为了减轻数据库和应用之间的交互负担，PostgreSQL 提供一种非标准的可编程拓展（即 触发器，本文在后续的阐述中使用 trigger 这个名词）。Trigger 定义一个函数在其他表操作的前后执行。\nTrigger 可以影响如下几个表操作（即 event）：\nINSERT UPDATE DELETE 创建 Trigger # 创建 Trigger 之前，我们需要执行的函数首先需要存在。PostgreSQL 支持多种语言的函数，如 PL/pgSQL。\n函数一旦定义，我们就可以创建 Trigger。先看看创建语法：\nCREATE TRIGGER name { BEFORE | AFTER | INSTEAD OF } { event [ OR event ... ] } ON tablename FOR EACH { ROW | STATEMENT } EXECUTE PROCEDURE functionname ( arguments ) BEFORE | AFTER | INSTEAD OF: 事件之前,之后或者替代该事件操作； event [OR event]: 即前文提到的 CUD 操作，可以绑定多个事件，事件之间用 OR 隔开； ROW | STATEMENT: ROW 对每行执行一次函数；STETEMENT 对每个执行语句执行一次； EXECUTE PROCEDURE functionname ( arguments ): 即调用的函数，即参数。 注意：只有超级用户或者数据库拥有者才能够创建 Trigger。\n让我们幻想一个场景，就像 PostgreSQL - 序列 提到的，使用自增序列作为表的主键存在风险，我们可以使用 Trigger 来规避不确定的用户行为带来的问题。 首先建立一个测试表 shipments：\npigdb=# CREATE SEQUENCE shipments_ship_id_seq pigdb-# MINVALUE 0; CREATE SEQUENCE pigdb=# CREATE TABLE shipments ( id integer NOT NULL PRIMARY KEY, customer_id integer, isbn text, ship_date timestamp ); CREATE TABLE 然后创建一个函数来完成自增的操作（后续的文章会详细阐述这一点，这里只是带过一下）：\npigdb=# CREATE OR REPLACE FUNCTION insert_id() RETURNS trigger AS $$ pigdb$# DECLARE pigdb$# seq_id integer;\t-- 声明一个变量，存储新的序列值 pigdb$# BEGIN pigdb$# SELECT INTO seq_id nextval('shipments_ship_id_seq'); -- 获取新序列值 pigdb$# NEW.id = seq_id;\t-- 赋值给记录 pigdb$# return NEW;\t-- 返回修改后的记录 pigdb$# END; pigdb$# $$ LANGUAGE plpgsql VOLATILE;\t-- 指定使用 PL/PGSQL 作为脚本语言 CREATE FUNCTION 最后，我们开始创建 Trigger：\npigdb=# CREATE TRIGGER insert_ship_id BEFORE INSERT pigdb-# ON shipments pigdb-# FOR EACH ROW pigdb-# EXECUTE PROCEDURE insert_id(); CREATE TRIGGER 现在我们查看下表结构：\npigdb=# \\d shipments Table \u0026quot;public.shipments\u0026quot; Column | Type | Modifiers -------------+-----------------------------+-------------------------------- id | integer | not nul customer_id | integer | isbn | text | ship_date | timestamp without time zone | Indexes: \u0026quot;shipments_pkey\u0026quot; PRIMARY KEY, btree (id) Triggers: insert_ship_id BEFORE INSERT ON shipments FOR EACH ROW EXECUTE PROCEDURE insert_id() 可以看到，这个表上已经挂载到 Trigger，现在我插入几个数据看看：\npigdb=# SELECT * FROM shipments; id | customer_id | isbn | ship_date ----+-------------+------------+---------------------------- 2 | 221 | 0394800753 | 2015-04-15 14:12:55.744302 (1 row) 我们不需要指定 id，现在我们试试指定了主键 id 后会有什么效果？\npigdb=# INSERT INTO shipments (id, customer_id, isbn, ship_date) VALUES (4 ,221, '0394800753', 'now'); INSERT 0 1 pigdb=# SELECT * FROM shipments; id | customer_id | isbn | ship_date ----+-------------+------------+---------------------------- 2 | 221 | 0394800753 | 2015-04-15 14:12:55.744302 3 | 221 | 0394800753 | 2015-04-15 14:13:24.810759 (2 rows)\t输出结果很明显，不论你指定还是不指定主键 id，插入的数据都不会受到影响，返回都是序列的下一个值。\n查看 Trigger # Trigger 是存储在 pg_trigger 表中的，我们查看下它的结构：\npigdb=# \\d pg_trigger Table \u0026quot;pg_catalog.pg_trigger\u0026quot; Column | Type | Modifiers ----------------+--------------+----------- tgrelid | oid | not null tgname | name | not null tgfoid | oid | not null tgtype | smallint | not null tgenabled | \u0026quot;char\u0026quot; | not null tgisinternal | boolean | not null tgconstrrelid | oid | not null tgconstrindid | oid | not null tgconstraint | oid | not null tgdeferrable | boolean | not null tginitdeferred | boolean | not null tgnargs | smallint | not null tgattr | int2vector | not null tgargs | bytea | tgqual | pg_node_tree | Indexes: \u0026quot;pg_trigger_oid_index\u0026quot; UNIQUE, btree (oid) \u0026quot;pg_trigger_tgrelid_tgname_index\u0026quot; UNIQUE, btree (tgrelid, tgname) \u0026quot;pg_trigger_tgconstraint_index\u0026quot; btree (tgconstraint) 删除 Trigger # 删除 Trigger 就更简单：\npigdb=# DROP TRIGGER insert_ship_id ON shipments; DROP TRIGGER ON (tgrelid = relfilenode) WHERE tgname = 'insert_ship_id '; 删除之前，你还可以查看下要删的 Trigger 相关联的对象有哪些：\npigdb=# SELECT relname FROM pg_class INNER JOIN pg_trigger ON (tgrelid = relfilenode) WHERE tgname = 'insert_ship_id'; relname ----------- shipments (1 row) tgname 就是所要查询的 Trigger 名称。还有给需要注意的就是，当 Trigger 使用的函数被重建时，Trigger 也需要重建才能生效。\n结语 # Trigger 是对 PostgreSQL 约束（Contraints）的补充，可以配合 PL 语法进行输入值的复杂验证，或者屏蔽某些用户的误操作（如上述例子中，自增主键的实现）。另外，很多人从 PostgreSQL 转到 MySQL，都很怀恋 Replace 语法的简便；PostgreSQL 虽然不直接支持，但是可以通过 Trigger 和 PL/pgSQL 实现，后续的涉及 PL 语法的时候，将详细阐述该实现。\n","date":"2015-04-14","permalink":"/n3xtchen/postgresql/2015/04/14/postgresql-trigger/","section":"时间线","summary":"通常情况，常规的 SQL 事件（Event） 应该在普通行为的之前或之后被触发。这个行为可以是对插入的值的类型检查，可以是在插入前的格式化，或者是变更和删除数据之后对相关的表的数据修改。传统的处理方式是通过连接数据库的应用的编码层来做，而不是数据库软件本身。","title":"PostgreSQL 触发器（Trigger）- 创建安全的自增主键"},{"content":"PostgreSQL 中的序列是一个数据库对象，本质上是一个自增器。因此，序列在其他同类型数据库软件中以 autoincrment 值的形式存在。在一张表需要非随机，唯一标实符的场景下，Sequence 非常有用。\nSequence 对象中包含当前值，和一些独特属性，例如如何递增（或者递减）。实际上，Sequence 是不能被直接访问到的；他们需要通过 PostgreSQL 中的相关函数来操作他们。\n创建序列 # 看看创建的语法：\nCREATE SEQUENCE sequencename [ INCREMENT increment ]\t-- 自增数，默认是 1 [ MINVALUE minvalue ]\t-- 最小值 [ MAXVALUE maxvalue ]\t-- 最大值 [ START start ]\t-- 设置起始值 [ CACHE cache ]\t-- 是否预先缓存 [ CYCLE ]\t-- 是否到达最大值的时候，重新返回到最小值 Sequence 使用的是整型数值，因此它的取值范围是 [-2147483647, 2147483647] 之间；现在我们创建一个简单的序列：\npigdb\u0026gt; CREATE SEQUENCE shipments_ship_id_seq MINVALUE 0; CREATE SEQUENCE 查看序列 # psql 的 \\d 命令输出一个数据库对象，包括 Sequence，表，视图和索引。你还可以使用 \\ds 命令只查看当前数据库的所有序列。例如：\npigdb-# \\ds List of relations Schema | Name | Type | Owner --------+-----------------------+----------+-------- public | author_ids | sequence | ichexw public | shipments_ship_id_seq | sequence | ichexw (2 rows) Sequence 就像表和视图一样，拥有自己的结构，只不过它的结构是固定的:\npigdb=# \\d shipments_ship_id_seq Sequence \u0026quot;public.shipments_ship_id_seq\u0026quot; Column | Type | Value ---------------+---------+----------------------- sequence_name | name | shipments_ship_id_seq last_value | bigint | 0\tstart_value | bigint | 0 increment_by | bigint | 1 max_value | bigint | 9223372036854775807 min_value | bigint | 0 cache_value | bigint | 1 log_cnt | bigint | 0 is_cycled | boolean | f is_called | boolean | f\t我们现在查询下 shipments_ship_id_seq 的 last_value（当前的序列值）和 increment_by (当 nextval() 被调用，当前值将会被增加)。\npigdb=# SELECT last_value, increment_by FROM shipments_ship_id_seq; last_value | increment_by ------------+-------------- 0 | 1 (1 row) 由于序列刚刚被创建，因此 last_value 被设置成 0。\n使用序列 # 我们需要知道的 Sequence 的函数使用：\nnextval(\u0026lsquo;sequence_name\u0026rsquo;): 将当前值设置成递增后的值，并返回 currval(\u0026lsquo;sequence_name\u0026rsquo;): 返回当前值 setval(\u0026lsquo;sequence_name\u0026rsquo;, n, b=true): 设置当前值；b 默认设置 true，下一次调用 nextval() 时，直接返回 n，如果设置 false，则返回 n+increment: nextval() 函数要求一个序列名（必须由单引号包围）为第一个参数。 需要注意的是，当你第一次调用 nextval() 将会返回序列的初始值，即 START；因为他没有调用递增的方法。\npigdb=# SELECT nextval('shipments_ship_id_seq'); nextval --------- 0 (1 row) pigdb=# SELECT nextval('shipments_ship_id_seq'); nextval --------- 1 (1 row) Sequence 一般作为表的唯一标识符字段的默认值使用（这是序列的最常见的场景）；看个例子：\npigdb=# CREATE TABLE shipments (id integer DEFAULT nextval('shipments_ship_id_seq') PRIMARY KEY, customer_id integer, isbn text, ship_date timestamp); CREATE TABLE pigdb=# \\d shipments Table \u0026quot;public.shipments\u0026quot; Column | Type | Modifiers -------------+-----------------------------+------------------------------------------------------------- id | integer | not null default nextval('shipments_ship_id_seq'::regclass) customer_id | integer | isbn | text | ship_date | timestamp without time zone | Indexes: \u0026quot;shipments_pkey\u0026quot; PRIMARY KEY, btree (id) 这张表中的 id 字段的默认值将被设置成 shipments_ship_id_seq 的 nextval() 值。如果插入值的时候，没有指定 id 的值，将会自动选择 nextval('shipments_ship_id_seq') 的值。\n注意： id 字段的默认值并不是强制使用的。用户仍然可以手动插入值，这样潜在地造成与未来的序列值冲突的风险。这个可以通过 trigger 来防止这个问题，后续将详细介绍。\n为了防止同一个序列同时被多个被多个用户访问导致错误，序列的当前值与 session 关联。两个用户可能在两个不同的会话访问同一个序列，但是调用 currval() 时，只会返回同一会话下的当前值。\n现在看看 curval() 的简单用法：\npigdb=# INSERT INTO shipments (customer_id, isbn, ship_date) VALUES (221, '0394800753', 'now'); INSERT 0 1 pigdb=# SELECT * FROM shipments WHERE id = currval('shipments_ship_id_seq'); id | customer_id | isbn | ship_date ----+-------------+------------+---------------------------- 2 | 221 | 0394800753 | 2015-04-12 00:38:07.298688 (1 row) 另外，一个序列也可以通过 setval() 将 last_value 设置成任意值（必须在序列的取值范围内）。这个要求一个序列名（必须由单引号包围着）作为第一个参数，以及要设置的最后值作为第二个参数；看几个例子：\npigdb=# SELECT setval('shipments_ship_id_seq', 1010); setval -------- 1010 (1 row) pigdb=# SELECT nextval('shipments_ship_id_seq'); nextval --------- 1011 (1 row) 前文中，我们还提到了 setval() 的第三个参数；现在把它设置成 false，验证下效果：\npigdb=# SELECT setval('shipments_ship_id_seq', 1010, false); setval -------- 1010 (1 row) pigdb=# SELECT nextval('shipments_ship_id_seq'); nextval --------- 1010 (1 row) 当第三个参数设置成 false 的时候，就像重新创建序列时，第一次调用的时候，只是初始化 last_val，不会调用递增函数。\n删除序列 # 你可以使用：\nDROP SEQUENCE seq_name[, ...] 来删除一个或者多个序列。命令中的 seq_name 是序列名，不须被引号包围；如果是多个序列，可以使用逗号隔开。\n现在我们试一下这个命令：\npigdb=# DROP SEQUENCE shipments_ship_id_seq; ERROR: cannot drop sequence shipments_ship_id_seq because other objects depend on it DETAIL: default for table shipments column id depends on sequence shipments_ship_id_seq HINT: Use DROP ... CASCADE to drop the dependent objects too. 这里报错了，由于该序列被其他对象引用，因此无法直接删除，除非你使用 DROP ... CASCADE。\n我们可以使用下面的语句来查看的序列是否被数据库中的其他对象引用，：\npigdb=# SELECT p.relname, a.adsrc FROM pg_class p JOIN pg_attrdef a on (p.relfilenode = a.adrelid) WHERE a.adsrc ~ 'shipments_ship_id_seq'; relname | adsrc -----------+-------------------------------------------- shipments | nextval('shipments_ship_id_seq'::regclass) (1 row) 这里检查到 shipments_ship_id_seq 序列被 shipments 引用。你可以把这个序列名替换成任何一个你像查看的序列；或者不添加任何条件查看当前数据库中所有序列的引用。\n现在我们成功地执行一次序列删除：\npigdb=# DROP TABLE shipments; DROP TABLE pigdb=# DROP SEQUENCE shipments_ship_id_seq; DROP SEQUENCE ","date":"2015-04-10","permalink":"/n3xtchen/2015/04/10/postgresql---sequence/","section":"时间线","summary":"PostgreSQL 中的序列是一个数据库对象，本质上是一个自增器。因此，序列在其他同类型数据库软件中以 autoincrment 值的形式存在。在一张表需要非随机，唯一标实符的场景下，Sequence 非常有用。","title":"PostgreSQL - 序列（Sequence）"},{"content":"","date":"2015-04-05","permalink":"/n3xtchen/tags/nosql/","section":"标签","summary":"","title":"nosql"},{"content":"PostgreSQL 可以通过一种数据结构在独立字段中存储非原子的值。这个数据结构就是我们今天要谈的数组（Array），它本身不是一种数据类型，而是任何数据类型的一种拓展。\n创建一个数组字段 # 在 CREATE TABLE 或 ALTER 语句中需要声明成数组的字段类型后面追加上方括号，这样就可以创建一个数组字段。\n现在来看个例子，声明一个一维数组：\nsingle_array type[] 可以将多个方括号追加在类型的后面，就可以声明一个多维数组，可以在数组中存储数组：\nmulti_array type[][] 理论上说，可以在方括号中插入一个整数 n 来声明一个定长（fixed-length）的数组。但是在 PostgreSQL 7.1.x 之后，这个限制非强制的，实际上定长数组和非定长数组之间没有实际的不同。\n接下来是一个创建包含数组的表：\npigdb\u0026gt; CREATE TABLE faviroute_book ( employee_id INTEGER, books TEXT[] ); CREATE TABLE 这个例子中，books 字段存储任何数量的书名数组。创建多维数据的方法也是类似的，唯一的区别就是有另一对方括号要紧跟在第一对的后面，就像前面的例子那样。\npigdb\u0026gt; CREATE TABLE favorite_authors ( employee_id INTEGER, author_and_titles TEXT[][] ); CREATE TABLE 给数组追加数据 # 为了在一个字段插入多个值，PostgreSQL 引入了一个特殊的语法。这个语法允许你描述一个数组常量。这个数组常量有话括号，双引号和逗号组成，外层由单引号包裹着。双引号只有在字符数组的时候，需要用到，通用的模式：\n'{\u0026quot;text1\u0026quot;,[...]}'\t-- 字符数组 '{ numeric, [...]}' -- 数字数组 这个语法模式演示如何使用字符和数字数组，但是每个数组必须定义单一数据类型（可以是任何的数据类型，boolean，date 或者是 time）。通常情况下，你使用单引号来描述非数组上下文的值，双引号用来数组中的值。\n现在，我们插入数据：\npigdb\u0026gt; INSERT INTO faviroute_book VALUES (102, E'{\u0026quot;The Hichhiker \\'s GUIDE to he Galaxy\u0026quot;}');\t-- 插入一个值的数组，注意这边的单引号需要转移，E - escape, 后续会单独讲解这块 INSERT 0 1 pigdb\u0026gt; INSERT INTO faviroute_book VALUES (103, '{\u0026quot;The Hobbit\u0026quot;, \u0026quot;Kitten, Squared\u0026quot;}');\t-- 插入多个值的数组 INSERT 0 1 pigdb\u0026gt; SELECT * FROM faviroute_book; +---------------+------------------------------------------+ | employee_id | books | |---------------+------------------------------------------| | 102 | [u\u0026quot;The Hichhiker 's GUIDE to he Galaxy\u0026quot;] | | 103 | [u'The Hobbit', u'Kitten, Squared'] | +---------------+------------------------------------------+ 在插入单个值的时候，花括号还是需要的，这是因为数组常量本身也需要被当作字符串来解析，随后基于上下文解析成真正的数组。\n多维数组的插入也是类似，看看例子：\npigdb\u0026gt; INSERT INTO favorite_authors VALUES (102, '{ {\u0026quot;J.R.R. Tokeien\u0026quot;, \u0026quot;The Silmarillion\u0026quot;}, {\u0026quot;Charless Dickness\u0026quot;, \u0026quot;Great Expectations\u0026quot;} }' ); INSERT 0 1 pigdb\u0026gt; SELECT * FROM favorite_authors; +---------------+-------------------------------------------------------------------------------------------+ | employee_id | author_and_titles | |---------------+-------------------------------------------------------------------------------------------| | 102 | [[u'J.R.R. Tokeien', u'The Silmarillion'], [u'Charless Dickness', u'Great Expectations']] | +---------------+-------------------------------------------------------------------------------------------+ 查询数组 # 之前已经演示查询数组字段，它会返回整个数组。但是数组的最大用处实际是依赖于他的下标操作。\n数组下标（Subscripts） # 下标的语法由方括号包围着整形数值；这个数值描述的是从左开始的位置距离。\n不像大部分编程语言（比如 C），PostgreSQL 的下标是从 1 开始的，而不是 0；现在看个例子\npigdb\u0026gt; SELECT books[1] FROM faviroute_book; +-------------------------------------+ | books | |-------------------------------------| | The Hichhiker 's GUIDE to he Galaxy | | The Hobbit | +-------------------------------------+ SELECT 2 注意到了吧，查询返回的值不包含话括号的双引号；这是因为单个文本值只需要作为文本常量返回，而不是一个数组；在看一条：\npigdb\u0026gt; SELECT books[2] FROM faviroute_book; +-----------------+ | books | |-----------------| | | | Kitten, Squared | +-----------------+ 如果筛选的的结果值被存在会返回空值。你可以使用 IS NOT NULL 的语法来过滤它：\npigdb\u0026gt; SELECT books[2] FROM faviroute_book WHERE books[2] IS NOT NULL; +-----------------+ | books | |-----------------| | Kitten, Squared | +-----------------+ 现在，我们来操作下多维数组：\npigdb\u0026gt; SELECT author_and_titles[1][1] AS author, author_and_titles[1][2] AS title FROM favorite_authors; +----------------+------------------+ | author | title | |----------------+------------------| | J.R.R. Tokeien | The Silmarillion | +----------------+------------------+ SELECT 1 数组切片（Slices） # PostgreSQL 同样支持数组的切片操作；原理和下标操作类似，只是它返回制定范围段的值。切片的语法由一对整形数字，被冒号隔开，被方括号包围；例如 [2:5] 返回的是，数组的第二，第三，第四和第五的值，以数组形式返回：\npigdb\u0026gt; INSERT INTO faviroute_book VALUES (104, '{\u0026quot;The Hobbit\u0026quot;, \u0026quot;Kitten, Squared\u0026quot;, \u0026quot;Practical PostgreSQL\u0026quot;}');\t-- 为了掩饰效果，插入一条数组 INSERT 0 1 pigdb\u0026gt; SELECT * FROM faviroute_book; +---------------+--------------------------------------------------------------+ | employee_id | books | |---------------+--------------------------------------------------------------| | 102 | [u\u0026quot;The Hichhiker 's GUIDE to he Galaxy\u0026quot;] | | 103 | [u'The Hobbit', u'Kitten, Squared'] | | 104 | [u'The Hobbit', u'Kitten, Squared', u'Practical PostgreSQL'] | +---------------+--------------------------------------------------------------+ SELECT 3 pigdb\u0026gt; SELECT books[1:2] FROM faviroute_book; +------------------------------------------+ | books | |------------------------------------------| | [u\u0026quot;The Hichhiker 's GUIDE to he Galaxy\u0026quot;] | | [u'The Hobbit', u'Kitten, Squared'] | | [u'The Hobbit', u'Kitten, Squared'] | +------------------------------------------+ SELECT 3 而多维数组在切片方面有点不可预期，因此不推荐在多维数组使用切片操作。\n数组维度（Dimensions） # 有些时候，我们需要知道数组中存储值的数量，你可以使用 array_dims() 函数；它接受一个数组参数，返回的字符串的格式和切片语法相同，描述有点拗口，不多说，看看例子就知道了：\npigdb\u0026gt; SELECT array_dims(books) FROM faviroute_book; +--------------+ | array_dims | |--------------| | [1:1] | | [1:2] | | [1:3] | +--------------+ SELECT 3 更新数组 # 数组更新的方式有三种：\n整体变更：修改整个字段 切片变更：修改某个范围的值 元素变更：修改单个元素的值 PostgreSQL 对数组的更新操作没有限制，我们通过几个简单的例子就可以一窥究竟了；\npigdb\u0026gt; UPDATE faviroute_book SET books=E'{\u0026quot;The Hichhiker\\'s GUIDE to he Galaxy\u0026quot;}' WHERE employee_id = 102;\t-- 整体变更 UPDATE 1 pigdb\u0026gt; UPDATE faviroute_book SET books[2:3]='{\u0026quot;Kitten, Squared\u0026quot;, \u0026quot;Practical PostgreSQL-2014\u0026quot;}' WHERE employee_id = 104;\t-- 切片变更 UPDATE 1 pigdb\u0026gt; UPDATE faviroute_book SET books[1]=E'{\u0026quot;There and Back Again: AHobbit\\'s Holiday\u0026quot;}' WHERE employee_id = 103;\t-- 元素变更 UPDATE 1 pigdb\u0026gt; SELECT * FROM faviroute_book; +---------------+-----------------------------------------------------------------------+ | employee_id | books | |---------------+-----------------------------------------------------------------------| | 102 | [u\u0026quot;The Hichhiker's GUIDE to he Galaxy\u0026quot;] | | 104 | [u'The Hobbit', u'Kitten, Squared', u'Practical PostgreSQL-2014'] | | 103 | [u'{\u0026quot;There and Back Again: AHobbit\\'s Holiday\u0026quot;}', u'Kitten, Squared'] | +---------------+-----------------------------------------------------------------------+ SELECT 3 字符转义 http://stackoverflow.com/questions/19812597/postgresql-string-escaping-settings http://blog.163.com/digoal@126/blog/static/163877040201342185210972/\n","date":"2015-04-05","permalink":"/n3xtchen/postgresql/2015/04/05/postgresql-array/","section":"时间线","summary":"PostgreSQL 可以通过一种数据结构在独立字段中存储非原子的值。这个数据结构就是我们今天要谈的数组（Array），它本身不是一种数据类型，而是任何数据类型的一种拓展。","title":"PostgreSQL - 数组(Array)"},{"content":"","date":"2015-04-04","permalink":"/n3xtchen/tags/object-relation/","section":"标签","summary":"","title":"object-relation"},{"content":"PostgreSQL 支持高级的 objdect-relational 机制，继承。继承允许一张表继承一张（或多张）表的列属性，来建立 parent-child 关系。子表可以继承父表的字段以及约束，同时可以拥有自己的字段。\n当执行一个父表查询的时候，这个查询可以获取来自本表和它的子表，也可以指定只查询本表。在子表中查询，则不会返回父表的数据。\n###创建子表\n通过使用 INHERITES 的建表语法来创建子表：\nCREATE TABLE childtable definition INHERITS ( parenttable [, ...] ) 一张表可以继承多个父表，父表之间是有逗号隔开。现在，来看一个例子：\n首先，我们创建一张表：\npigdb\u0026gt; CREATE TABLE authors ( id INTEGER NOT NULL PRIMARY KEY, last_name TEXT, first_name TEXT ); CREATE TABLE Command Time: 0.157353878021 Format Time: 1.81198120117e-05 然后，我们想创建一张叫 distinguished_authors , 只包含一个 award 字段，让它继承 author 表；\npigdb\u0026gt; CREATE TABLE distinguish_authors ( awards text ) INHERITS (authors); CREATE TABLE Command Time: 0.0200970172882 Format Time: 1.81198120117e-05 现在我们来查看下子表的结构：\npigdb\u0026gt; \\d distinguish_authors; +------------+---------+-------------+ | Column | Type | Modifiers | |------------+---------+-------------| | id | integer | not null | | last_name | text | | | first_name | text | | | awards | text | | +------------+---------+-------------+ Inherits: ('authors',), Command Time: 0.0306630134583 Format Time: 0.000441074371338 使用 # 父子表共同的字段的关系并不仅仅是装饰用的。在 distinguish_authors 中插入数据，同样在 authors 是可见，但是只能看到共享的字段。如果你想要在查询父表的数据，忽略子表，可以使用 ONLY 关键字，现在我们来看看具体用法；\n首先，我们先插入数据到 distinguish_authors:\npigdb\u0026gt; INSERT INTO distinguish_authors VALUES (nextval('author_ids'), 'Simon', 'Neil', 'Pulitzer Prize'); INSERT 0 1 Command Time: 0.0186150074005 Format Time: 1.59740447998e-05 现在，我们查看下效果：\npigdb\u0026gt; SELECT * FROM distinguish_authors WHERE last_name='Simon'; +------+-------------+--------------+----------------+ | id | last_name | first_name | awards | |------+-------------+--------------+----------------| | 1 | Simon | Neil | Pulitzer Prize | +------+-------------+--------------+----------------+ SELECT 1 Command Time: 0.0188879966736 Format Time: 0.00103807449341 pigdb\u0026gt; SELECT * FROM authors WHERE last_name='Simon'; +------+-------------+--------------+ | id | last_name | first_name | |------+-------------+--------------| | 1 | Simon | Neil | +------+-------------+--------------+ SELECT 1 Command Time: 0.0167798995972 Format Time: 0.000402927398682 这两个查询语句中，我们可以看出子表的数据在父表中是可见；\npigdb\u0026gt; SELECT * FROM ONLY authors WHERE last_name='Simon'; +------+-------------+--------------+ | id | last_name | first_name | |------+-------------+--------------| +------+-------------+--------------+ SELECT 0 Command Time: 0.0306789875031 Format Time: 0.000223875045776 在第三个查询语句中，在表名前指定了 ONLY 关键字时，这是只返回父表中满足条件的数据。这里需要理解一个概念：给子表插入数据的时候，并不是同时把数据中的共享字段插入到父表中，只是简单通过继承关系使子表的数据在父表可见。如果你指定了 ONLY 关键字，则查询不在从子表中获取数据。\n另外需要注意的是，由于继承表的本质原因，一些约束条件可能会被打破；例如，声明一个唯一字段，可能在查询结果中2条相同的值。因此，在使用的继承的过程中，你要特别注意约束；因为他们在各自的表中没有违反约束条件；因此，如果你在查询父表的时候没有制定 ONLY 字段，那它可能会返回你非预期的结果。\n更新继承表 # 如果修改子表的数据，将不会影响到父表的数据，这个很好理解；但是修改父表的数据的效果就没有那么显而易见了；UPDATE 和 DELETE 在父表中执行，默认情况下，不仅会影响父表的数据，同时也会修改子表中满足条件的数据。\npigdb\u0026gt; UPDATE authors SET last_name='Chen' WHERE last_name='Simon'; UPDATE 1 Command Time: 0.0584959983826 Format Time: 4.60147857666e-05 pigdb\u0026gt; SELECT * FROM distinguish_authors; +------+-------------+--------------+----------------+ | id | last_name | first_name | awards | |------+-------------+--------------+----------------| | 1 | Chen | Neil | Pulitzer Prize | +------+-------------+--------------+----------------+ SELECT 1 Command Time: 0.0330460071564 Format Time: 0.00148916244507 为了防止这种层叠的副作用，你可以像查询那样处理，使用 ONLY 来处理：\npigdb\u0026gt; UPDATE authors SET last_name='Chen' WHERE last_name='Geisel'; UPDATE 1 Command Time: 0.0163300037384 Format Time: 1.69277191162e-05 pigdb\u0026gt; UPDATE ONLY authors SET last_name='Chen' WHERE last_name='Geisel'; UPDATE 0 Command Time: 0.0319559574127 Format Time: 2.121925354e-05 pigdb\u0026gt; select * from distinguish_authors; +------+-------------+---------------+----------------+ | id | last_name | first_name | awards | |------+-------------+---------------+----------------| | 1 | Chen | Neil | Pulitzer Prize | | 2 | Geisel | Theodor Seuss | Pulitzer Prize | +------+-------------+---------------+----------------+ SELECT 2 Command Time: 0.000549077987671 Format Time: 0.000324964523315 删除也是一样的：\npigdb\u0026gt; DELETE FROM ONLY authors WHERE last_name='Geisel'; DELETE 0 Command Time: 0.00134491920471 Format Time: 2.09808349609e-05 结语 # PostgreSQL 是一门最接近编程语言的数据库，9.4 中引入的 NOSQL 的特性，糢糊了关系数据和非关系型数据的界面，增加特性的同时，并不考虑性能；希望越来越多的人加入进来。\n","date":"2015-04-04","permalink":"/n3xtchen/2015/04/04/postgresql---inheritance/","section":"时间线","summary":"PostgreSQL 支持高级的 objdect-relational 机制，继承。继承允许一张表继承一张（或多张）表的列属性，来建立 parent-child 关系。子表可以继承父表的字段以及约束，同时可以拥有自己的字段。","title":"PostgreSQL - 继承（inheritance）"},{"content":"","date":"2015-02-27","permalink":"/n3xtchen/tags/command-line/","section":"标签","summary":"","title":"command-line"},{"content":"我喜欢使用 Python 创建命令行应用，它写起来比 Bash 脚本更有逻辑直观。\nPython 有很多库提供给你解析命令行参数和运行其它 Shell 命令，同时你还能充分利用强大的面向对象语言的优势；你还可以使用 Python 的单元测试来帮助你检验和注释你的应用。\n你可以在 github 找到这个教程的 demo。\n结构化你的应用 # 从我的经验来看，Python最好的目录结构就是将可执行脚本放到 bin 目录中，你的项目放到项目名目录下。这样，你可以分离你的核心功能，保持可复用性。这也是其它类型应用的创建标准规则。\n在 project 目录下，你应该使用 main.py 作为应用的主要访问入口。你的通用函数应该放到 lib 目录下，测试脚本放在 tests 目录下。结构如下：\n├── README.md ├── bin │ ├── project │ └── project.bat └── project ├── lib ├── main.py └── tests 你的应用可以这样执行：\n$ bin/project \u0026lt;parameters\u0026gt; 分离参数，shell 命令和功能函数 # 和所有面相对象编程一样，你应该遵循 关注点分离（SoC）原则。由于用 Python 读取命令行参数，处理选项和执行其他 Shell 命令是在它方便了，导致时常忽略了这个原则。\n解析命令行参数 # 创建一个定义和收集命令行参数的类。Python 提供了 argparse(原教程使用的旧库 optparse，目前已经被 Python 2.7 弃用)，这样你可以非常容易定义配置和行为(介绍 ArgumentParser 的基础用法)：\nfrom argparse import ArgumentParser usage = 'bin/project' self.parser = ArgumentParser(usage=usage) self.parser.add_argument('-x', '--example', default='example-value', dest='example', help='An example option') 现在你已经创建了一个解释器，它通过执行 bin/project -x \u0026lt;target value\u0026gt; 或者 bin/project --example \u0026lt;target value\u0026gt;，读取目标值到 example 变量中。\n执行其他 Shell 命令 # 如果你需要创建一个依赖于其他 Shell 命令的应用的时候，你应该从它的类中把命令抽离出来。这样，你的核心功能可以在不同环境下被复用，你也更容易收集外部代码产生的日志，错误和异常。我推荐你使用 Python 的 subprocess 来执行外部命令。\n创建一个类用来处理进程执行和异常（见 process-class）。\n核心功能 # 在 project/lib/project.py 中，你可以实现核心功能。由于这是个例子，我只包含接受参数，并通过 Process-class 执行 date 命令（见 porject.py）。\n运行 # 从 bin/project 可执行文件中调用你项目的 maim.py\n#!/bin/bash BINPATH=`dirname $0` python \u0026quot;$BINPATH/../project/main.py\u0026quot; $@ 不要忘了为它设置可执行权限：\n$ chmod 755 bin/project 在 main.py 中收集命令行参数值和运行你的应用。\nimport sys from lib import Project from lib import Options if __name__ == '__main__': options = Options() opts, args = options.parse(sys.argv[1:]) v = Project(opts) print v.date() 你现在可以执行你的应用了：\n$ bin/project \u0026lt;arguments\u0026gt; 测试 # 介于文章的主题，这里我不再强调测试的重要性。简单扼要的说，单元测试可以引导你的开发，验证功能，执行应用的行为。\n把测试添加到 project/tests 目录中。我推荐使用 nose 来运行你的测试。\n测试命令行参数 # import unittest from lib import Options class TestCommandLineArguments(unittest.TestCase): def setUp(self): self.options = Options() def test_defaults_options_are_set(self): opts, args = self.options.parse() self.assertEquals(opts.example, 'example-value') 运行测试 # $ nosetests ..... Ran 5 tests in 0.054s OK 引用自 https://coderwall.com/p/lt2kew/python-creating-your-project-structure\n","date":"2015-02-27","permalink":"/n3xtchen/2015/02/27/python---create-project-struct-of--command-line-applications/","section":"时间线","summary":"我喜欢使用 Python 创建命令行应用，它写起来比 Bash 脚本更有逻辑直观。","title":"Python 命令行应用 - 创建项目结构"},{"content":"","date":"2015-02-15","permalink":"/n3xtchen/tags/composer/","section":"标签","summary":"","title":"composer"},{"content":"","date":"2015-02-15","permalink":"/n3xtchen/tags/wordpress/","section":"标签","summary":"","title":"wordpress"},{"content":"这篇文章，我们将使用 Composer 来管理 Wordpress Core，插件以及主题。\n当你考虑构建标准的 WP 站点的时候，关于依赖（Dependencies）的最简单例子就是插件。但是你需要意识到：Wordpress 本身也是一个插件。\n首先，你得认识到 Wordpress 核心（Core）实际上是一个第三方库。这个概念对于你来说不好理解，是因为大部分 Wordpress 站点结构像下面这样的：\nindex.php license.txt readme.html wp-activate.php wp-admin wp-blog-header.php wp-comments-post.php wp-config-sample.php wp-content wp-cron.php wp-includes wp-links-opml.php wp-load.php wp-login.php wp-mail.php wp-settings.php wp-signup.php wp-trackback.php xmlrpc.php 你应该很熟悉这样的文件夹结构吧。Wordpress Core 文件就在你的项目根目录下。wp-content 中包含的是应用层面的代码，比如主题和插件。这样的结构确实很难能让人想到 Wordpress Core 是一个第三方库，而我们一般都在 wp-content 目录下编写代码和安装第三方插件。\n庆幸的是，大部人也认为这样做是不合理，应该把 Wordpress 放到他的子目录。因为我们很少去改动（实际上是不允许的，会导致升级问题）Wordpress Core 代码，像下面这样才合理：\nwp-content index.php wp wp-config.php 接着进一步该井，将 Wordpress 作为 Git 子模块。 WordPress-Skeleton 就是一个例子。你的文件结构同上，但是 wp/ 文件夹不是你的代码库的一部分，因为它是独立的子模块。\n如果你读过 Git 子模块也不是处理依赖的好方法 ，这就是为什么我们选用 composer 的原因了。\nGit 子模块请参见： Git 工具 - 子模块\n常规的 Composer 包只会被安装在 vendor/ 中，你不能为每个包选择不同的安装目录。\n当然也有可以变通的方法，否则这篇博客就不存在了。\n配置 composer.json # 在讨论变通方法之前，先看看实例：\n{ \u0026quot;repositories\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;package\u0026quot;, \u0026quot;package\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;wordpress\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;webroot\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;3.8\u0026quot;, \u0026quot;dist\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;zip\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://github.com/WordPress/WordPress/archive/3.8.zip\u0026quot; }, \u0026quot;require\u0026quot; : { \u0026quot;fancyguy/webroot-installer\u0026quot;: \u0026quot;1.0.0\u0026quot; } } } ], \u0026quot;require\u0026quot;: { \u0026quot;php\u0026quot;: \u0026quot;\u0026gt;=5.3.0\u0026quot;, \u0026quot;wordpress\u0026quot;: \u0026quot;3.8\u0026quot;, \u0026quot;fancyguy/webroot-installer\u0026quot;: \u0026quot;1.0.0\u0026quot; }, \u0026quot;extra\u0026quot;: { \u0026quot;webroot-dir\u0026quot;: \u0026quot;wp\u0026quot;, \u0026quot;webroot-package\u0026quot;: \u0026quot;wordpress\u0026quot; } } 知道 Packagist 吗？他是 PHP 插件源代码库。每一个包需要自己的源代码库。因此，如果你需要的库不在 Packagist 中怎么办呢？哈哈，你可以自定义源。\n由于 Wordpress 还不是一个正式的 Composer 包，我们需要自定义一个源:\n\u0026quot;repositories\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;package\u0026quot;, \u0026quot;package\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;wordpress\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;webroot\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;3.8\u0026quot;, \u0026quot;dist\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;zip\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://github.com/WordPress/WordPress/archive/3.8.zip\u0026quot; }, \u0026quot;require\u0026quot; : { \u0026quot;fancyguy/webroot-installer\u0026quot;: \u0026quot;1.0.0\u0026quot; } } } ] 上面这段 Json 自定义一个代码库。这就是使用 Composer 引入外部项目最灵活的方式了，你不需要给被引入的包编写 composer.json。还有另外一个小技巧，webroot-installer；它是一个自定义安装器，让我们可以为任何包定义安装路径。自定义安装需要在包类型中指定才能生效，在我们的源配置中也有用到。\n另外你需要注意的是：\n\u0026quot;extra\u0026quot;: { \u0026quot;webroot-dir\u0026quot;: \u0026quot;wp\u0026quot;, \u0026quot;webroot-package\u0026quot;: \u0026quot;wordpress\u0026quot; } 我们想要把名叫 Wordpress 的包安装在 wp 目录下；这个就是我们上述配置中实际完成的工作。\n安装 # 现在我们已经有了一个可用的 composer.json 的文件，然后我们要做的就是安装它。我们需要执行 composer install：\nLoading composer repositories with package information Installing dependencies (including require-dev) - Installing fancyguy/webroot-installer (1.0.0) Downloading: 100% - Installing wordpress (3.8) Downloading: 100% Writing lock file Generating autoload files 然后产生的文件如下：\ncomposer.json composer.lock vendor wp 这时，我按照 将WordPress安装在网站子目录 的步骤完成设置，修改 wp-content 目录路径。完成后，我们的目录结构如下所示：\n├── composer.json ├── composer.lock ├── index.php ├── vendor ├── wp ├── wp-config.php └── wp-content 更多关于包的安装 # 我之前说过的，“所有的包都会被安装到 vendor/ 目录下，并你不能改变它“，这实际上是一个谎言；因为我使用了自定义安装器改变了包的安装路径。好吧，还是有多种方式改变你的安装目录路径。但是只有包引入了 composer-installers 才能用。（记住，Wordpress Core 实现它，因为它没有一个 composer.json 文件）。\ncomposer-installers 为一个包指定它的类型和自定义的安装目录。他们已经包含了一些我们需要的类型：\n* wordpress-plugin =\u0026gt; wp-content/plugins/{$name}/ * wordpress-theme =\u0026gt; wp-content/themes/{$name}/ * wordpress-muplugin =\u0026gt; wp-content/mu-plugins/{$name}/ 因此，任何带有 wordpress-plugin 的类型的包都会默认被安装到 wp-content/plugins/{$name}/ 目录中。\n插件(Plugins) # 因此，我们已经安装好了 Wordpress，现在我们想安装一些插件。多亏了有 composer-installers，我们可以把他们安装到正确的位置，不是吗？\n是的，但是插件需要有一个 composer.josn，引入 composer-installers，并且它们包的类型要设置正确。明显，目前这样的包不是很多。我们为你找一个， wordpress-monolog。\n让我们把它添加到我们的 composer.josn 中：\n\u0026quot;require\u0026quot;: { \u0026quot;php\u0026quot;: \u0026quot;\u0026gt;=5.3.0\u0026quot;, \u0026quot;wordpress\u0026quot;: \u0026quot;3.8\u0026quot;, \u0026quot;fancyguy/webroot-installer\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;fancyguy/wordpress-monolog\u0026quot;: \u0026quot;dev-master\u0026quot; } 然后我们执行 composer update，你将会看到 wordpress-monolog 被安装到 wp-content/plugins/wordpress-monolog。\nWordPress Packagist # 幸运的是，有一个叫做 wpackagist（由 http://outlandish.com/ 维护），里面的包都针对 wordpress 优化过的插件包。\n这个站点提供了支持 Composer 的 WordPress 插件库镜像。 基本上，它们保存了所有的 WordPress 插件，并加上了 composer.josn，包含 composer-installers，并且指定为 wordpress-plugin 包类型。\n它的用法也简单：\n把源添加到你的 composer.josn 中 添加你想要的插件到 require 中，使用 wpackagist 作为 vendor 名 然后执行 composer update 插件将会安装到 wp-content/plugins/ 中。 添加几个插件后，我们的 composer.josn 最终会变成这样：\n{ \u0026quot;repositories\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;composer\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;http://wpackagist.org\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;package\u0026quot;, \u0026quot;package\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;wordpress\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;webroot\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;3.8\u0026quot;, \u0026quot;dist\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;zip\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://github.com/WordPress/WordPress/archive/3.8.zip\u0026quot; }, \u0026quot;require\u0026quot;: { \u0026quot;fancyguy/webroot-installer\u0026quot;: \u0026quot;1.0.0\u0026quot; } } } ], \u0026quot;require\u0026quot;: { \u0026quot;php\u0026quot;: \u0026quot;\u0026gt;=5.3.0\u0026quot;, \u0026quot;wordpress\u0026quot;: \u0026quot;3.8\u0026quot;, \u0026quot;fancyguy/webroot-installer\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;fancyguy/wordpress-monolog\u0026quot;: \u0026quot;dev-master\u0026quot;, \u0026quot;wpackagist/advanced-custom-fields\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;wpackagist/posts-to-posts\u0026quot;: \u0026quot;1.4.x\u0026quot; }, \u0026quot;extra\u0026quot;: { \u0026quot;webroot-dir\u0026quot;: \u0026quot;wp\u0026quot;, \u0026quot;webroot-package\u0026quot;: \u0026quot;wordpress\u0026quot; } } 结语 # 混沌代码布局，现在总算抽离开了。看起来舒服多了，难道不是吗？至少对于我来说，是的！\nhttp://roots.io/using-composer-with-wordpress/\n","date":"2015-02-15","permalink":"/n3xtchen/2015/02/15/wordpress-n-composer/","section":"时间线","summary":"这篇文章，我们将使用 Composer 来管理 Wordpress Core，插件以及主题。","title":"使用 composer 管理 wordpress 应用包依赖"},{"content":"","date":"2015-01-11","permalink":"/n3xtchen/tags/microframework/","section":"标签","summary":"","title":"microframework"},{"content":"","date":"2015-01-11","permalink":"/n3xtchen/tags/pyramid/","section":"标签","summary":"","title":"pyramid"},{"content":"此教程为你展示 Pyramid 开发 Web 应用的基本步骤。文章很短，使用尽可能少的代码实现一个待办事务（todo）应用。为了简洁，这里采用 Pyramid 的单文件应用模式来开发。\ntodo 应用将具有如下几个特点：\n提供任务列表，插入和关闭任务界面 使用路由模式来映射 URLs 到对应的视图函数 使用 Mako 模版来渲染你的视图 使用 SQLite 作为你的后端数据库 准备工作 # 你需要的如下包：\nPyramid 框架 mako 模版扩展（add-on） 如果你没有，你可以通过 pip 命令进行安装：\npip install pyramid pyramid_mako 第一步 - 组织你的应用结构 # 我们将为我们的应用创建如下目录结构：\ntasks/ ├── static └── templates 第二步 - 配置你的应用 # 首先，我们将在 tasks 目录下创建一个 tasks.py 文件，并添加如下代码：\nimport os import logging from pyramid.config import Configurator from pyramid.session import UnencryptedCookieSessionFactoryConfig from wsgiref.simple_server import make_server 然后，我们配置日志记录方式，并把当前目录设置项目根目录：\nlogging.basicConfig() log = logging.getLogger(__file__) here = os.path.dirname(os.path.abspath(__file__)) 最后，开始编写我们的应用主体（__main__）； 传递配置选项（settings）和会话工厂（session_factory）给 Pyramid，并创建和运行 WSGI 应用：\nif __name__ == '__main__': # 配置选项设置 settings = {} settings['reload_all'] = True settings['debug_all'] = True # 会话配置 session_factory = UnencryptedCookieSessionFactoryConfig('itsaseekreet') # 配置 config = Configurator(settings=settings, session_factory=session_factory) # 启动应用 app = config.make_wsgi_app() server = make_server('0.0.0.0', 8080, app) server.serve_forever()\t关于 Session，详见 会话配置\n现在，我们就有一个可以运行我们应用的基础项目架构，但我们还需要加入数据库，路由，视图和模版的支持。\n第三步 - 数据库设计和应用初始化 # 为了让事情简单点，我们将使用 SQLite 数据库（因为它在大部分操作系统中都默认安装）。\n我们任务的数据结构很简单：\nid：记录的唯一标识 name：不超过 100 个字符的任务名 close：任务是否关闭 在 tasks 目录下添加一个文件，命名为 schema.sql，内容如下：\n/** 建表和初始化数据 */ create table if not exists tasks ( id integer primary key autoincrement, name char(100) not null, closed bool not null ); insert or ignore into tasks (id, name, closed) values (0, '开始 Pyramid', 0); insert or ignore into tasks (id, name, closed) values (1, '快速入门', 0); insert or ignore into tasks (id, name, closed) values (2, '来杯咖啡吧!', 0); 在上一步创建的 tasks.py 文件顶部引入库，内容如下：\nimport sqlite3 from pyramid.events import subscriber, \\ NewRequest, ApplicationCreated 为了让创建数据库的过程稍微简单点，我们将 subscribe 一个 Pyramid 事件来完成执行数据库创建和初始化。把该函数挂载到 ApplicationCreated 事件，每一次我们启动应用的时候，我们将会触发该函数。因此，我们数据库将会在应用启动之后被创建，必要时会被更新。\n@subscriber(ApplicationCreated) def application_created_subscriber(event): log.warn('Initializing database...') with open(os.path.join(here, 'schema.sql')) as f: stmt = f.read() settings = event.app.registry.settings db = sqlite3.connect(settings['db']) db.executescript(stmt) 我们还需要为我们应用设置数据库连接串。我们将提供一个连接对象作为应用请求的属性。关联 NewRequest 事件，当 Pyramid 请求开始的时候，我们将会初始化数据连接。你可以通过 request.db 来访问创建的数据库连接。在请求周期的最后使用 request.add_finished_callback 方法，来关闭数据库连接。\n@subscriber(NewRequest) def new_request_subscriber(event): request = event.request settings = request.registry.settings request.db = sqlite3.connect(settings['db']) request.add_finished_callback(close_db_connection) def close_db_connection(request): request.db.close() 为了使这些代码可用，我们将要在配置中指定数据的位置和使用 config.scan() 确保我们的 @subscriber 装饰器在应用运行时被扫描到。\nif __name__ == '__main__': ... settings['db'] = os.path.join(here, 'tasks.db') ... config.scan() ... 目前为止，我们已经为我们的应用添加了创建和访问数据机制。\nPyramid 事件机制 # 事件（event ）存在于 Pyramid 应用的整个生命周期。大部分 Pyramid 应用不需要应用，但是它在某些场景下将很有用（就像我们这个例子一样，在应用启动的时候，初始化数据库）。 事件函数结构：\ndef mysubscriber(event): print(event) 然后添加到配置中：\nconfig.add_subscriber(mysubscriber, NewRequest) # 将 `mysubscriber` 挂载到 `NewRequest` 事件中。 而本文使用的是装饰器进行挂载，使用如下事件：\nApplicationCreated：应用创建时触发 NewRequest：新请求进入时触发 注意：使用装饰器，需要使用 config.scan() 来扫描挂载事件的函数。 详见：http://docs.pylonsproject.org/docs/pyramid/en/latest/narr/events.html\n第四步 - View 函数的基本 Web 操作 # 开始这个步骤之前，我们需要了解关于 View 的尝试。为了实现一个 View，我们需要完成如下步骤：\n编写 View 函数 将 View 函数登记到应用配置中 使用路由（Route）将指定的 Url 模式映射到 View 函数中 根据配置生成 wsgi 应用 看看，一个例子：\nfrom pyramid.response import Response from pyramid.view import view_config from pyramid.config import Configurator # 1. 编写 View 函数，route_name 用于路由匹配 URL 的（add_route） @view_config(route_name='hello') def hello(): return Response('\u0026lt;body\u0026gt;Visit \u0026lt;a href=\u0026quot;/howdy\u0026quot;\u0026gt;hello\u0026lt;/a\u0026gt;\u0026lt;/body\u0026gt;') ...\t# 此处省略基础配置\tconfig = Configurator(settings=settings) config.add_route('hello', '/')\t# 3. 使用路由（Route）将指定的 Url 模式映射到 View 函数中 config.scan()\t# 2. 将 View 函数登记到应用配置中 config.make_wsgi_app()\t# 4. 根据配置生成 wsgi 应用 详见 http://docs.pylonsproject.org/docs/pyramid/en/latest/narr/viewconfig.html\n现在，继续我们的应用。首先，我们先导入一些需要的库。\n... from pyramid.exceptions import NotFound from pyramid.httpexceptions import HTTPFound from pyramid.view import view_config ...\t我们现在开始实现任务列表和任务的添加与关闭。\n任务列表 # 使用这个视图用来展示所有未完成的任务实体。view_config 的可选参数 renderder 告诉视图返回的数据字典使用指定的模版文件来渲染。视图函数的返回值必须是字典。任务列表的视图从数据库中查询处未完成的任务，使用字典返回数据，提供给 list.mako 模版文件：\n@view_config(route_name='list', renderer='list.mako') def list_view(request): rs = request.db.execute(\u0026quot;select id, name from tasks where closed = 0\u0026quot;) tasks = [dict(id=row[0], name=row[1]) for row in rs.fetchall()] return {'tasks': tasks} 新增任务 # 这个视图让用户创建新的任务。如果在表单中提供了一个 name，一个任务就创建。然后生成一个信息在接下来的请求中显示，用户的浏览器将被从定向到 list_view 中。如果没东西提供，将会产生一条提醒消息，在 new_view 中显示。\n@view_config(route_name='new', renderer='new.mako') def new_view(request): if request.method == 'POST':\t＃ 是否是 POST 请求 if request.POST.get('name'):\t＃ Post 请求过来的 name 字段是否存在 request.db.execute( 'insert into tasks (name, closed) values (?, ?)', [request.POST['name'], 0]) request.db.commit() request.session.flash('New task was successfully added!') # 重定向到 list 页面 return HTTPFound(location=request.route_url('list')) else: request.session.flash('Please enter a name for the task!') return {} request 对象 # 这里 view 函数 new_view 接受一个参数 request。request 对象是 WSGI 环境变量字典对象的 Pyramid 封装。下面是 request 一些常见的属性：\nreq.method\n请求方法, e.g., \u0026lsquo;GET\u0026rsquo;, \u0026lsquo;POST\u0026rsquo;\nreq.GET\n查询字符串的字典\nreq.POST\n请求体（request body）的字典。只有在请求方法是 POST 获取表单提交的时候可用。\nreq.params\n融合 req.GET 和 req.POST 字典。\nreq.body\n请求体的内容。它包含所有请求内容，以字符形式存储。当一个 POST 请求不是表单提交或者一个 PUT 请求时，它就变的很有用。你还可以通过 req.body_file 获取，它以文件形式存储。\nreq.cookies\ncookie 字典。\nreq.headers\n所有头信息的字典，大小写敏感。\nreq.session 如果会话工厂被配置（回顾下第二部的会话配置，session_factory），这个属性就会返回当前用户的 session 对象；否则被调用的时候会抛出 pyramid.exceptions.ConfigurationError 异常。\n另外我们还使用一个 request 的方法：\nroute_url：它根据 Pyramid 的路由生成与之对应的全路径 URL。先来看个例子：\n@view_config(route_name='hello') def hello(): return Response('\u0026lt;body\u0026gt;Visit \u0026lt;a href=\u0026quot;/howdy\u0026quot;\u0026gt;hello\u0026lt;/a\u0026gt;\u0026lt;/body\u0026gt;') @view_config(route_name='use_route_url') def use_route_url(request): return HTTPFound(location=**request.route_url('hello')**) ...\tconfig.add_route('hello', '/he/ll/o') 看看 request.route_url('hello')，这是实际上 request.route_url 根据 config 中路由中查找路由名为 hello 的 view 函数，然后找到它匹配的 url（/he/ll/o）并返回，这是它返回的是 http://your.domain.com/he/ll/o（ http://your.domain.com 你应用所属的域名）\n详见 http://docs.pylonsproject.org/projects/pyramid/en/1.0-branch/narr/webob.html\nFlash Messages（信息） # 闪存信息是一个简单的存储在会话中的消息队列。\n闪存信息有两个主要用途：\n内部跳转后只显示一次状态信息 使用通用代码来为单次显示记录日志，而不用定向到一个 HTML 模版。 用法如下：\n\u0026gt;\u0026gt;\u0026gt; request.session.flash('info message') \u0026gt;\u0026gt;\u0026gt; request.session.pop_flash()\t# 我们将在模版文件中使用 ['info message'] \u0026gt;\u0026gt;\u0026gt; request.session.pop_flash() []\t详见 http://docs.pylonsproject.org/docs/pyramid/en/latest/narr/sessions.html#using-the-default-session-factory\n关闭任务 # @view_config(route_name='close') def close_view(request): task_id = int(request.matchdict['id']) request.db.execute(\u0026quot;update tasks set closed = ? where id = ?\u0026quot;, (1, task_id)) request.db.commit() request.session.flash('Task was successfully closed!') return HTTPFound(location=request.route_url('list')) NotFound 视图 # 使用自己的模版自定义默认的 NotFound 页面。当一个 URL 找不到映射的 View 函数的时候，将会被定向到 NotFound 页面。这里，我们通过自己编写的 View 函数来覆盖框架自带的 404 页面。\n@view_config(context='pyramid.exceptions.NotFound', renderer='notfound.mako') def notfound_view(request): request.response.status = '404 Not Found'\t添加路由 # 我们最后需要添加一些路由配置到应用高配置中，用来映射视图函数和 URL 之间的关系。\n# routes setup config.add_route('list', '/') config.add_route('new', '/new') config.add_route('close', '/close/{id}') 第五步 - 视图模版 # 视图可以工作，但是他们需要渲染成浏览器可以理解的语言：HTML。我们已经看到在我们的视图配置中接受一个 renderer 参数。我们将使用 Pyramid 框架常见模版引擎之一：Mako Templates。\n我们夜间个使用 Mako 模版的继承机制。模版继承使得多个模版共用一套模版，易于前端布局的维护和保持一致。\n我发现不错的 Mako 入门译文，看完它，你几乎可以掌握 Mako， Python模板库Mako的语法，这里不展开讲了。\nlayout.mako # 这个模版包含与其他模版共享的基本布局结构。在 body 标签中，我们定义一个块来显示应用发送的提醒信息，另一个用来显示使用 Mako 命令 ${next.body} 继承这个主布局的页面内容。\n# -*- coding: utf-8 -*- \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Pyramid Task's List Tutorial\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026quot;author\u0026quot; content=\u0026quot;Pylons Project\u0026quot;\u0026gt; \u0026lt;link rel=\u0026quot;shortcut icon\u0026quot; href=\u0026quot;/static/favicon.ico\u0026quot;\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;/static/style.css\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; % if request.session.peek_flash(): \u0026lt;div id=\u0026quot;flash\u0026quot;\u0026gt; \u0026lt;% flash = request.session.pop_flash() %\u0026gt; % for message in flash: ${message}\u0026lt;br\u0026gt; % endfor \u0026lt;/div\u0026gt; % endif \u0026lt;div id=\u0026quot;page\u0026quot;\u0026gt; ${next.body()} \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; list.mako # # -*- coding: utf-8 -*- \u0026lt;%inherit file=\u0026quot;layout.mako\u0026quot;/\u0026gt; \u0026lt;h1\u0026gt;Task's List\u0026lt;/h1\u0026gt; \u0026lt;ul id=\u0026quot;tasks\u0026quot;\u0026gt; % if tasks: % for task in tasks: \u0026lt;li\u0026gt; \u0026lt;span class=\u0026quot;name\u0026quot;\u0026gt;${task['name']}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;actions\u0026quot;\u0026gt; [ \u0026lt;a href=\u0026quot;${request.route_url('close', id=task['id'])}\u0026quot;\u0026gt;close\u0026lt;/a\u0026gt; ] \u0026lt;/span\u0026gt; \u0026lt;/li\u0026gt; % endfor % else: \u0026lt;li\u0026gt;There are no open tasks\u0026lt;/li\u0026gt; % endif \u0026lt;li class=\u0026quot;last\u0026quot;\u0026gt; \u0026lt;a href=\u0026quot;${request.route_url('new')}\u0026quot;\u0026gt;Add a new task\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; new.mako # # -*- coding: utf-8 -*- \u0026lt;%inherit file=\u0026quot;layout.mako\u0026quot;/\u0026gt; \u0026lt;h1\u0026gt;Add a new task\u0026lt;/h1\u0026gt; \u0026lt;form action=\u0026quot;${request.route_url('new')}\u0026quot; method=\u0026quot;post\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;text\u0026quot; maxlength=\u0026quot;100\u0026quot; name=\u0026quot;name\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;submit\u0026quot; name=\u0026quot;add\u0026quot; value=\u0026quot;ADD\u0026quot; class=\u0026quot;button\u0026quot;\u0026gt; \u0026lt;/form\u0026gt; notfound.mako # # -*- coding: utf-8 -*- \u0026lt;%inherit file=\u0026quot;layout.mako\u0026quot;/\u0026gt; \u0026lt;div id=\u0026quot;notfound\u0026quot;\u0026gt; \u0026lt;h1\u0026gt;404 - PAGE NOT FOUND\u0026lt;/h1\u0026gt; The page you're looking for isn't here. \u0026lt;/div\u0026gt; 配置模版路径 # 为了视图可以通过模版名来找到相应的模版，我们现在需要指定模版的位置：\n... settings['mako.directories'] = os.path.join(here, 'templates') ... # add mako templating config.include('pyramid_mako') ... 第六步 － 添加样式 # 现在是时候为你的应用添加样式了。我们在 static 中新建一个名为 style.css 的文件，内容如下：\nbody { font-family: sans-serif; font-size: 14px; color: #3e4349; } h1, h2, h3, h4, h5, h6 { font-family: Georgia; color: #373839; } a { color: #1b61d6; text-decoration: none; } input { font-size: 14px; width: 400px; border: 1px solid #bbbbbb; padding: 5px; } .button { font-size: 14px; font-weight: bold; width: auto; background: #eeeeee; padding: 5px 20px 5px 20px; border: 1px solid #bbbbbb; border-left: none; border-right: none; } #flash, #notfound { font-size: 16px; width: 500px; text-align: center; background-color: #e1ecfe; border-top: 2px solid #7a9eec; border-bottom: 2px solid #7a9eec; padding: 10px 20px 10px 20px; } #notfound { background-color: #fbe3e4; border-top: 2px solid #fbc2c4; border-bottom: 2px solid #fbc2c4; padding: 0 20px 30px 20px; } #tasks { width: 500px; } #tasks li { padding: 5px 0 5px 0; border-bottom: 1px solid #bbbbbb; } #tasks li.last { border-bottom: none; } #tasks .name { width: 400px; text-align: left; display: inline-block; } #tasks .actions { width: 80px; text-align: right; display: inline-block; } 为了让你的静态文件可以被使用，我们修添加一个 “static view” 到你的应用配置中：\n... config.add_static_view('static', os.path.join(here, 'static')) ... 第七步 - 运行你的应用 # 我们已经完成全部的步骤。运行它之前，我们先预览一遍我们的主程序 tasks.py：\nimport os import logging import sqlite3 from pyramid.config import Configurator from pyramid.events import NewRequest from pyramid.events import subscriber from pyramid.events import ApplicationCreated from pyramid.httpexceptions import HTTPFound from pyramid.session import UnencryptedCookieSessionFactoryConfig from pyramid.view import view_config from wsgiref.simple_server import make_server logging.basicConfig() log = logging.getLogger(__file__) here = os.path.dirname(os.path.abspath(__file__)) # views @view_config(route_name='list', renderer='list.mako') def list_view(request): rs = request.db.execute(\u0026quot;select id, name from tasks where closed = 0\u0026quot;) tasks = [dict(id=row[0], name=row[1]) for row in rs.fetchall()] return {'tasks': tasks} @view_config(route_name='new', renderer='new.mako') def new_view(request): if request.method == 'POST': if request.POST.get('name'): request.db.execute( 'insert into tasks (name, closed) values (?, ?)', [request.POST['name'], 0]) request.db.commit() request.session.flash('New task was successfully added!') return HTTPFound(location=request.route_url('list')) else: request.session.flash('Please enter a name for the task!') return {} @view_config(route_name='close') def close_view(request): task_id = int(request.matchdict['id']) request.db.execute(\u0026quot;update tasks set closed = ? where id = ?\u0026quot;, (1, task_id)) request.db.commit() request.session.flash('Task was successfully closed!') return HTTPFound(location=request.route_url('list')) @view_config(context='pyramid.exceptions.NotFound', renderer='notfound.mako') def notfound_view(request): request.response.status = '404 Not Found' return {} # subscribers @subscriber(NewRequest) def new_request_subscriber(event): request = event.request settings = request.registry.settings request.db = sqlite3.connect(settings['db']) request.add_finished_callback(close_db_connection) def close_db_connection(request): request.db.close() @subscriber(ApplicationCreated) def application_created_subscriber(event): log.warn('Initializing database...') with open(os.path.join(here, 'schema.sql')) as f: stmt = f.read() settings = event.app.registry.settings db = sqlite3.connect(settings['db']) db.executescript(stmt) db.commit() if __name__ == '__main__': # configuration settings settings = {} settings['reload_all'] = True settings['debug_all'] = True settings['mako.directories'] = os.path.join(here, 'templates') settings['db'] = os.path.join(here, 'tasks.db') # session factory session_factory = UnencryptedCookieSessionFactoryConfig('itsaseekreet') # configuration setup config = Configurator(settings=settings, session_factory=session_factory) # add mako templating config.include('pyramid_mako') # routes setup config.add_route('list', '/') config.add_route('new', '/new') config.add_route('close', '/close/{id}') # static view setup config.add_static_view('static', os.path.join(here, 'static')) # scan for @view_config and @subscriber decorators config.scan() # serve app app = config.make_wsgi_app() server = make_server('0.0.0.0', 8080, app) server.serve_forever() 我们执行它把：\n$ python tasks.py WARNING:tasks.py:Initializing database... 它默认监听 8080 端口\n结语 # 本来只想翻译一下文章，突然发现过于介绍的简单，所以又加入了不少的东西。关于 Pyramid 的文章真的太少，中文的就更少了！本文跟着作者的思路，难以下笔，只能忍痛把它结掉，如果有任何问题或者建议，请赐教。下一篇将以完全我的视角来为大家介绍 Pyramid 的迷人特性。\n参考 http://docs.pylonsproject.org/projects/pyramid-tutorials/en/latest/single_file_tasks/single_file_tasks.html\n","date":"2015-01-11","permalink":"/n3xtchen/2015/01/11/pyramid---todo-simple/","section":"时间线","summary":"此教程为你展示 Pyramid 开发 Web 应用的基本步骤。文章很短，使用尽可能少的代码实现一个待办事务（todo）应用。为了简洁，这里采用 Pyramid 的单文件应用模式来开发。","title":"Pyramid - 一个文件实现 Todo 应用"},{"content":"","date":"2015-01-06","permalink":"/n3xtchen/tags/rbenv/","section":"标签","summary":"","title":"rbenv"},{"content":" 译自 https://github.com/sstephenson/rbenv/wiki/Why-rbenv%3F\nrbenv 能做… # 提供应用级别（application-specific）的 Ruby 版本控制； 为每个登录用户设置不同的全局 Ruby 版本； 允许你使用环境变量来覆盖系统的 Ruby 版本。 和 RVM 不同的是，rbenv 不需要做… # 需要重载到 Shell。 而 rbenv 提供一个更简便的方法，只需要把路径添加到 $PATH 变量中即可。 覆盖 Shell 命令（如 cd）或者需要及时破解（prompt hacks）。这样做是很危险的，而且容易出错（error-prone）。 需要配置文件。除了配置你所需要的 Ruby 版本外，你不需要做其他任何事情。 安装 Ruby。你可以自己编译安装 Ruby，或者使用 ruby-build 自动化这个安装过程； 管理 gemsets。Bundler 是管理应用依赖的最好方式哦。如果你的项目还没使用 Bundler的话，你可以通过安装 rbenv-gemset 来使用这个特性。 为了兼容，需要改变 Ruby 库。rbenv 的简单之处就在于，一旦你把它设置到环境变量之后，其它你都不需要管了。 ","date":"2015-01-06","permalink":"/n3xtchen/2015/01/06/ruby---why-rbenv/","section":"时间线","summary":"译自 https://github.","title":"Ruby - Why rbenv?"},{"content":" 02. Python Pyramid 应用包 # 原文见 quick_tutorial-package\n大部分现代的 Python 项目都使用 Python 包（package）作为开发工具，Pyramid 也能很好的利用这一工具。这个章节，我们将构建一个迷你的 Python 包来实现上一个章节的 Hello World 应用。\n知识背景 # Python 开发者可以将模块和文件集合打包到一个命名空间单元中（我们称之为 Package）。如果一个目录存在于 sys.path 中，并且其中包含一个名叫 init.py 的特殊文件，那这个目录就会被当作一个 Python 包（Package，后续都用这个名称来指代包）。\n我们可以把自己的应用封装成供后续安装的 Package，通过一种工具链（toolchain，Python 使用的通用工具叫做 setuptools，，虽然相对较为混乱，但是已经在不断的完善中。 ）来安装，前提是你需要编写一个 setup.py 文件来控制安装流程。为了不让你抓狂，整个章节将会为围绕这个主题进行讲解。接下来的教程，你将学到：\n我们将为后续的每一个步骤创建一个文件夹作为 setuptool 项目 这个项目将会包含一个 setup.py，它将把 setuptool 的工程特性到目录中 这个项目中，我们将会使用 init.py 把教程的子目录封装成 Package 我们可以使用 python setup.py develop 在开发模式下安装我们的项目 接下来：\n你将在 Package 中进行开发 这个包将会是 setuptool 项目的一部分 目标 # 创建一个带 init.py 的 Python 包目录 编写 setup.py 生成一个最小的 Python 项目 开发者模式下安装我们的教程目录 步骤 # 为这个章节，腾个空间，创建一个目录：\n$ cd ..; mkdir package; cd package 创建 package/setup.py 文件，并输入如下内容：\nfrom setuptools import setup requires = [ 'pyramid', ] setup(name='tutorial', install_requires=requires, ) 安装新的项目开始进行开发，然后创建一个目录用来保存实际代码：\n$ $VENV/bin/python setup.py develop $ mkdir tutorial 在 package/tutorial/init.py 中输入如下：\n# package 在 package/tutorial/app.py 中输入如下：\nfrom wsgiref.simple_server import make_server from pyramid.config import Configurator from pyramid.response import Response def hello_world(request): print ('Incoming request') return Response('\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;') if __name__ == '__main__': config = Configurator() config.add_route('hello', '/') config.add_view(hello_world, route_name='hello') app = config.make_wsgi_app() server = make_server('0.0.0.0', 6543, app) server.serve_forever() 运行 WSGI 应用：\n$ $VENV/bin/python tutorial/app.py 打开浏览器访问你的应用\n分析 # Python Package 为我们提供了一个良好可组织的开发环境单元。通过 setup.py 安装的 Python 项目为我们提供了一个 setuptool 的特性（这个 case 中，我们使用本地开发模式的特性）。\n在这个章节中，我们得到一个名为 tutorial 的 Python 包。在接下来的章节中，我们都将使用这个名称，避免不必要的重复输入。\n在 tutorial 上，我们还需要一个文件来负责项目的封装和打包。这里，我们所需要的就是 setup.py 。\n其他东西都和我们这个应用中一样。我们只是简单地使用一个 setup.py 来制作一个 **Python ** Package，并使用开发者模式安装它。\n注意：我们运行应用的方式有点儿冠以。除非我们是在写教程，尝试了解程序执行的每一个步骤，否则我们不会这么做。直接在一个 Package 中运行它的模块不是一个好主意。\n","date":"2014-12-21","permalink":"/n3xtchen/2014/12/21/pyramid---quick-tutorial-2/","section":"时间线","summary":"02.","title":"pyramid 快速入门 - (2)使用 Package 开发"},{"content":"","date":"2014-12-19","permalink":"/n3xtchen/tags/mvc/","section":"标签","summary":"","title":"mvc"},{"content":" 前奏：从脚手架快速开始你的项目 # 原文见： quick_tutorial - scaffolds\n知识背景 # 这个章节将会涵盖很多内容，一次一个主题，从零开始构建。尽管是暖场，但是我们也会把它做到尽可能的美观。\n和其他 Web 开发框架一样， Pyramid 提供了很多 Scaffolds （脚手架）工具，例如生成可执行的 Python，模版（template）以及 CSS 代码等等。我们开始前，我们将会使用内建的 scaffold 生成一个 Pyramid 应用。\n目标 # 使用 Pyramid 的 pcreate 命令下查看可用的脚手架，以及创建一个新的项目 创建一个 Pyramid 应用，并在浏览器中访问 步骤 # Pyramid 的 pcreate 命令可以查看可用的脚手架：\n$ pcreate --list Available scaffolds: alchemy: Pyramid SQLAlchemy project using url dispatch starter: Pyramid starter project zodb: Pyramid ZODB project using traversal 告诉 pcreate 使用 starter 脚手架来创建我们的应用：\n$ pcreate --scaffold starter scaffolds 指定开发者模式喜下安装你的项目：\n$ cd scaffolds $ python setup.py develop 通过指定配置文件来启动我们的应用：\n$ pserver development.ini --reload Starting subprocess with file monitor Starting server in PID 80798. serving on http://0.0.0.0:6543 在浏览器中访问 http://localhost:6543\n分析 # 和从零开始构建不同的是，pcreate 使得创建一个 Pyramid 应用变得非常简单。Pyramid 自带了一些 scaffolds；并且还可以通过插件包的形式安装更多的 Pyramid 的 scaffolds。\npserver 是 Pyramid 的应用启动器，独立于你的具体业务逻辑代码之外。一旦安装了 Pyramid，pserver 就已经躺在你的 bin 目录下了。这个程序是一个 Python 模块。它需要传递一个配置文件给它（本章节中，是 development.ini）\n01: 单文件应用 # 原文见： quick_tutorial-hello_world\n快速创建 Pyramid 应用的最简单方式是什么？单文件模块。没有 Python 模块，没有 setup.py，没有其他重型代码。\n知识背景 # 近年来，微框架已经大行其道了。Microframework 只是一个营销术语，而不是技术术语。他们的开销很低：他们做的很少，你只要关心的就是你自己的代码。\n可以像单文件模块的微框架那样被使用使得 Pyramid 很特别。你可以只编写一个 Python 文件，直接执行它。同时，Pyramid 也提供了让它拓展成更大应用的解决方案。\nPython 有一个叫做 WSGI 的标准，它定义了 Python Web 应用如何与标准的服务器兼容，获取传入的请求和返回响应。大部分现在的 Python Web 框架都遵循 MVC （模块-视图-控制）的应用模式，模块中的的数据有一个视图与外部系统进行交互。\n本章节，我们将短暂一瞥 WSGI 服务器应用，请求，响应以及视图（Views）。\n目标 # 得到一个可以运行的简单 Pyramid Web 应用 为了后续的可扩展性，使用尽可能易于理解的底层 初次接触 WSGI 应用，请求，视图和响应 步骤 # 确认你的环境已经按要求安装完毕\n从创建你的工作区开始（~/projects/pyramid_quick_tutorial），这一步我们将创建一个目录：\n$ mkdir hello_world; cd hello_world 把如下代码拷贝到 hello_world/app.py：\n1. from wsgiref.simple_server import make_server 2. from pyramid.config import Configurator 3. from pyramid.response import Response 4. 5. 6. def hello_world(request): 7. print('Incoming request') 8. return Response('\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello World!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;') 9. 10. 11.if __name__ == '__main__': 12. config = Configurator() 13. config.add_route('hello', '/') 14. config.add_view(hello_world, route_name='hello') 15. app = config.make_wsgi_app() 16. server = make_server('0.0.0.0', 6543, app) 17. server.serve_forever() 运行应用：\n$ $VENV/bin/python app.py 在浏览器中访问\n分析 # 首次使用 Python 吗？如果是，那模块中的一些行需要别解释：\n第 11 行. if __name__ == 'main__'：意思就是 “在命令行执行时，从这里开始”；但该文件作为模块导入的时候，将不会被执行。 第 12-14 行. 使用 Pyramid 的配置器将特定的 URL 路由到特定的 view 代码中。 第 6-8 行. 实现发起响应的 view 脚本。 第 15-17 行. 使用 HTTP 服务器来启动 WSGI 应用。 例子所示，配置器在 Pyramid 开发过程中扮演着核心位置。通过应用配置把一个个松散耦合（loosely-coupled） 部分串联成一个应用，这就是 Pyramid 的核心思想。我们将在接下来的章节中不断的重温这个思想。\n","date":"2014-12-19","permalink":"/n3xtchen/2014/12/19/pyramid---quick-tutorial/","section":"时间线","summary":"前奏：从脚手架快速开始你的项目 # 原文见： quick_tutorial - scaffolds","title":"Pyramid 快速入门 - (1)单文件应用"},{"content":"在前一篇 如何使用命令行进行数据分析 中，我们使用几个命令来进行一个简单的数据探索，向大家揭示了 命令行 毫不逊色于专业的数据分析工具；我们可以通过管道（Pipelines）将不同的命令串联在一块（我们称之为 one-liners）进行一些复杂的数据处理，清理，探索以及建模的任务（Bash 天生就是一种胶水语言）。\n一些任务你可能只会使用一次，然而有些任务你则需要经常使用；一些任务很有针对性，另一些则可以通用化。如果你可以预见到你需要定期重复使用某个 one-liners，那你就应该把它封装成命令行工具（commandline tools）。\ncommandline tools 和 one-liners 都有它们的各自用途。较于 one-liners, Commandline Tools 有如下特点：\n你不需要记住其中的具体代码实现，你只需要知道它的用途和用法就可以了； 增加命令的可读性；尤其是你在管道中和其他命令一起使用时； 使用编程语言的好处就是你可以将代码固化在文件中。这意味着，它易于你复用。如果它可以被参数化（parameters），它甚至可以应用于同一模式的问题。而 commandline tools 具备这两个特定：它在命令行中执行，可以接受参数；因此可以一次创建多次使用。\n将 One-Liner 封装成 Shell Script # 我们使用统计单词数的程序（word counter，几乎可以算是数据分析的 Hello world 程序了，^_^）来阐述这个章节。先来看一个 one-liners；\nEx 1-1:\n$ curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt | # (1) \u0026gt; tr '[:upper:]' '[:lower:]' |\t# (2) \u0026gt; grep -oE '\\w+' |\t# (3) \u0026gt; sort |\t# (4) \u0026gt; uniq -c | # (5) \u0026gt; sort -nr | # (6) \u0026gt; head -n 10\t# (7) 6441 and 5082 the 3666 i 3258 a 3022 to 2567 it 2086 t 2044 was 1847 he 1778 of 从输出中你可以猜出它的用途，这个 one-liners 返回《哈克贝利·费恩历险记》使用最频繁的头十个词。它主要完成如下功能：\n使用 curl 下载《哈克贝利·费恩历险记》； 使用 tr 把所有文字转化成小写的； 使用 grep 把所有词分割到单独行中； 使用 sort 把所有单词根据字母顺序排序； 使用 uniq 进行去重，和频度计算； 使用 sort 根据单词出现的频度进行倒序排列； 使用 head 取出 Top10； 如果你只使用一次的话，那没什么问题。想象一下，如果你想要找出 Gutenberg 项目中每一本电子书的出现最频繁的十个词；或者从一个新闻站点分时统计热词。这时，最好把 one-liner 封装成 commandline tools。我们通过参数化，给这个 one-liner 增加一些灵活性。\n我们将会使用 Bash 语言来编写这个脚本。我们需要从 one-liner 开始，逐步地改进它。将 one-liner 封装成可复用的 commandline tool，一般需要遵循下面 6 个步骤：\n复制 one-liner 到文件中 追加该文件的执行权限 在文件头部定义 shebang，用来告诉命令行使用某个程序来执行它 去除固定的输入部分 添加参数 （可选）将该脚本的路径添加到 PATH 中，使它在任何地方都可以执行。 步骤 1：复制和黏贴 # 第一步为了创建的心文件。打开你最喜欢的编辑器或者 IDE，复制黏贴我们的 one-liner，保存文件名 top-words-1.sh（1 代表我们目前处在第一步，你可以选择你换的名称，但是最好该名称能表达一定的含义，让使用者看出它的用途）。\n$ cat top-words-1.sh curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt | # (1) tr '[:upper:]' '[:lower:]' |\tgrep -oE '\\w+' | sort | uniq -c | sort -nr | head -n 10 现在我们使用 bash 命令执行它，它的结果和 Ex 1-1 是相同的：\n$ bash top-words-1.sh 6441 and 5082 the 3666 i 3258 a 3022 to 2567 it 2086 t 2044 was 1847 he 1778 of 第一步，已经帮我节省下次使用该命令码代码的时间了。但是因为它不能自我执行，需要通过 bash 命令来执行，所以它还不是严格意义上的 commandline tool。这就是我们下一步要做的；\n步骤 2: 追加该文件的执行权限 # 我们不能直接执行它的原因是我们没有响应的访问权限。一般来说，你，作为一个用户，你需要有执行文件的权限。\n在完成这个操作之前，我们需要为这个步骤复制一份文件：\n$ cp top-words-{1,2}.sh 当你完成所有步骤的时候，你就可以清晰地看出每一个步骤的差异，便于你加深印象；后续的几个步骤都会复制前一步骤的文件。好了，现在开始步骤2 ：\n$ chmod u+x top-words-2.sh chmod，命令行工具，字面意思 change mode；故名思义，变更文件模式。u+x 包含三个字符：\nu，表示你想要修改是该文件拥有者的权限；这里的 u 指的就是你，因为你创建了它； +，就更简单，代表要追加权限的动作； x，代表执行（execute）的意思； 这个整个命令连贯起来，就是你为你创建的文件（top-words-2.sh）添加执行的权限；为谁？文件的创建者，就是你。先来我们查看步骤 1 和 2 的权限：\n$ ls -l top-words-{1,2}.sh -rw-rw-r-- 1 vagrant vagrant 145 Jul 20 23:33 top-words-1.sh -rwxrw-r-- 1 vagrant vagrant 143 Jul 20 23:34 top-words-2.sh 第一列展示的就是每个文件的访问权限。现在我们解读下 top-words-2.sh 的访问权限；\n-rwxrw-r\u0026ndash;:\n第一个字符 -，知名文件类型：- 表示常规文件，d 代表文件（directory）。 第二到第四个字符，代表文件拥有者的访问权限，r 代表读 read，w 代表写 write，x 代表执行 execute；你可能注意到了，top-words-1.sh 对应的 x 的位置是 -，表示我们不能直接执行它。 第五到第七个字符，也是对应的访问权限，但是他设置的一个分组的权限，Unix/Linux 中每个用户都有组的概念，便于用户管理，你可能注意到 ls 命令还显示其他东西，其中第三列就是该文件的拥有者的用户名（vagrant，第二到第四个字符，就是为这个用户设置权限），第四列是该文件所属于的用户组（vagrant），rw- 代表该分组的用户只有读写权利，而没有执行权限。 剩下的三个字符知道所有其他用户对该文件的权限，这里他们只有读的权限。 设好执行权限，现在我们可以这么执行文件：\n$ ./top-words-2.sh 6441 and 5082 the 3666 i 3258 a 3022 to 2567 it 2086 t 2044 was 1847 he 1778 of 执行同样命令，我又可以少输 4 个字符（bash），如果你需要在未来执行它成千上万次，那就节省很可观的输入次数。\n注意到 ./ 了吗？这个代表你和该执行文件处在同一个目录下，你可以这样执行，如果切换到其他目录的时候，就需要你使用绝对路径和相对路径来执行它，否则将直接抛出文件不存在的错误（步骤 6 将会为你介绍如何创建全局访问的命令行工具）：\n$ pwd ~/example/01/ $ cd ~ $ ./top-words-2.sh -bash: ./top-words-2.sh: No such file or directory $ ~/example/01/top-words-2.sh 6441 and 5082 the 3666 i 3258 a 3022 to 2567 it 2086 t 2044 was 1847 he 1778 of 步骤 3: 在文件头部定义 shebang，用来告诉命令行使用某个程序来执行它 # shebang（也称为Hashbang）是一个由井号（she）和叹号（bang）构成的字符序列（#!），其出现在文本文件的第一行的前两个字符。\n如果忽略它，将不是一个好主意，就像我们上一步的做法那样，因为脚本行为将会是未定义。由于我们使用命令行终端的默认脚本是 /bin/bash，所以我们可以不添加 shebang，就能正确的执行。\n当时考虑到脚本可能执行在不同的环境下（例如 zsh），执行的过程就会出现问题。因此，需要显性定义它：\n#!/usr/bin/env bash\t# 这行就是 shebang 定义 curl -s http://www.gutenberg.org/cache/epub/76/pg76.txt | tr '[:upper:]' '[:lower:]' | grep -oE '\\w+' | sort | uniq -c | sort -nr | head -n 10 添加这一行告诉命令行，使用 bash 来执行下面的脚本。将来，你可能使用 perl，python，Ruby 或者 R 等等来编写你的命令行行工具，这是你就可以为它们各自追加 shebang 来保证脚本使用正确解释器来运行。\n详细介绍见： http://zh.wikipedia.org/wiki/Shebang\n步骤 4: 去除固定的输入部分 # 目前为止，我们已经写好了一个有效的 commandline tool，但是还有很多需要优化的，我们需要让我们的程序复用度更高。\n我们的脚本文件的第一行是 curl 命令，用来下载用来统计词频度的文本文件，因此数据和操作被混合在一块。\n但是如果我们希望统计其他电子书的话，或者其他站点的文本呢？工具本身的输入数据是固定不变的。如果能将数据抽离出去，那复用度将会更高。\n我们假设使用这个命令用户将要提供一个文本文件，数据和操作分离的行为将会更有效。因此，第一种方案我们只需要简单讲 curl 命令去掉就可以了。下面是修改后的脚本：\n#!/usr/bin/env bash tr '[:upper:]' '[:lower:]' | grep -oE '\\w+' | sort | uniq -c | sort -nr | head -n 10 假设我们已经把电子书存储到 data/finn.txt 文件中，我们就可以这样调用自述统计脚本：\n$ cat data/ | ./top-words-4.sh 我们把数据抽离出来，作为标准的输入流传入脚本中，那它就能够正常工作。\n步骤 5: 添加参数 # 现在，我们将我们的脚本参数化。在我们的脚本中，我们有一些固定的命行参数－举个例子，sort 的 -nr 以及 head 的 -n 10。如果脚本可以定义 top n 的排行，那将会很有用；修改见如下：\n#!/usr/bin/env bash NUM_WORDS=\u0026quot;$1\u0026quot;\t（1） tr '[:upper:]' '[:lower:]' | grep -oE '\\w+' | sort | uniq -c | sort -nr | head -n $NUM_WORDS\t（2） 变量 NUM_WORDS 值设置成 $1，他是 bash 的特殊变量，它保存命令行传入的第一个参数的值。 使用美元符号（$）＋变量名来获取该变量值 现在如果你想要查看前五个被频繁的词，我们需要使用如下方式调用：\n$ cat data/finn.txt | top-words-5.sh 5 如果没提供一个给这个命令，将会返回一个错误信息。\n$ cat data/finn.txt | top-words-5.sh head: option requires an argument -- 'n' Try 'head --help' for more information. Note: 参数化带来的好处是增加脚本的灵活度，但是过度的参数化，将会获取相反的效果；参数过多的情况使用默认值来屏蔽一些不常用于设置的参数。\n步骤 6: （可选）将该脚本的路径添加到 PATH 中，使它在任何地方都可以执行。 # 现在已经完成了脚本的编写，得到一个可服用的命令行工具。然而你使用脚本（跨目录或跨用）的时候，还要带上屯长的路径，会用户非常困扰。这一步，让你像调系统命令的一样调用的你代码（例如，你可以在任何目录，调用 ls，而不需要知道 ls 脚本位于哪一个目录），这样是不是更爽，少敲了不少字符。\n首先，我们需要知道 Bash 如何寻找你命令行工具；它实际上是通过轮训存储在 PATH 环境变量中的目录来完成的。\n$ echo $PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/loc al/games:/home/vagrant/tools:/usr/lib/go/bin:/home/vagrant/.go/bin:/home/vagrant /.data-science-at-the-command-line/tools:/home/vagrant/.bin 这个是我的 PATH 变量值，目录之间使用冒号隔开。为了使它更直观，做一些格式化：\n$ echo $PATH | tr : '\\n' | sort /bin /home/vagrant/.bin /home/vagrant/.data-science-at-the-command-line/tools /home/vagrant/.go/bin /home/vagrant/tools /sbin /usr/bin /usr/games /usr/lib/go/bin /usr/local/bin /usr/local/games /usr/local/sbin /usr/sbin 你可以将脚本存放在上面目录中的一个（建议放在你用户根目录的 ～/.bin，在 Linux 中，每个目录都有不同的功能，混杂到其中，会迷惑后来使用的人），或者把你的脚本目录追加到 PATH 中：\n$ export PATH=$PATH:/dir/to/your/scrpit 如果你想要把这个 PATH 固化（不用每次都手动配置），你可以把上面的脚本追加到 ~/.bashrc 或者 ~/.profile 中，例如：\n$ echo 'PATH=$PATH:/dir/to/your/scrpit' \u0026gt;\u0026gt; ~/.bashrc $ source ~/.bashrc\t# 使 ~/.bashrc 的变动生效 享受生活，告别冗长脚本！^_^ # ","date":"2014-11-30","permalink":"/n3xtchen/2014/11/30/data-science-at-the-command---create-reusable-cmd-line-tools/","section":"时间线","summary":"在前一篇 如何使用命令行进行数据分析 中，我们使用几个命令来进行一个简单的数据探索，向大家揭示了 命令行 毫不逊色于专业的数据分析工具；我们可以通过管道（Pipelines）将不同的命令串联在一块（我们称之为 one-liners）进行一些复杂的数据处理，清理，探索以及建模的任务（Bash 天生就是一种胶水语言）。","title":"Data Science at the Commandline - 如何创建可复用命令行工具（Commandline Tools）"},{"content":"","date":"2014-11-30","permalink":"/n3xtchen/tags/data_analytics/","section":"标签","summary":"","title":"data_analytics"},{"content":"","date":"2014-11-01","permalink":"/n3xtchen/tags/oracle/","section":"标签","summary":"","title":"Oracle"},{"content":"","date":"2014-11-01","permalink":"/n3xtchen/categories/oracle/","section":"分类页","summary":"","title":"Oracle"},{"content":"如果你在你的 PC 机上安装了 Oracle 数据库，不久你就会发现 /home/username 目录下会产生一个 oradiag_\u0026lt;username\u0026gt;。你删除了之后，还是会不断的产生！没办法，本人有洁癖，于是在 StackOverflow 上找到了 解决方法，这里做个笔记。\n首先，你需要找到发生这个的根源，其实根源你厌恶的文件中：\n$ head ~/oradiag_\u0026lt;username\u0026gt;/diag/clients/user_\u0026lt;username\u0026gt;/host_*/trace/sqlnet.log Sat Nov 01 16:28:19 2014 Create Relation ADR_CONTROL Create Relation ADR_INVALIDATION Create Relation INC_METER_IMPT_DEF Create Relation INC_METER_PK_IMPTS Directory does not exist for read/write [/usr/lib/log/diag/] [/usr/lib/log/diag/clients] 原因是由于 [/usr/lib/log/diag/] [/usr/lib/log/diag/clients] 不存在或者不可读写\n不同的软件可能目录有偏差，我使用是 OS X 下的 navicat-for-oracle，我的要修复的目录如下：\n[/opt/homebrew-cask/Caskroom/navicat-for-oracle/11.0.20/Navicat for Oracle.app/Contentts/OCI/log] [/opt/homebrew-cask/Caskroom/navicat-for-oracle/11.0.20/Navicat for Oracle.app/Contents/OCI/log/diag/clients]` 所以，具体目录要根据的日志信息！\n现在找到原因，对症下药自然就药到病除：\ncd /opt/homebrew-cask/Caskroom/navicat-for-oracle/11.0.20/Navicat for Oracle.app/Contentts/OCI/ sudo chmod -R 777 log sudo mkdir log/clients\t# 如果存在，就不需要创建 sudo chmod -R 777 log/clients ^_^，轻松搞定，烦劳不再来！\n","date":"2014-11-01","permalink":"/n3xtchen/2014/11/01/oracle---remove-oradiag_username/","section":"时间线","summary":"如果你在你的 PC 机上安装了 Oracle 数据库，不久你就会发现 /home/username 目录下会产生一个 oradiag_\u0026lt;username\u0026gt;。你删除了之后，还是会不断的产生！没办法，本人有洁癖，于是在 StackOverflow 上找到了 解决方法，这里做个笔记。","title":"Oracle - 如何让你的 oradiag_\u003cusername\u003e 永久消失"},{"content":"《Data Science at the Commandline》中使用了命令行进行数据分析。我利用书中的例子给大家演示下如何使用 Commandline 进行数据分析的。不过，首先你需要有一台 Unix/Linux 机子，如果你使用的 Windows，那请安装 Linux 虚拟机。由于本文使用的数据需要在墙外（你懂得），这里同样也提供他的使用的 样本数据 。那开始进入主题：\n每年的纽约时装周（Fashion Week in New York）总是令人惊讶不已。因此我们以此为例子，我们将讨论如何使用纽约时报（The New York Times）的开放接口获取 Fashion Week 的信息。首先，你需要在 The New York Times 的 开发者平台注册账户（墙外，请自找梯子），获取你自己的 API keys；这样你才可以检索文章，获取畅销和重大事件列表。\n我们将使用文章检索（Article Search API）的 API。我使用 The New York Times Fashion Week 作为检索词来查询是否有举办时装周。API 返回的结果数据是分页的，这意味着我们需要执行多次同样的查询（只是指定的页数不同，就像在搜索引擎点击下一页那样）。我们要使用 parallel 命令来轮询这些页数。具体命令见下方（不用纠结于弄懂命令的素有参数，我们只要了解我们使用的），现在打开你的命令行：\n$ cd /你下载的/文件解压的路径//book/ch01/data/ $ parallel -j1 --progress --delay 0.1 --results results \u0026quot;curl -sL \u0026quot; \\ \u0026quot;'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=New+York+'\u0026quot;\\ \u0026quot;'Fashion+Week\u0026amp;begin_date={1}0101\u0026amp;end_date={1}1231\u0026amp;page={2}\u0026amp;api-key='\u0026quot;\\ \u0026quot;'\u0026lt;your-api-key\u0026gt;'\u0026quot; ::: {2009..2014} ::: {0..99} \u0026gt; /dev/null 命令详解：\n首先把他们拆成两个命令：\n获取接口数据命令\ncUrl，用途：从具体的 URL 链接中下载数据\ncurl -sL \u0026quot;http://api.nytimes.com/svc/search/v2/articlesearch.json\u0026quot; \\ \u0026quot;?q=New+York+Fashion+Week\u0026amp;begin_date={1}0101\u0026amp;end_date={1}1231\u0026quot; \\ \u0026quot;\u0026amp;page={2}\u0026amp;api-key='\u0026lt;your-api-key\u0026gt;'\u0026quot; 批处理操作（parallel）\nparallel -j1 --progress --delay 0.1 --results results \\ \u0026quot;实际请求的命令\u0026quot; ::: {2009..2014} ::: {0..99} \u0026gt; /dev/null -j1：限定 job 数为 1\n--progress：现实进度条\n--delay 0.1：每个命令间隔 0.1 s\n--result results：结果文件的前缀\n::: {2009..2014} ::: {0..99}，这个命令每次生成一个组数据传入到上一个命令；你可能注意到了 begin_date={1}0101\u0026amp;end_date={1}1231\u0026amp;page={2}，这里的 {1} 就是知道 ::: {2009..2014} 每次产生的数字替换它，以此类推，{2} 也是一样的，命令类似如下：\nfor x in {2009..2014} do for x in {0..99} do curl -sL \u0026ldquo; http://api.nytimes.com/svc/search/v2/articlesearch.json\" \u0026ldquo;?q=New+York+Fashion+Week\u0026amp;begin_date=${x}0101\u0026amp;end_date=${x}1231\u0026amp;page=${y}\u0026rdquo; \u0026ldquo;\u0026amp;api-key=\u0026rsquo;\u0026rsquo;\u0026rdquo; \u0026gt; results/1/x/2/y/stdout 2\u0026gt; results/1/x/2/y/stderr sleep 0.1 done done\n我们执行多个相同的查询来获取 2009 年到 2014 年的数据。接口只允许获取 100 页的星系，因此我们使用 brace expansion（花括号扩展，例子上是 {2009..2014}，{0..99} ）来生成 100 个数字。因为接口有明确的限制，我们必须确保一次一个请求，间隔 1 秒。并且确认你申请的 API Key 替换 \u0026lt;your-api-key\u0026gt;。\n每一个请求返回十篇文章，因此总共 1000 篇。这些信息是根据信息访问量来排序的，因此它能给我们一个很好新闻报道样本。这个结果存储字啊 JSON 中，我们把它存储在 results 目录中。我们可以使用 tree 命令来查看该目录结构：\n$ tree results | head results └── 1 ├── 2009 │ └── 2 │ ├── 0 │ │ ├── stderr │ │ └── stdout │ ├── 1 │ │ ├── stderr │ │ └── stdout 接下来，我们可使用 cat，jq 和 json2csv 命令来合并和处理结果数据：\ncat results/1/*/2/*/stdout | jq -c '.response.docs[] | {date: .pub_date, type: .document_type, '\\ 'title: .headline.main }' | json2csv -p -k date,type,title \u0026gt; fashion.csv 现在开始拆解命令：\n我们把多个结果文件合并到起来输出给另一个命令\ncat results/1/*/2/*/stdout 我们使用 jq 提取发布日期，文档类型已经文章标题\njq -c '.response.docs[] | {date: .pub_date, '\\ 'type: .document_type, title: .headline.main }' 我们使用 json2csv 将 JSON 转换成 CSV，并保存到 fashion.csv 文件中。\njson2csv -p -k date,type,title \u0026gt; fashion.csv 使用 wc -l，我们可以查出总共有 4,855 篇报道：\n$ wc -l fashion.csv 4856 fashion.csv 让我们审查喜爱前十篇报道，来验证我们是否成功获取数据。为了只保留日志，我们使用 cols 和 csvcut（书中作者写的是 cut 命令，我查看了命令的源码中得出的这个结论） 把时间和时区去掉：\n$ \u0026lt; fashion.csv cols -c date cut -dT -f1 | head | csvlook |-------------+------------+-----------------------------------------| | date | type | title | |-------------+------------+-----------------------------------------| | 2009-02-15 | multimedia | Michael Kors | | 2009-02-20 | multimedia | Recap: Fall Fashion Week, New York | | 2009-09-17 | multimedia | UrbanEye: Backstage at Marc Jacobs | | 2009-02-16 | multimedia | Bill Cunningham on N.Y. Fashion Week | | 2009-02-12 | multimedia | Alexander Wang | | 2009-09-17 | multimedia | Fashion Week Spring 2010 | | 2009-09-11 | multimedia | Of Color | Diversity Beyond the Runway | | 2009-09-14 | multimedia | A Designer Reinvents Himself | | 2009-09-12 | multimedia | On the Street | Catwalk | |-------------+------------+-----------------------------------------| 似乎它已经成功了。为了深入了解它（insight），我们最好进行可视化。我们使用 R 的 ggplot 和 rio来创建一个线型图。\n$ \u0026lt; fashion.csv Rio -ge 'g + ' \\ 'geom_freqpoly(aes(as.Date(date), color=type), ' \\ 'binwidth=7) + scale_x_date() + ' \\ 'labs(x=\u0026quot;date\u0026quot;, title=\u0026quot;Coverage of New York' \\ 'Fashion Week in New York Times\u0026quot;)' | display ![image]({{ site.production_url }}/assets/fashion.png)\n这样就可以看出 New York Fashion Week 一年举办两次，而且可以很清晰的看出，一次在 2 月份，一次在 8 月份。我们希望也在相同的时间点举办，我们就可以事先准备了。\n","date":"2014-10-30","permalink":"/n3xtchen/2014/10/30/data-science-at-the-command---a-real-world-use-case/","section":"时间线","summary":"《Data Science at the Commandline》中使用了命令行进行数据分析。我利用书中的例子给大家演示下如何使用 Commandline 进行数据分析的。不过，首先你需要有一台 Unix/Linux 机子，如果你使用的 Windows，那请安装 Linux 虚拟机。由于本文使用的数据需要在墙外（你懂得），这里同样也提供他的使用的 样本数据 。那开始进入主题：","title":"Data Science at the Commandline - 如何使用命令行进行数据分析"},{"content":" 交互式 Python # Python 是一个编程语言，它允许你快速创建和简单地编码就能完成相当复杂的任务。使用交互式 Python 解释器，试试输入一些命令来弄清楚它们工作原理。如果你完成一些基本 Python 教程，这里第一步对于你来说非常简单，只需要在命令行输入 python。\npython 命令将会打开一个解释器，你可以在里面输入一些命令，并且实时返回结果给你。这是 powerful one-liners 的一个非常简单的例子：\n$ python Python 2.7.6 (default, Mar 22 2014, 22:59:56) [GCC 4.8.2] on linux2 Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; import pprint \u0026gt;\u0026gt;\u0026gt; pprint.pprint(zip(('Byte', 'KByte', 'MByte', 'GByte', 'TByte'), (1 \u0026lt;\u0026lt; 10*i for i in xrange(5)))) [('Byte', 1), ('KByte', 1024), ('MByte', 1048576), ('GByte', 1073741824), ('TByte', 1099511627776)] \u0026gt;\u0026gt;\u0026gt; 虽然交互式环境很有用，但并不助于对 Python 的更深入的探索。在你的 python 之旅的早期，你可以听过 IPython。IPython 在 Python 的解释器的基础上，还提供了很多好用的特性，其中包括：\ntab 补齐 对象探测（exploration） 历史命令 打开 IPython 和打开 Python 一样简单，但是你马上就注意到很不同的交互界面：\n$ ipython Python 2.7.6 (default, Mar 22 2014, 22:59:56) Type \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. IPython 2.3.0 -- An enhanced Interactive Python. ? -\u0026gt; Introduction and overview of IPython's features. %quickref -\u0026gt; Quick reference. help -\u0026gt; Python's own help system. object? -\u0026gt; Details about 'object', use 'object??' for extra details. In [1]: import pprint In [2]: pprint.pprint(zip(('Byte', 'KByte', 'MByte', 'GByte', 'TByte'), (1 \u0026lt;\u0026lt; 10*i for i in xrange(5)))) [('Byte', 1), ('KByte', 1024), ('MByte', 1048576), ('GByte', 1073741824), ('TByte', 1099511627776)] In [3]: help(pprint) In [4]: pprint. pprint.PrettyPrinter pprint.isrecursive pprint.pprint pprint.warnings pprint.isreadable pprint.pformat pprint.saferepr In [4]: pprint. 在这个例子中，我运行同样的命令，得到了相同的输出，可以尝试下 help 函数中，意识使用 TAB 键对 pprint 进行补齐。我还使用的另一个命令，使用向上的光标可以查看我输入的命令历史，编辑它们和执行结果：\nIn [4]: pprint.pprint(zip(('Byte', 'KiloByte', 'MegaByte', 'GigaByte', 'TeraByte'), (1 \u0026lt;\u0026lt; 10*i for i in xrange(5)))) [('Byte', 1), ('KiloByte', 1024), ('MegaByte', 1048576), ('GigaByte', 1073741824), ('TeraByte', 1099511627776)] In [5]: pprint.pprint(zip(('Byte', 'KByte', 'MByte', 'GByte', 'TByte'), (1 \u0026lt;\u0026lt; 10*i for i in xrange(5)))) [('Byte', 1), ('KByte', 1024), ('MByte', 1048576), ('GByte', 1073741824), ('TByte', 1099511627776)] IPython 获取对象的帮助也很方便。如果你遇到麻烦，尝试使用 ? 获取更多信息：\nIn [9]: s = {'1','2'} In [10]: s? Type: set String form: set(['1', '2']) Length: 2 Docstring: set() -\u0026gt; new empty set object set(iterable) -\u0026gt; new set object Build an unordered collection of unique elements. In [11]: IPython 提供的这些功能很 cool 而且有用，所以我鼓励你在你的系统中安装它并使用它。\nIPython Notebook # IPython 非常有用；在 Django 项目，我已经使用很多年了。大概在 2011 年，Django 把 IPython notebook 引进到它的强大工具集中。由于某些原因，很迟才用上它；但是现在我已经改变了想法，使用它；我可以看到她所散发的无穷力量。\nIPython Notebook， 简单地概括就是在浏览器中提供了 IPython 控制台。不过，它不仅仅是在浏览器中提供类 IPython 的特性，还提供了很易用的记录你的操作步骤和分享功能。在业务应用方面，有两个令人印象深刻的特点：\nNotebooks 易于交互和探索你的数据。 数据探索需要自记录（self-documenting）和分享功能。 想象你一下你正在使用 Excel，比如创建一个数据透视表（Pivot Table），或者做一些其他的分析。如果你想要向别人解释如何操作，你会怎么做呢？截图和注释？通过默写屏幕记录工具？还是直接把 Excel 文档给他们，让他们自己把它弄清楚？\n上面都是很糟糕的建议；Excel 在 ad-hoc（即席） 分析领域的霸主地位导致了它成为了业界标准。当然，IPython Notebook 与 pandas 配合提供了一个分析大量数据和团队分享的有效途径。\nPython 数据分析库 # pandas， Python 的一个数据分析库，由 BSD 授权，提供高性能，易于使用的数据结构和数据分析工具。Pandas 是一个非常复杂的程序，你可以使用它做非常复杂数学计算。\n启动环境 # 启动 ipython notebook 会话：\n$ ipython notebook 你的浏览器会自动打开，并自动跳转到 notebook 的页面。这里主屏幕的截图（你有可能是空白的，这里只是展示一些例子）：\n点击 New Notebook 按钮，启动新的环境开始编码：\n你将看到的输入框非常像之前我们看到的 IPython 命令提示符。\n文章接下来的部分，我将讲解在输入框输入各种命令。我使用 reSt（reStructedText，一种标记语言）格式 下载全部的会话，因此，它能与我的博客无缝地整合在一起。\n使用 Pandas 进行快速数据分析 # 现在，我使用我的 notebook 开始，我做一些更强大的分析。\n首先，我们需要导入标准的 pandas 库\nimport pandas as pd import numpy as np 接下来，我们读取样本数据：\nSALES=pd.read_csv(\u0026quot;sample-sales.csv\u0026quot;) SALES.head() Account Number Account Name sku category quantity unit price ext price date 0 803666 Fritsch-Glover HX-24728 Belt 1 98.98 98.98 2014-09-28 11:56:02 1 64898 O'Conner Inc LK-02338 Shirt 9 34.80 313.20 2014-04-24 16:51:22 2 423621 Beatty and Sons ZC-07383 Shirt 12 60.24 722.88 2014-09-17 17:26:22 3 137865 Gleason, Bogisich and Franecki QS-76400 Shirt 5 15.25 76.25 2014-01-30 07:34:02 4 435433 Morissette-Heathcote RU-25060 Shirt 19 51.83 984.77 2014-08-24 06:18:12 现在我们使用数据透视表来汇总销量，把数据转换成有意义的信息。我们先从一些简单的开始：\nreport = SALES.pivot_table(values=['quantity'],index=['Account Name'],columns=['category'], aggfunc=np.sum) report.head(n=10) quantity category Belt Shirt Shoes Account Name Abbott PLC NaN NaN 19 Abbott, Rogahn and Bednar NaN 18 NaN Abshire LLC NaN 18 2 Altenwerth, Stokes and Paucek NaN 13 NaN Ankunding-McCullough NaN 2 NaN Armstrong, Champlin and Ratke 7 36 NaN Armstrong, McKenzie and Greenholt NaN NaN 4 Armstrong-Williamson 19 NaN NaN Aufderhar and Sons NaN NaN 2 Aufderhar-O'Hara NaN NaN 11 这个命令给我们展示每个用户购买的产品数量 —— 所有只要一个命令！看到它强大的同时，你将会注意到输出中有一堆的数据是 NaN。这个的意思是非数字（Not A Number），意思就是此处无值。\n如果使用 0 代替，是不是看起来更美观清晰？这个就是 fill_value 的用途：\nreport = SALES.pivot_table(values=['quantity'],index=['Account Name'],columns=['category'], fill_value=0, aggfunc=np.sum) report.head(n=10) quantity category Belt Shirt Shoes Account Name Abbott PLC 0 0 19 Abbott, Rogahn and Bednar 0 18 0 Abshire LLC 0 18 2 Altenwerth, Stokes and Paucek 0 13 0 Ankunding-McCullough 0 2 0 Armstrong, Champlin and Ratke 7 36 0 Armstrong, McKenzie and Greenholt 0 0 4 Armstrong-Williamson 19 0 0 Aufderhar and Sons 0 0 2 Aufderhar-O'Hara 0 0 11 这样，它看起来干净多了！我们将在这个例子中演示数据透视表的更强大的地方。让我们算算他们的总销售收入：\nreport = SALES.pivot_table(values=['ext price','quantity'],index=['Account Name'],columns=['category'], fill_value=0,aggfunc=np.sum) report.head(n=10) ext price quantity category Belt Shirt Shoes Belt Shirt Shoes Account Name Abbott PLC 0.00 0.00 755.44 0 0 19 Abbott, Rogahn and Bednar 0.00 615.60 0.00 0 18 0 Abshire LLC 0.00 720.18 90.34 0 18 2 Altenwerth, Stokes and Paucek 0.00 843.31 0.00 0 13 0 Ankunding-McCullough 0.00 132.30 0.00 0 2 0 Armstrong, Champlin and Ratke 587.30 786.73 0.00 7 36 0 Armstrong, McKenzie and Greenholt 0.00 0.00 125.04 0 0 4 Armstrong-Williamson 1495.87 0.00 0.00 19 0 0 Aufderhar and Sons 0.00 0.00 193.54 0 0 2 Aufderhar-O'Hara 0.00 0.00 669.57 0 0 11 如果我们想，你可以把结果导入到 Excel 中。我们要把它转换成 DataFrame，然后写到 Excel 文件中：\npd.DataFrame(report).to_excel('report.xlsx', sheet_name='Sheet1') 看一下我使用的 pandas 的版本，因为新的版本引入很多新的语法：\nln [15]: pd.__version__ 0.14.1 最后的思考 # 这个文章的目的就是展示对交互式 python 工具的基本了解，和如何使用它做一些快速可以重现的复杂分析。\n译自 http://pbpython.com/simple-data-analysis.html\n","date":"2014-10-22","permalink":"/n3xtchen/2014/10/22/python---simple-interactive-data-analysis/","section":"时间线","summary":"交互式 Python # Python 是一个编程语言，它允许你快速创建和简单地编码就能完成相当复杂的任务。使用交互式 Python 解释器，试试输入一些命令来弄清楚它们工作原理。如果你完成一些基本 Python 教程，这里第一步对于你来说非常简单，只需要在命令行输入 python。","title":"Python - 简单的交互式数据分析"},{"content":"","date":"2014-10-22","permalink":"/n3xtchen/tags/ruby/","section":"标签","summary":"","title":"ruby"},{"content":" 前言 # Rack，Ruby 的 GEM，是所有 Ruby 网页编程相关项目中公认最流行的底层框架了。你的框架可以不断的改变，但是不管你怎么换，你仍然需要使用 Rack。Rack 之所以如此流行和经久不衰来源是它的灵活特性。作为服务器和 Web 应用之间的中间人角色，Rack 只要求你遵循极少的标准，就能得到大量的中间件（Middlewares）支持。在 Ruby 的世界了， 它在解构标准方面拥有最大的成就。\n对了，今天的主角就是它了，但是不是教你怎么使用它，而是克隆的它。我将一步一步通过用例来克隆一个 Gem。这样你即可以熟悉 Gem 的工作方式（我也将使用同样的类名和方法名），还可以拥有自己的版本。\nRack 经典的 Hello World （这里命名为 config.ru）：\nrun Proc.new { |env| ['200, {'Content-Type' =\u0026gt; 'text/html'}, ['hello world]] } 现在你执行 rakeup config.ru，并使用你的浏览器查看 localhost:9292。看似无用，但是 Rails，Sinatra 以及其他基于 Rack 的 Web 应用就是这样在 Rack 的基础上，一步步构建起来的。\n从头开始 # 因此，现在我将要创建一个属于自己的 Rack，它可以做的就是上面 Hello World 的事情。首先，我们需要一个需求清单：\n拥有可以接受一个 .ru 文件的 rackup 命令 在本地启动服务 每个请求都会启动来自我们 .ru 文件中的进程 有一个很 cool 的 Gem 名 明显，上面最重要的就是第四点，^_^。我将要把我们的 Rack 克隆命名为 Plank，它听起来比 Rack 还要轻量。好的，名字解决了，感觉挺高大上吧，嘻嘻。接下来，Coding！\n设置 # 首先，创建一个名为 plank 的目录，在其中创建两个字目录，分别为 bin 和 lib：\n$ mkdir plank $ cd plank $ mkdir bin lib 现在创建我们的测试文件。Rack 使用 ru 作为扩展名，但是 Plank 更好，他使用 .pu 作为扩展名：\n$ touch test.pu 在 test.pu 中，我们需要和 Plank 交互，就像 Rack 一样：\nrun Proc.new { |env| ['200, {'Content-Type' =\u0026gt; 'text/html'}, ['Hello World'] } Plankup # 完成上述步骤，我们得到了创建好的 Plank 模版（boilerplate）。我们的终极目标就是像 Rack 一样，调用 plankup test.ru；然后很神奇地在我们的本地运行服务器。但是我们该先从哪里下手呢？我们先从 plankup 命令开始吧，因为他是我们直接接触到。下面是 bin/plankup 的代码：\n#!/usr/bin/env ruby Plank::Server.start 第一行是告诉 shell 使用 Ruby 来执行接下来脚本。Plank 作为我们项目的命名空间。Server.start 帮助我们与底层命令实现交互。写到这里，接下来相信大家就知道要做什么了。这是目前我们仅有的信息，不过已经足够继续了。那就开始实现 Server 吧！\nPlankup::Server # 遵循 Ruby 的打包标准，lib 作为我们脚本的存放目录。如果你读过 Rack 的脚本，你可能注意到这个目录里面有很多文件。幸运的事，Plank 只需要实现里面的一个 100 行的文件，同样也是最核心的部分。因此，在 lib 目录中，创建一个 plank.rb 文件：\ntouch lib/plank.rb 现在我们可以开始编写真正的 Ruby 脚本了。正如我们所知，Plank 将是我们最顶级的命名空间，因此先写：\nmodule Plank end Plank::Server 呢？它应该是一个类（Class）或者一个模块（Module）。好的，我现在猜测 Plank::Server 应该是一个对象，理由是它需要被初始化并拥有生命周期；因此让我们为它编写一个类（Class），里面包含一个 start 的方法：\nmodule Plank class Server def self.start end end end 现在，类方法 start 只是一个空方法，我们要做的就是实现它！\nServer.start # 那么 Server.start 实际做什么的呢？我们已经决定 Server 是一个类（Class），因此初始化实例是很合理的也是必要的步骤：\ndef self.start new end 好的，没有任何功能。但是它是一个好的开端。我们现在知道 start 是一个工厂（Factory）方法；将会创建一个 Server 对象。然而，这还不够。如果我们只想要一个对象实例的话，那我们可以调用 Plank::Server.new。start 暗含一些其他的功能，比如启动一个服务。因此，启动什么呢？一个对象还是类？因为只有对象启动，所以我们假设使用 Server 对象调用 start：\ndef self.start new.start end 好的，我们到这一步了。但是我们缺少了两样东西：其中一个是 server.start 的实现；另一个是获取 test.pu 的索引。Huh？当我们执行 plankup test.pu，我们只是作为命令行参数传递它。目前 .start 的实现还没涉及到参数，因此这部分我们添加它：\ndef self.start new(ARGV).start end ARGV，它是系统全局变量，保存所有从命令行传递过来的参数。在我们的例子中，ARGV 是一个一维数组，['test.pu']。\nApps 和 Options # 在分解代码之前，让我们先停下来，想一想我们的 Server 对象需要启动哪些东西？当我们运行 rail s 的时候，为了运行你的 App，你将启动一个服务。对于任何一个服务都是这样的；我们的 Plank::Server 也不例外。首先，我们需要运行一个 App。如果你回想起来了，我们已经写好了一个可以执行的 Plank 应用。它就是 test.pu 中的 Proc。一个服务器应该还需要一些基本的配置：端口（Port）、主机（Host）、环境变量等等。\n好的！我们服务器只需要识别要运行的应用和设置一些配置选项；这些配置就像下面那样配置成常量：\nclass Server PORT = \u0026quot;9292\u0026quot; HOST = \u0026quot;localhost\u0026quot; ENVIRONMENT = \u0026quot;development\u0026quot; ... end 我们不希望在其他地方重新配置这些常量，因此我们将在初始化 Server 时，传入到 @options 数组中，并设置成默认值：\nclass Server ... def initialize @options = default_options end ... private def default_options { environment: ENVIRONMENT, Port: PORT, Host: HOST, } end 现在轮到 App 了。我们知道我们的 App 是 ARGV 中的引用，至少有一个文件名。因此，为了调用 Proc，我们需要把文件中的内容转化成服务器能识别的变量。这个听起来好像有很多工作要做，更重要的是，它听起来不像是 Server 所要承担的职责，例如解析业务逻辑。作为替代，我们创建一个 Builder 类来处理我们的应用。它可以灵活地选择不同的方式，根据不同配置来创建应用，而不会改变 Server 类。在我们的 plank.rb，添加 1 个类：\nmodule Plank ... class Builder end end 让我们给 Server 中添加一些方法，以便他能调用 Builder 来处理传入的 test.pu：\nmodule Plank class Server ... def initialize(args) ... @options[:config] = args[0] @app = build_app end private .... def build_app Plank::Builder.parse_file(@options[:config]) end end end 首先我们把 @options 数组的 :config 中设置成有命令行传入的文件名；然后服务器调用它的私有方法 .build_app 来间接触发 Builder。\nBuilder 对我们来说，是一个工厂类；它接受我们传给它的文件作为参数，处理后返回一个可运行的 App 。为了实现这个，我们需要一个类方法。就叫 parse_file 。我们所需要做的就是把文件名（test.ru）发送给 Builder。\nmodule Plank ... class Builder def self.parse_file(file) cfg_file = File.read(file) new_from_string(cfg_file) end def self.new_from_string(builder_script) eval \u0026quot;Plank::Builder.new {\\n\u0026quot; + builder_script + \u0026quot;\\n}.to_app\u0026quot; end end end 这边跳的有点快，这里让我解释下。Builder.parse_file 很容易实现。读取一个文件的内容，到一个变量 cfg_file 中；然后传递给 Builder.new_from_string，一个工厂方法运行一些怪异的 Ruby 代码，它的作用是执行 test.pu 中的代码。需要的注意的，eval 是一个很取巧的做法，可能会造成安全漏洞，但是在我们的例子中，它可以很好将文本转化成 Ruby 代码。\n记住，我们的 test.pu 只有一行：\nrun Proc.new { |env| ['200, {'Content-Type' =\u0026gt; 'text/html'}, ['hello world]] } 现在我们把它作为 block 发送给 Builder.new；这是我们所希望的。这行代码构建了两个组件，分别是 run 和 Proc。run 是作为一个方法被我们的 Builder 类调用。通过调用 run，我们把 Proc 赋值给一个实例变量 @app，最后使用 .to_app 返回@app。了解了吗？它解释起来很长，但是实现起来很简单：\nclass Builder ... def initialize(\u0026amp;block) instance_eval(\u0026amp;block) if block_given? end def run(app) @run = app end def to_app @run end end 为什么我们需要 instance_eval？由于他允许我们根据实例对象的上下文，block 才可以访问到 Builder.run。如果我们直接执行 block.call，它将会报错，告知 run 未定义，因为 block 和 Builder 不在同一个作用域。\nServer\u0026rsquo;s server # 现在我们的 Server 对象实例有了可执行的 @app 和相关配置信息（如端口和主机名）。我们几乎完成。剩下的就是启动服务的实现。如果你看到我们的 Server.start 方法，我们实例化的最后有个 .start，说明这个方法负责启动 Plank，不要搞混了，这是实例变量，不是我们之前实现的类变量，同名但是不同的两个变量。让我们实现它：\nmodule Plank class Server ... def start server.run @app, @options end private ... def server @server ||= Plank::Handler.default end end end 所有的这些代码看起来有点突兀，不过相信我，它奏效。Server 识别到一个服务，并且负责运行传入的 App 和配置。 @Server 本来是个空变量，现在把 Plank::Handler 赋给它。记住，我们只要 **Plank 和真正的网页服务器（ 如 WebRick，Thin 或者 Mongrel ）交互。为了根据用户选择服务器脚本的不同偏好，我们将要创建一个 Plank 的 Server 的抽象层（Plank::Handler），使后端服务成变成一个组件是很有意义的，可以很容易被切换：\nmodule Plank ... module Handler def self.default end end ... end Handler 是一个模块；我们不会直接和 Handler 类交互，只是和我们的服务器的特定实现进行交互（如 Thin，WebRick 等等）。明显，Module.default 还没完成。现在，我们需要挑选默认的服务器。这里我选择 thin，理由是它简单易于使用，而且配置比 WebRick 简单很多。如果你真的了解 Thin，你基本上可以使用任何东西，你怎么设置，Plank 就可以使用。\nmodule Plank ... module Hanlder def self.default Plank::Handler::Thin end class Thin end end end 你可能会问为什么 module.default 不直接调用我们 Thin 类的方法。虽然有点绕，但是我们的 Plank::Server 对象只需要我们类实例的名称。好的，我们的 Thin 类 真的很瘦，所以我们首先充实它：\nmodule Plank ... module Handler ... class Thin def self.run(app, options={}) host = options[:Host] port = options[:Port] args = [host, port, app, options] server = ::Thin::Server.new(*args) server.start end end end end Thin.run 简单组织一些输入，然后初始化成 Thin::Server 实例。需要使用前导 ::，它会先寻找 Thin GEM 包，而不是我们自己创建的 thin 类。我们几乎已经完成了，还有两件事件需要扫尾下： 1 确认你的机子已经安装了 thin，并添加 require 在你的代码头部；\nrequire 'thin' module Plank ... 2.确认你在你的 bin/planup 文件包含 plank.rb：\n#! /usr/bin/env ruby require_relative '../lib/plank' ... 在你的命令行运行 ./bin/plankup test.pu，打开浏览器 http://localhost:9292/，见证奇迹的时候。。。\n结语 # 我希望在这个博客中，你可以理解 Rack 的运行原理。用了不到 100 行的代码中，我实现了一个最简陋的 Rack。Rack 本身除了 run 外还有其他丰富的接口，因此尝试增加一些 Rack 特性到 Rack 中，比如路由，守护进程，以及其他在 Rack 文档中列举的特性。本人来说，深挖 Rack 的同时激起了我对 Thin 的兴趣。\n这里提供给 源码，和丰富的注释，如果你的无法正常运行，可以对比下！\n参考 http://www.kavinder.com/blog/2014-10-10-rebuild-a-gem-rack/\n","date":"2014-10-22","permalink":"/n3xtchen/2014/10/22/ruby---rebuild-rack/","section":"时间线","summary":"前言 # Rack，Ruby 的 GEM，是所有 Ruby 网页编程相关项目中公认最流行的底层框架了。你的框架可以不断的改变，但是不管你怎么换，你仍然需要使用 Rack。Rack 之所以如此流行和经久不衰来源是它的灵活特性。作为服务器和 Web 应用之间的中间人角色，Rack 只要求你遵循极少的标准，就能得到大量的中间件（Middlewares）支持。在 Ruby 的世界了， 它在解构标准方面拥有最大的成就。","title":"Ruby - 重构 Rack"},{"content":" Science! true daughter of Old Time thou art! Who alterest all things with thy peering eyes. Why preyest thou thus upon the poet\u0026rsquo;s heart, Vulture, whose wings are dull realities?\n— Edgar Allen Poe\n在 2010 年， Konstantin Haase，Sinatra 的首席开发，使用 Ruby 中所有混淆的代码精简技巧，编写了一个他所能写的最小的 web 框架。它就是后来的 Almost Sinatra，一个与 Sinatra 相互兼容的框架，代码仅有 999 B ，总共 6 行。\n它是一个令人印象深刻的壮举；下面是 Almost Sinatra 的全部代码：\n%w.rack tilt date INT TERM..map{|l|trap(l){$r.stop}rescue require l};$u=Date;$z=($u.new.year + 145).abs;puts \u0026quot;== Almost Sinatra/No Version has taken the stage on #$z for development with backup from Webrick\u0026quot; $n=Module.new{extend Rack;a,D,S,q=Rack::Builder.new,Object.method(:define_method),/@@ *([^\\n]+)\\n(((?!@@)[^\\n]*\\n)*)/m %w[get post put delete].map{|m|D.(m){|u,\u0026amp;b|a.map(u){run-\u0026gt;(e){[200,{\u0026quot;Content-Type\u0026quot;=\u0026gt;\u0026quot;text/html\u0026quot;},[a.instance_eval(\u0026amp;b)]]}}}} Tilt.mappings.map{|k,v|D.(k){|n,*o|$t||=(h=$u._jisx0301(\u0026quot;hash, please\u0026quot;);File.read(caller[0][/^[^:]+/]).scan(S){|a,b|h[a]=b};h);v[0].new(*o){n==\u0026quot;#{n}\u0026quot;?n:$t[n.to_s]}.render(a,o[0].try(:[],:locals)||{})}} %w[set enable disable configure helpers use register].map{|m|D.(m){|*_,\u0026amp;b|b.try :[]}};END{Rack::Handler.get(\u0026quot;webrick\u0026quot;).run(a,Port:$z){|s|$r=s}} %w[params session].map{|m|D.(m){q.send m}};a.use Rack::Session::Cookie;a.use Rack::Lock;D.(:before){|\u0026amp;b|a.use Rack::Config,\u0026amp;b};before{|e|q=Rack::Request.new e;q.params.dup.map{|k,v|params[k.to_sym]=v}}} 出于对探索黑暗语言艺术的渴望，我认为我应该一句一句弄透 Almost Sinatra，弄清楚 Konstantin 使用的相关技术。我希望这么做的同时不“驱散彩虹（unweaving the rainbow）”，不摧毁象征主义诗歌般、深奥而富有意义的代码所蕴含的美感；通俗点讲，就是既要尊重他的艺术工作，又要对代码斟言酌句。\n设置 # 事不宜迟，让我们从脚本的第一个语句开始：\n%w.rack tilt date INT TERM..map{|l|trap(l){$r.stop}rescue require l}; 这里已经有些语法相当的陌生，尤其对于 Ruby 新手来说；因此，让我们重新格式化它们，使其表述得稍微清楚一点：\n%w{rack tilt date INT TERM}.map do |l| trap(l) do $r.stop end rescue require l end\t这是节省空间（space-saving）的基本技巧之一：你可以使用 %w 来创建一个字符数组；可以写成我们最常见的样子：\n[\u0026quot;rack\u0026quot;, \u0026quot;tilt\u0026quot;, \u0026quot;date\u0026quot;, \u0026quot;INT\u0026quot;, \u0026quot;TERM\u0026quot;] … 也可以写成这样：\n%w{rack tilt date INT TERM} 你可以使用任何你喜欢的字符作为分隔符（delimiters）；Konstantin 使用了 .，不过我们一般会使用各式各样的括号（如 []，()，{}）来包围数组。\n这里还展示了另一个技巧就是循环（loops）的复用。两个事情需要在脚本的开始实现：第一，Almost Sinatra 的依赖（Dependencies）需要被载入（require）；第二，你需要告诉 Ruby 要捕捉（trap）的信号（signals）。我们的程序需要不做两种信号：一个是 SIGINT（当你在终端执行 CTRL + C 时，信号将被发送。），另一个是 SIGTERM（进程被杀死的时候，信号将被发送。）。\n这需要两步来完成：遍历依赖，并载入（require）；遍历要捕捉的信号，并传递给 trap 方法。但是当空格都是一种奢侈的情况下，两次循环同样很奢侈。\n因此，我们见识了如下技巧：脚本把 \u0026quot;rake\u0026quot;，\u0026quot;tilt\u0026quot; 和 \u0026quot;date\u0026quot; （它们都不是信号）传递给捕捉器（trap）。如果我们把不是信号的对象传递给 trap 会发生什么？我们使用 REPL 查看下：\n2.1.1 :001 \u0026gt; trap(\u0026quot;rack\u0026quot;) {} ArgumentError: unsupported signal SIGrack from (irb):1:in `trap' 数组中的前三个元素都会触发 ArgumentError 异常。rescue， 当使用它来修饰这一行的时候，将会恢复所有的异常；因此，为了代替捕捉信号，脚本通过 rescue 调用 require 来加载它们。Bingo：一个循环完成两个目的。\ntrap 中的实际句柄（handler）将会明确地中止服务。后续，我们将会讨论 $r 到底是什么；当它被确切创建之前：还有很长的路，先走在说：\n$u=Date; An even simpler statement for number two, but an important concept: here, a constant that’s used multiple times in the script is assigned to a one-letter variable. Although this seems like it might be a waste, the characters are made up if it’s even used once or twice more — or even quicker, if the constant has a particularly long name. 这个语句再简单不过，但是这里有一个很重要的概念：将一个常量被赋予一个单字符变量中，后续将会被使用多次。虽然它看起来像多余的，如果它只被使用一次或两次 －甚至更多次，如果一个常量有特别长的名字，那字符数就被省下来了。\n$z=($u.new.year + 145).abs; 这里是个有趣的技术。$z 用来作 Sinatra 的端口号；具体实现是，Date.new.year 被调用（它永远返回 -4712，年的默认值）；加上 145，就是 －4567；最后，符号将会被 abs 忽略，剩下的 4567 - Sinatra 默认的端口。\n当然，它和下面的语句效果是相同：\n$z=4567 貌似这样精简加以 被拒绝了，舌头长在别人身上（爱怎么讲就怎么讲），原因是不鼓励使用魔法数字（ magic numbers）； it seems that sometimes even the principles of Golf can play second fiddle to the church of DRY.似乎有时高尔夫法则也可以像 DRY 的教堂法则那样影响开发。\nputs \u0026quot;== Almost Sinatra/No Version has taken the stage on #$z for development with backup from Webrick\u0026quot; 一个简单的 puts 语句，它也包含一个精简技巧：如果你想要访问标记的变量（全局变量以 $ 开头，以及实例变量以 @ 开头），你不需要在插入操作中使用花括号。下面是常见的用法：\n@foo = \u0026quot;bar\u0026quot; $foo = \u0026quot;bar\u0026quot; puts \u0026quot;@foo is #{@foo} and $foo is #{$foo}\u0026quot; 也可以写成：\n@foo = \u0026quot;bar\u0026quot; $foo = \u0026quot;bar\u0026quot; puts \u0026quot;@foo is #@foo and $foo is #$foo\u0026quot; 为了节省关键的两个字符，牺牲了可读性。\n主逻辑 # $n=Module.new 第一行结束了，大部分用于配置；现在我们将接触到大量的逻辑（logic）。定义一个包含 Sinatra 相关的功能的模块（module）。\nextend Rack; Rack 被混合（Mix）到这个模块中；这样你就可以不用使用命名空间来调用 Rack 中的方法；接着使用 Rack::Builder 来创建应用，它提供 map 方法（使用它作为 Sinatra 路由的一种节省空间做法）。\na,D,S,q=Rack::Builder.new,Object.method(:define_method),/@@ *([^\\n]+)\\n(((?!@@)[^\\n]*\\n)*)/m 在这个批量赋值的语句中包含了很多东西；但是如果它们分开来的话，就简单很多。\n首先，初始化一个 Rack::Builder 新实例被初始化，并赋给 a：\na = Rack::Builder.new （a 就是 app。）\n然后，创建一个快捷方式。这个脚本后续将动态定义多个方法，因此，使用 D 作为 define_method 的别名后续将会节省一堆的字符：\nD = Object.method(:define_method) 严格来说，由于 Ruby 不允许我们把函数作为值来传递，因此它是被设置成 define_method 的 方法对象（method object）;但是在实际上，它们很相似的。这个快捷方式定义之后，D.call(*) {} 就等同于 define_method(*) {}。\n最后，S 被设置成用来解析 Sinatra 内置模版的需要的正则表达式：\nS = /@@ *([^\\n]+)\\n(((?!@@)[^\\n]*\\n)*)/m 处理请求 # %w[get post put delete].map{|m|D.(m){|u,\u0026amp;b|a.map(u){run-\u0026gt;(e){[200,{\u0026quot;Content-Type\u0026quot;=\u0026gt;\u0026quot;text/html\u0026quot;},[a.instance_eval(\u0026amp;b)]]}}}} 这里我们开始看到看起来像 “Sinatra” 的元素了：这个代码块（block）创建了一个全局方法，定义了路由（比如，get “／foo” do，等等）。让我们格式化它，加上一些别名，以及重命名一些单字符名变量，我们就能看得更清楚点：\n%w{get post put delete}.map do |method| define_method(method) do |path, \u0026amp;block| a.map(path) do run -\u0026gt;(e) { [200, {\u0026quot;Content-Type\u0026quot; =\u0026gt; \u0026quot;text/html\u0026quot;}, [a.instance_eval(\u0026amp;block)]] } end 现在看起来更简单点了。对于每一个 HTTP 请求方法（使用 map 而不是 each，因为可以省一个字符），一个全局方法 —— 都会带一个路径作为他们的第一参数，然后是一个代码块（block）。这些方法使用 define_method 快捷方式来定义；除了用 D.(\u0026quot;method_name\u0026quot;) 取代 D.call(\u0026quot;method_name\u0026quot;)， 被使用 —— 因为他的代码更短短，其他不值得一提。你实际上还可以更短，因为[] 是 call 的别名，可以使用 D[\u0026quot;method_name\u0026quot;]。\n被调用的时候，它将会使用 Rack::Builder 的 map 方法创建一个方法，追加一个新的路由（route）到 Rack app 中（用来匹配路径）。当被执行的时候，这个句柄将会设定响应状态为 200（“ok”），内容类型是 HTML ，以及代码块返回值的响应体（response body）（使用 instance_eval 来关联应用的上下文）。\n这个基本上和原生的 Sinatra app 的实现方式一样。\n解析模版 # Tilt.mappings.map{|k,v|D.(k){|n,*o|$t||=(h=$u._jisx0301(\u0026quot;hash, please\u0026quot;);File.read(caller[0][/^[^:]+/]).scan(S){|a,b|h[a]=b};h);v[0].new(*o){n==\u0026quot;#{n}\u0026quot;?n:$t[n.to_s]}.render(a,o[0].try(:[],:locals)||{})}} 接下来的一行全部都是关于模版的（templates）；它允许你调用，例如，haml :foo，将有一个 foo.haml 的模版作为响应体被渲染（render）。\n同样，赋予变量更有意义的名称，令复杂的代码更容易被理解。先从头两句开始：\nTilt.mappings.map{|k,v|D.(k){|n,*o|$ 我们可以改写成：\nTilt.mappings.map do |template_type, constant| define_method(template_type) do |n, *o| Tilt 是一个用来解析许多不同模版语言的库；常规的 Sinatra 也使用它。\n对于每一种 Titl 支持的模版语言，这个代码定义一个全局方法（例如，叫做 erb 或者 haml）。这个方法像下面这样使用：\n$t||=(h=$u._jisx0301(\u0026quot;hash, please\u0026quot;);File.read(caller[0][/^[^:]+/]).scan(S){|a,b|h[a]=b};h);v[0].new(*o){n==\u0026quot;#{n}\u0026quot;?n:$t[n.to_s]}.render(a,o[0].try(:[],:locals)||{}) 这个代码的第一部分从当前文件提取内联模版（inline templates），追加到一个哈希数组中。\n有趣的事，不像你所想的，使用 {} 创建空哈希数组，而是：\nDate._jisx0301(\u0026quot;hash, please\u0026quot;) _jisx0301 是 Date 类的半私有（semi-private）方法，它期望一个 JIS X 0301-2002 格式的字符串 —— 在日本使用的一个标准。如果被给予，它将返回所给日期的年，月，日组成的哈希数组；但是如果输入不规范，它将返回一个空哈希数组。因此，传递 “hash, please” 给它可能是生成空哈希数组最诡异的做法之一了 —— 这是另一个和端口号生成代码一样有趣的笑话，还能让代码的最终容量那么笑就足够令人敬佩了。\n下一步，提取调用 Sinatra 的代码中的内联模版。我们不能使用 __FILE__ 来获取实际的执行文件名，因为所指向的文件和调用 Sinatra 方法的文件是不同的；同样，也不能使用 $0（它指向被执行的脚本）。\n因此，它使用了 caller(0) 来查看调用栈（call stack）中最新的文件实体，解析出文件名。然后，使用正则表达式扫描（scan）提取出所有的内联模版（inline templates）并追加到 h 哈希数组中，被记忆在 $t 中；这个记忆意味着解析将在一次性执行，而不是每次都被解析。\n扩展它，使用更清晰的变量名，下面是我们重写后的：\n$templates ||= begin templates = {} File.read(caller[0][/^[^:]+/]).scan(S) do |template_name, template| templates[template_name] = template end templates end 接下来，得到解析好的模版：\nv[0].new(*o){n==\u0026quot;#{n}\u0026quot;?n:$t[n.to_s]}.render(a,o[0].try(:[],:locals)||{}) Tilt 允许你指定一种模版类型 —— “erb”，比如 —— 取回需要初始化解析这些模版的类。这个类在 $v[0] 中；它被初始化，一个包含模版数据的代码块（block）传递给它。渲染（render）方法将被调用，作为解析好的值传递给应用，任何本地变量被赋值和传递给它（如果没有本地变量，那就传递控哈希数组给它）。\n下面这个就相对不太有趣了。\n%w[set enable disable configure helpers use register].map{|m|D.(m){|*_,\u0026amp;b|b.try :[]}}; 这里定义更多的全局方法；目前为止，我们都在使用 D.(m)。但是你可能注意到了，这些方法实际上都是一样的，处理执行传递给它的代码外，什么都不做；这个意味着他们实际上没有什么差别：\nconfigure do enable :sessions end 等同于：\nhelpers do enable :sessions end 这个和 sinatra 实际实现是不同的，但是提供了足够的兼容性。但这也意味着你不能在 Almost Sinatra 中的 configure 中修改环境变量 —— 就这点代码了，你也不能要求太多了！\nEND{Rack::Handler.get(\u0026quot;webrick\u0026quot;).run(a,Port:$z){|s|$r=s}} 上面是 END 代码块（block），这是为了让代码更少而不不使用标准的 at_exit；这个的作用是启动 web 服务器。我们看到把服务器实例赋给 $r 变量，就是我们在第一行 trap 语句看到的那个变量。\n其中使用一些精简技巧：run 的第二个参数是一个数组，其中的花括号被省略（完全有效，也确实相当常见）以及 Ruby 1.9 数组语法被使用，这里 Port: $z 等同于 :Port =\u0026gt; $z。\n%w[params session].map{|m|D.(m){q.send m}}; 定义了 params 和 session 方法，并被传递给 q（它的定义在后面，被设置成请求对象）。你也许已经注意到了，回到第二行，我们看到不对称的赋值语句，4 个变量只有 3 个被赋值：\na,D,S,q=Rack::Builder.new,Object.method(:define_method),/@@ *([^\\n]+)\\n(((?!@@)[^\\n]*\\n)*)/m 这个效果等同于把 nil 赋值给 q（变量定义了，而不分配任何值给它）。It allows you to scope a variable in two characters, rather than the minimum of three or four you’d need to assign an actual value to it. 它允许你用两个字符界定一个变量，而不是需要最少也要 3 到 4 个字符来给他分配一个实际值。\na.use Rack::Session::Cookie;a.use Rack::Lock; Sinatra 引用了 Rack 回话的 Cookie ，并使用 Rack::Lock 来同步请求。\nD.(:before){|\u0026amp;b|a.use Rack::Config,\u0026amp;b};b 定义一个 before 方法将逻辑委托给 Rack::Builder，调用 use 来载入中间件（middleware）。\n然后他使用这个 before 句柄载入他自己的中间件（middleware）：\nbefore{|e|q=Rack::Request.new e;q.params.dup.map{|k,v|params[k.to_sym]=v}}} 将新建请求（Request）对象，并分配给 q，然后将参数中的键转变成变量（symbol）。\n代码到这里就结束了；实际上，你需要 END 代码块（block）重新调用服务；但是如果没有后续的脚本，END 将会被自动触发启动服务，这样代码又节省了！\nThat\u0026rsquo;s it；我们的 Almost Sinatra 之旅已经结束。我不知道对你是否有帮助，但是我学习了不少东西。我学会如何使用 Rack::Builder；动态绑定循环来精简代码；调用方法对象来减少 method.() 和 method.call 的代码；以及 \u0026quot;#$foo\u0026quot; 等同于 \u0026quot;#{$foo}\u0026quot;；等等。\n但是，不仅仅是学些这些技巧。Almost Sinatra 的 Github 页面的开篇：\nUntil programmers stop acting like obfuscation is morally hazardous, they’re not artists, just kids who don’t want their food to touch.\n认为混淆是罪恶的程序员不能称为艺术家，仅仅是不许他人动自己食物的小孩罢了。（摘自 ruby-china）\n通过阅读这个代码和解构它，我也深有此感触。混沌有时是一种美德：间接影射常常比直率的语句更加强大。但是最重要的是：它真的很有乐趣！\n译自 https://robm.me.uk/2013/12/13/decoding-almost-sinatra.html\n","date":"2014-10-19","permalink":"/n3xtchen/2014/10/19/ruby---decoding-almost-sinatra/","section":"时间线","summary":"Science!","title":"Ruby - 解读 Almost Sinatra"},{"content":"","date":"2014-10-19","permalink":"/n3xtchen/tags/sinatra/","section":"标签","summary":"","title":"sinatra"},{"content":"","date":"2014-10-17","permalink":"/n3xtchen/tags/php/","section":"标签","summary":"","title":"php"},{"content":" 译自 http://www.sitepoint.com/7-reasons-choose-yii-2-framework/\n在过去的一年中， SitePoint 发布了一篇介绍 最流行的 PHP 框架的文章。在当时，Yii 的最新稳定版本是 1.1.14。现在 Yii 2.0 已经分布，所以你可以开始在产品中使用它。\n在 RC 版的时候，我们已经在谈论它了。现在正式发布了，我们觉得应该重新和大家谈谈选择 Yii2 的一些原因了。\n1. 易于安装 # 对于 Web 开发者来说，时间就是金钱，没人愿意把宝贵的时间浪费在繁琐的安装和配置上。\nYii2 使用 Composer 来安装。如果你想要详细的安装教程，Sitepoint 提供了一个很棒的 文章。如果你的开发模式是前后端分离的，我倾向使用基本的应用模版（Basic）。同时，我建议使用一个模块（Module）作为我们的后端部分。（Yii 模块（Module）就像一个微应用进驻在你的主应用中一样。）\n2. 利用最主流的技术 # Yii 是一个纯面向对象（OOP） 框架，很好地利用了 PHP 的一些最新特性，包括后期静态绑定（late static binding）， SPL类和接口（SPL classes and interfaces）以及匿名函数（anonymous functions）。\n所有的类（Class）都被命名空间（namespace）化了，让你充分利用兼容 PSR-4 的自动载入（Autoloader）方式。这意味着很容易就能引用 Yii 的 HTML Helper：\nuse yii\\helpers\\Html; Yii 也允许你定义别名来简化你的命名空间（namespae）。上述例子中，use 语句从 /vendor/yiisoft/yii2/helpers 载入一个类定义。这个别名定义在 BaseYii 类的第 79 行：\npublic static $aliases = ['@yii' =\u0026gt; __DIR__]; 框架本身是使用 Composer 安装的，就像扩展一样。发布扩展的方式同样很简单；你只需简单地编写自己 composer.josn ，代码上传到 Github 上，登记到 Packist 包列表中（供大家下载使用）。\n3. 高可扩展性 # Yii 就像一件非常合身的西装，但是裁缝也容易根据你的特殊需求进行订制。实际上，框架的每一个组件都是可扩展的。一个简单的例子，你想要为你的视图（View）添加唯一标识 ID。（这时，你可能很疑惑这么做的用意，请参见 Css-Trick）\n首先，我在我的 app\\componts 目录中中创建一个名为 View.php 文件，把下面的代码：\nnamespace app\\components; class View extends yii\\web\\View { public $bodyId; /* Yii allows you to add magic getter methods by prefacing method names with \u0026quot;get\u0026quot; */ public function getBodyIdAttribute() { return ($this-\u0026gt;bodyId != '') ? 'id=\u0026quot;' . $this-\u0026gt;bodyId . '\u0026quot;' : ''; } } 然后，在你的主布局（Layout，app\\views\\layouts\\main.php）文件中，添加下面语句到你的 Body 标签中：\n\u0026lt;body \u0026lt;?=$this-\u0026gt;BodyIdAttribute?\u0026gt;\u0026gt; 最后，我将把下面嫁到我的主配置文件中，让 Yii 可以识别它，并将它替换默认的 View 。\nreturn [ // ... 'components' =\u0026gt; [ // ... 'view' =\u0026gt; [ 'class' =\u0026gt; 'app\\components\\View' ] ] ]; 4. 鼓励测试 # Yii 与 Codeception 紧密整合在一块。Codeception 是一个非常棒的 PHP 测试框架，它简化了创建单元测试（Unit Test），功能测试（Functional Test）和验收测试（Acceptance Test）的过程。因此你将为你所有的应用编写自动化测试脚本，对吧？\nCodeception 扩展简化了配置，只需简单地编辑 /tests/_config.php 文件就可以了；例如：\nreturn [ 'components' =\u0026gt; [ 'mail' =\u0026gt; [ 'useFileTransport' =\u0026gt; true, ], 'urlManager' =\u0026gt; [ 'showScriptName' =\u0026gt; true, ], 'db' =\u0026gt; [ 'dsn' =\u0026gt; 'mysql:host=localhost;dbname=mysqldb_test', ], ], ]; 使用上述配置，将会导致如下结果：\n在功能测试（Functional Test）和验收测试（Acceptance Test）的过程中，任何要发送的邮件都不会被发送，将写到文件中代替。 你在测试中的 URL 展示格式将会是 index.php/controller/action，而不是 /controller/action。 你将在你的测试数据库中测试，而不是你的生产环境的数据库中。 一个特殊的模块（Module）都共同存于 Yii 框架和 Codeception 中。它给 TestGuy 类增加了一些方法，帮助你在功能测试（Functional Test）中操纵 Active Record (Yii 的 ORM)。举个例子，如果你想要查看你的注册表单是否成功创建个叫做 “testuser” 的用户，你可以这么做：\n$I-\u0026gt;amOnPage('register'); $I-\u0026gt;fillField('username', 'testuser'); $I-\u0026gt;fillField('password', 'qwerty'); $I-\u0026gt;click('Register'); $I-\u0026gt;seeRecord('app\\models\\User', array('name' =\u0026gt; 'testuser')); 5. 简化安全方案 # 安全性对于任何 Web 应用，都是很关键的部分；幸运的是， Yii 本身就包含一些很棒的特征来帮你排忧解难。\nYii 携带一个安全应用组件，它包含一些方法来协助你创建一个更安全的应用。下面列举了一些创建有用的方法：\ngeneratePasswordHash：从一个密码和一个随机盐（Salt）中生成一个安全哈希值。这个方法将为你生成一个随机盐（Salt），然后使用 PHP 的 crypt 函数来为提供的字符创建一个哈希值。 validatePassword：这个是 generatePasswordHash 的伴侣函数，它允许你检查用户提供的密码匹配你的存储的哈希值。 generateRandomKey：允许你创建任何长度的随机字符。 Yii 会自动验证来不安全的 HTTP 请求方法（PUT，POST 和 DELETE）CSRF 令牌的有效性；当你使用 ActiveForm::begin() 方法来创建你的表单标签时，它将生成和输出一个令牌。这个特征你可以通过编辑你的主配置文件来禁用它：\nreturn [ 'components' =\u0026gt; [ 'request' =\u0026gt; [ 'enableCsrfValidation' =\u0026gt; false, ] ]; 为了防止被跨境攻击（XSS），Yii 提供了另一个辅助类 HtmlPurifier。这个类有一个静态方法 process，它使用与它同名的 流行 PHP 的过滤库来过滤你的输出。\nYii 也包含开箱即用的用户认证和认证类。认证（Authorization）分两种：访问控制过滤器（ACF，Access Control Filter）和基于角色的访问控制器（RBAC，Role-Based Access Control）。\n访问控制过滤器（ACF）是两个中最简单的，通过在你控制器（Controller）中添加下述方法来实现：\nuse yii\\filters\\AccessControl; class DefaultController extends Controller { // ... public function behaviors() { return [ // ... 'class' =\u0026gt; AccessControl::className(), 'only' =\u0026gt; ['create', 'login', 'view'], 'rules' =\u0026gt; [ [ 'allow' =\u0026gt; true, 'actions' =\u0026gt; ['login', 'view'], 'roles' =\u0026gt; ['?'] ], [ 'allow' =\u0026gt; true, 'actions' =\u0026gt; ['create'], 'roles' =\u0026gt; ['@'] ] ] ]; } // ... } 上述的代码告诉 Defaultcontroller 允许访客用户访问 login 和 view 动作（action），但是不允许访问 create 动作（action）。（? 是匿名用户的别名，@ 代表认证的用户）。\nRBAC 是一种控制不同类型用户执行特定动作的强大方法。它需要为你的用户创建角色，为你的应用定义权限，并且为相应的角色开通相应的权限。如果你想要开通 Moderator（评分者）角色的话，你可以使用这个方法，并为所有用户分配这个角色来投票这个博客（作者对自己的文章非常有自信，^_^）。\n你也可以使用 RBAC 来定义角色，允许用户在特定的条件分配特定的权限。举个例子，你应该创建一个规则允许所有用户编辑他们自己的文章，但不能编辑别人创建的文章。\n6. 缩短开发时间 # 大部分项目都包含大量的重复任务（没人想要在那上面浪费时间）。Yii 赋予了我们一些工具来协助我们花费更少的时间在这些工作上，而是花更多时间在定制你的应用来满足客户的需求。\nGii 就这些最强大工具中的一种。Gii 是一个基于 Web 的脚手架工具，它允许你快速的创建代码模版：\n模型（Models） 控制器（Controllers） 表单（Forms） 模块（Modules） 扩展（Extensions） CRUD 控制器（Controller）动作（action）和视图（views） Gii 是高度可定制化的。你可以将它设定在特定的环境（一般在开发环境中被启用），只需要像下面代码那样简单的编辑就可以了：\nif (YII_ENV_DEV) { // ... $config['modules']['gii'] = [ 'class' =\u0026gt; 'yii\\gii\\Module', 'allowedIPs' =\u0026gt; ['127.0.0.1', '::1'] ] } 它确保了 Gii 只在 Yii 环境设置成开发环境时才被载入，并且也只有在本地环境才能被访问。\n现在让我们看看模型（Model）生成器：\n它的表名使用响应式点击挂件来尝试猜测你的模型（model）相关联的表（指数据库中的表），所有的字段都会以弹出提示的方式来指导你如何填写。你可以在要求 Gii 生成代码之前预览生成的代码，所有的代码模版都是完全可定制的。\n也有一些命令行工具用来创建代码模版来协助数据库迁移，信息翻译（l18N）和创建用户自动化测试的数据库基境（Fixtures）。举个例子，你可以使用下面的命令来创建一个心的数据库迁移文件：\nyii migrate/create create_user_table 它将会在 {appdir}/migrations 创建一个新的迁移模版：\n\u0026lt;?php use yii\\db\\Schema; class m140924_153425_create_user_table extends \\yii\\db\\Migration { public function up() { } public function down() { echo \u0026quot;m140924_153425_create_user_table cannot be reverted.\\n\u0026quot;; return false; } } 因此，我们可以为这个表添加一些字段；我只需要在 up 方法中添加代码：\npublic function up() { $this-\u0026gt;createTable('user', [ 'id' =\u0026gt; Schema::TYPE_PK, 'username' =\u0026gt; Schema::TYPE_STRING . ' NOT NULL', 'password_hash' =\u0026gt; Schema:: TYPE_STRING . ' NOT NULL' ], null); } 然后我们需要确保我们正常迁移是可逆的，我们可以编辑 down 方法：\npublic function down() { $this-\u0026gt;dropTable('user'); } 你只需要执行一下简单的命令就可以创建这个表了：\n./yii migrate 以及删除这个表：\n./yii migrate/down 7. 简单的调整就可以达到很好的性能 # 每一个人都知道缓慢的网页导致不良的用户体验，因此 Yii 提供了一些工具来帮助你的应用获得更高的性能。\n所有的 Yii 的缓存组件都继承 yii/caching/Cache 类，允许选择不同的缓存系统，并且使用通用接口（API）来调用。你可以同时定义多个缓存组件。Yii 现在支持数据库和文件缓存，同样也支持 APC，Memcache，Redis，WinCache，XCache 和 Zend Data Cache。\n默认情况下，如果你使用 Active Record，然后 Yii 会执行额外的查询来确定所生成模型（Model）对应的表结构。你可以通过配置主配置文件来设置应用缓存这些表结构：\nreturn [ // ... 'components' =\u0026gt; [ // ... 'db' =\u0026gt; [ // ... 'enableSchemaCache' =\u0026gt; true, 'schemaCacheDuration' =\u0026gt; 3600, 'schemaCache' =\u0026gt; 'cache', ], 'cache' =\u0026gt; [ 'class' =\u0026gt; 'yii\\caching\\FileCache', ], ], ]; 最后，Yii 提供命令行工具来压缩前端资源（assets），只须简单运行下面命令来生成配置模版：\n./yii asset/template config.php 然后编辑配置来指定你想要使用的压缩工具（例如，Closure 编译器，Yui 压缩器或者 UglifyJS）。产生的配置模版就想下面那样：\n\u0026lt;?php return [ 'jsCompressor' =\u0026gt; 'java -jar compiler.jar --js {from} --js_output_file {to}', 'cssCompressor' =\u0026gt; 'java -jar yuicompressor.jar --type css {from} -o {to}', 'bundles' =\u0026gt; [ // 'yii\\web\\YiiAsset', // 'yii\\web\\JqueryAsset', ], 'targets' =\u0026gt; [ 'app\\config\\AllAsset' =\u0026gt; [ 'basePath' =\u0026gt; 'path/to/web', 'baseUrl' =\u0026gt; '', 'js' =\u0026gt; 'js/all-{hash}.js', 'css' =\u0026gt; 'css/all-{hash}.css', ], ], 'assetManager' =\u0026gt; [ 'basePath' =\u0026gt; __DIR__, 'baseUrl' =\u0026gt; '', ], ]; 然后运行命令来进行压缩任务：\n./yii asset config.php /app/assets_compressed.php 最后，编辑的你 Web 应用配置文件使用压缩后的资产（assets）。\n'components' =\u0026gt; [ // ... 'assetManager' =\u0026gt; [ 'bundles' =\u0026gt; require '/app/assets_compressed.php' ] ] 注：这个步骤你需要手动安装外部的工具。\n结语 # 和其他优秀的框架一样，Yii 帮助你快快速构建现代的 Web 应用，并确保优秀的性能。它通过提供一系列为你减负的工具促使你创建安全可测试的站点。你可以很容易使用它提供的所有特性，也可以修改它们来满足你的需求。我真心推荐在你的戏一个 web 项目中使用它。\n你尝试过 Yii 2 了吗？那请让我们知道！\n","date":"2014-10-17","permalink":"/n3xtchen/2014/10/17/php-7-reasons-to-choose-the-yii-2-framework/","section":"时间线","summary":"译自 http://www.","title":"PHP - 选择 Yii2 框架的7大原因"},{"content":"","date":"2014-10-16","permalink":"/n3xtchen/tags/rail/","section":"标签","summary":"","title":"rail"},{"content":" 译自 http://www.sitepoint.com/understanding-sql-rails/\n结构化的关系数据库现在已经无处不在了；它就是大家所说的用于 Web 前端接口的后端数据库；在无状态协议中（如 HTTP），你的数据库可以用来持久保存状态；它是机器后面的大脑。\n在 MVC 模式的传统场景中，你的数据模型（Model 层）用来解决一部分问题。你从数据库获取数据的方式直接影响到整体的方案。如果你的数据关系错综复杂和冗余，它会严重影响前端代码的实现。\n因此，我将在这里探讨 SQL 数据库。众所周知，Rails 提供很好的方案来解决此类问题。框架让我们更容易使用数据模型（Models）和关系（Relations）的方式来思考真实场景的问题。\n数据模型中的关系（Relation）让你的模型（Model）成为真实世界的对象。更令人兴奋的是它很好地适应 OOP 面向对象的开发范型。它简化了代码库，使你的架构更加智能和敏捷。\n我希望在这个文章结束的时候，你将会增加对关系数据库的了解；它将让你认识到你的后端结构存储对实际架构的重要性。\n接下来，我们都想设计一个漂亮的架构，Right？那就开始吧！\n安装 # 首先你需要有一台装好 Ruby 和 Rails 的 PC 机，下面是我的环境参数：\n$ ruby -v ruby 2.1.1p76 (2014-02-24 revision 45161) [x86_64-darwin12.0] $ gem -v 2.2.2 $ rails -v Rails 4.1.6 然后创建你的项目：\n$ rails new understanding_sql_through_rails 这是一个特定的例子，它实际上使一个简单的博客系统。它由 用户（Users），博客（Posts） 和 分类（Catagories） 组成。这样，我们先粗略地定义下数据结构和关系。\n我们的首要目的是 MVC 框架的 Model 设计。\n我们先用 Rails 创建需要的模型（Model）和关系（Relations）：\n$ rails g model User name:string{30} $ rails g model Post title:string{40} content:text{3200} user:references:index $ rails g model Category name:string{15} $ rails g migration CreateCategoriesPostsJoinTable categories:index posts:index 作为最佳实践，我建议为你的字段设置最大长度。在结构化的数据库中，如果你设置了合理的字段大小，计算机将更容易计算索引数据的时间。结构化数据集的最大好处就是每条数据只占用设定的空间，因此它更容易被查询。我的建议是最大程度的使用它。\n现在，开始生成数据库：\n$ rake db:migrate 简单。Rails 已经帮我们把所有需要的代码都生成好了。你可以浏览下下你创建的项目目录下生成的代码。最让人兴奋的是，通过简单的命令，你就能完成你的关系数据库的配置。\n为了确保你创建的模型（Model）中的关系（Relation）是正确的，你修改下你的代码文件，修改如下：\n# app/models/category.rb 这是代码路径，下面类推 class Category \u0026lt; ActiveRecord::Base has_and_belongs_to_many :posts\t# 添加这一行 end # app/models/post.rb class Post \u0026lt; ActiveRecord::Base belongs_to :user\thas_and_belongs_to_many :categories\t# 添加这一行 end # app/models/user.rb class User \u0026lt; ActiveRecord::Base has_many :posts\t# 添加这一行 end 通过上述，把这些数据的关系映射到 Rails 代码中。这个很重要，你需要记住。比如，一个用户（User）由很多帖子（Post），一个帖子（Post）属于一个用户（User）。\n将下面代码追加到 db/seeds.rb 中：\nuser = User.create({name: 'Joe'}) post = Post.create({title: 'Post', content: 'Something', user_id: user.id}) Category.create({name: 'Cat1', post_ids: [post.id]}) 然后执行它：\n$ rake db:seed 关系（Relations） # 在 Rails 中，你的整个数据都安装设置好了。现在，是时候深入探讨其中的原理。框架很好地抽象了这一层。为了更好了解它，我们需要看看它是如何被实现地：\n$ rails console Loading development environment (Rails 4.1.6) 2.1.1 :001 \u0026gt; post = Post.first 2.1.1 :002 \u0026gt; post.categories 2.1.1 :003 \u0026gt; category = Category.first 2.1.1 :004 \u0026gt; category.posts 2.1.1 :005 \u0026gt; user = User.first 2.1.1 :006 \u0026gt; user.posts 框架可以触发多种 SQL 关系查询。正如你所见，一篇博客（Post）有并且属于多个分类（Category）；一个分类（Category）有和属于多篇博客（Post）。一个用户（User）有多篇博客（Post）。\n这些关系非常直观，就像你用标准的英语描述他们一样。最激进的是你完全不需要知道底层的实现技术，只需要了解对应的问题所属域模型（Domain Model）。\n在任何解决方案中，领域模型（Domain Model）由对象（Object）和关系（Relations）组成；它用来关联用户的媒介；它加速了的人机沟通，以至于你不需要关心如何解决他们的问题。\n客户和个人只关心你的终端产品是否更好地服务他们。客户更乐见把一个问题对应到相关到领域。这意味着作为架构师的你能够将真实世界的问题关联到计算机语言中。\n无需多说，你只需要记住模型（Model）是如何解决你的问题的？\n在某种形式上，你的 SQL 数据库也是追踪数据之间关系（Relation）的一种方式。\nSQL 查询 # $ rails dbconsole 它将带你进入 SQL 的世界而不是 Rails。SQLite 应该是 Rails 默认的数据库。\n如果你想要查询某一个用户（User）发布的博客（posts）：\n-- 这是 SQL 语句 SELECT id, title FROM posts WHERE posts.user_id = 1; 如上展示的，当一个子对象归属一个父对象，子对象从父对象获取一个外健（Foreign Key）。\n现在来点有难度的，我想要属于 Cart1 分类（Category）的博客（Post）：\n-- 这是 SQL 语句 SELECT posts.id, posts.title FROM categories, posts, categories_posts WHERE categories.id = categories_posts.category_id AND posts.id = categories_posts.post_id AND categories.name = 'Cat1'; 我使用隐含的连接查询（Join）。在任何所给的数据表中，id 是主键（Primary Key）。在实际场景中，我仅使用 SQL 做简单的信息查询。如果要做的更多那意味着把业务逻辑（Business Logic）放到 SQL 中。我建议你尽可能避免这样。你的领域逻辑（Domain Logic）不应该属于 SQL 层。\n在这个基础上，你应该尽可能高效利用 SQL 关系，来避免数据冗余。每次你看到你的表中存在很多重复的数据，那你就该停下来，重新思考下数据结构的设计。\n总的来说，这就是结构化的关系数据库能为你做的事情。\n优化 # 你可能已经注意到我们每个 Model 创建脚本后面都有一个 :index。我把他们加到模型（Model）生成器中。实际上，它们是索引（Index）。在 SQL 中，当你通过关系查询数据的时候，索引（Index）将会优化你的查询。注意，每一个主键默认都是一个索引，但是外健不是。\n在 dbconsole 中，查询查询创表语句（这个实际上是 Sqlite 的语法）：\n.schema 你将会看到：\nCREATE TABLE \u0026quot;categories\u0026quot; (\u0026quot;id\u0026quot; INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \u0026quot;name\u0026quot; varchar(15), \u0026quot;created_at\u0026quot; datetime, \u0026quot;updated_at\u0026quot; datetime); CREATE TABLE \u0026quot;categories_posts\u0026quot; (\u0026quot;category_id\u0026quot; integer NOT NULL, \u0026quot;post_id\u0026quot; integer NOT NULL); CREATE TABLE \u0026quot;posts\u0026quot; (\u0026quot;id\u0026quot; INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \u0026quot;title\u0026quot; varchar(40), \u0026quot;content\u0026quot; text(3200), \u0026quot;user_id\u0026quot; integer, \u0026quot;created_at\u0026quot; datetime, \u0026quot;updated_at\u0026quot; datetime); CREATE TABLE \u0026quot;schema_migrations\u0026quot; (\u0026quot;version\u0026quot; varchar(255) NOT NULL); CREATE TABLE \u0026quot;users\u0026quot; (\u0026quot;id\u0026quot; INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \u0026quot;name\u0026quot; varchar(30), \u0026quot;created_at\u0026quot; datetime, \u0026quot;updated_at\u0026quot; datetime); CREATE INDEX \u0026quot;index_categories_posts_on_category_id_and_post_id\u0026quot; ON \u0026quot;categories_posts\u0026quot; (\u0026quot;category_id\u0026quot;, \u0026quot;post_id\u0026quot;); CREATE INDEX \u0026quot;index_categories_posts_on_post_id_and_category_id\u0026quot; ON \u0026quot;categories_posts\u0026quot; (\u0026quot;post_id\u0026quot;, \u0026quot;category_id\u0026quot;); CREATE INDEX \u0026quot;index_posts_on_user_id\u0026quot; ON \u0026quot;posts\u0026quot; (\u0026quot;user_id\u0026quot;); CREATE UNIQUE INDEX \u0026quot;unique_schema_migrations\u0026quot; ON \u0026quot;schema_migrations\u0026quot; (\u0026quot;version\u0026quot;); 因此，如果索引（Index）出现在你的领域模型（Domain Model）设计中的时候，你应该是提高性能方案方面的专家了!^_^\n结语 # 这就是今天的全部内容了。我希望你能看到 SQL 的强大之处，以及如何高效地在现实场景中使用它。\nHappy Hacking!\n","date":"2014-10-16","permalink":"/n3xtchen/2014/10/16/ruby---understanding-sql-through-rails/","section":"时间线","summary":"译自 http://www.","title":"Ruby - 通过 Rail 来学习 SQL"},{"content":" 译自 Hacker\u0026rsquo;s Guide to Setting up Your Mac\n骇客总是痴迷于自动化（Automation）。我们想要一个机器人帮我们完成繁重的工作，是的我们可以专注有趣的事情上。在自动化领域，自动化配置领域已经非常的成熟了！\n几天我将要大家展示一些将自动化应用到 Mac 安装配置的技术。这篇文章的目的是实现 80 ％ 的自动化，让你在几个小时内完成新 Mac 的配置，而不是花上几天的时间。\n我们的工具箱 # 这个博客使用如下开源工具来自动化配置你的 Mac：\n使用 Homebrew 安装源程序 使用 Homebrew cask 安装应用 使用 mackup 备份和还原配置 使用 osx-for-hacker.sh 优化 Mac 的默认配置 使用 dots 将它们整合在一起 使用 Homebrew 安装源程序 # Homebrew 是一个社区驱动的包安装工具，一个每一个黑客都需要拥有的工具。Homebrew 自动化了安装，编译和链接源代码。它也可以简化更新和卸载源程序。\n首先你需要把它安装在你的新 mac 上。将下面的代码片段黏贴到 shell 脚本文件中，执行它来验证你是否安装了 homebrew：\n# 检查 Homebrew, # 如果没有，安装它 if test ! $(which brew); then echo \u0026quot;Installing homebrew...\u0026quot; ruby -e \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026quot; fi # Update homebrew recipes brew update 然后，你需要做的就是更新你已经安装的 unix 工具。由于 shellshock 的 bash 漏洞，这一步是至关重要的。下面是更新 unix 工具的 shell 代码片段：\n# Install GNU core utilities (those that come with OS X are outdated) brew install coreutils # 安装 GNU `find`, `locate`, `updatedb`, and `xargs`, g-前缀 brew install findutils # Install Bash 4（10.9 默认版本是 3.2） brew install bash # 更新 grep brew tap homebrew/dupes\t# 添加新源 brew install homebrew/dupes/grep 安装完，你需要更新环境变量 $PATH，使你刚才安装的工具生效：\n# ~/.bash_profile $PATH=$(brew --prefix coreutils)/libexec/gnubin:$PATH 上述创建了 Mac 系统的基础。你也可以通过 Homebrew 安装其他工具来改善你的工作流。下面是 MATTHEW MUELLER 推荐的工具：\nbinaries=( graphicsmagick\t# 各种图片处理 webkit2png\t# 网页截屏 rename\t# 重名名工具 zopfli\t# 压缩工具，压缩比最高的算法 ffmpeg\t# 视频工具 sshfs\t# 文件系统远程挂载 trash\t# 删除工具，rm 造成不可挽回的后果，trash 可以设置缓存目录 node\t# nodejs tree\t# 展示文件树 ack\t# grep 类的工具，性能更强劲，纯 perl 写的 hub\t# git 工具的封装，添加更多人性化的功能 git\t# 这个就不啰嗦了 ) echo \u0026quot;installing binaries...\u0026quot; brew install ${binaries[@]} 所有都安装完，你需要清理所有东西：\nbrew cleanup 使用 Homebrew cask 安装应用 # Homebrew Cask 是 Homebrew 的一个扩展，它帮助你自动安装 Mac 应用和字体。\n装好 Homebrew 的前提下，你执行下面的命令来安装它：\nbrew install caskroom/cask/brew-cask Homebrew Cask 的应用库非常丰富，并且每天都在增长。你可以从它的官方源中查看可安装的应用，或者你可以通过下述命令搜索你要的应用：\nbrew cask search /google-chrome/ 对应用的需求因人而异，下面 MATTHEW MUELLER 推荐的应用：\n# Apps apps=( alfred dropbox google-chrome qlcolorcode # 代码高亮预览工具 screenflick # 屏幕录制，限制键盘操作 slack # 企业沟通工具，不错 transmit # 远程传输工具，之一 xFTP, S3 等等，付费 appcleaner # 应用清理 firefox hazel # 文件按规则分类移到设定的目录，付费 qlmarkdown # 预览 markdown seil # 将 caps lock 替换成其他键盘 vagrant # 虚拟机管理 arq # 云备份工具 flash iterm2 # 强大的终端，支持分屏， qlprettypatch # 快速查找和高亮文件内容 shiori # 书签和自动填单工具，支持 safari，chrome 和 firefox sublime-text3 # 最好用的编辑器之一，不是我的菜（vim 党） virtualbox # 虚拟机工具 atom # 编辑工具的新秀 flux # 调整屏幕适应你的作息 mailbox qlstephen # 识别和预览位置类型的文件 sketch # UI 设计工具 tower # 版本控制的管理工具（Git），付费 vlc # 跨平台视屏比方工具 cloudup # 流分享工具 nvalt # 快速笔记工具 quicklook-json # json 美化查看工具 skype transmission # 下载工具 # 下面是我补充的 filezilla # 免费的远程传输工具 sequel-pro # mysql 客户端工具 mou # markdown 编辑工具 ) # Install apps to /Applications # Default is: /Users/$user/Applications echo \u0026quot;installing apps...\u0026quot; brew cask install --appdir=\u0026quot;/Applications\u0026quot; ${apps[@]} 如果你你想安装测试版的应用。你需要添加版本源：\nbrew tap caskroom/versions 提醒下 Alfred 用户 # 如果你是 Alfred 用户，你可能需要注意下：你不能通过 Alfred 访问 cask 安装的应用，因为这些应用的安装位置并不是 /Applications 而是 /opt/homebrew-cask/Caskroom/。\n你可以使用下面的命令来把这个路径添加到 Alfred 中：\nbrew cask alfred link 附赠：安装字体 # Cask 页可以用来自动下载和安装字体。因此，你需要添加字体的 cask 源：\nbrew tap caskroom/fonts 字体库都是以 font- 作为前缀的，因此如果你想要下载 Roboto 字体，可以使用如下命令：\nbrew cask search /font-roboto/ 这里是我安装字体的方法：\n# fonts fonts=( font-m-plus font-clear-sans font-roboto ) # install fonts echo \u0026quot;installing fonts...\u0026quot; brew cask install ${fonts[@]} 你可以从 这里找到所有的字体库列表。\nMackup # Mackup 是用户系统和应用配置的开源工具。你可以从 lra/mackup查看它所支持的应用。\n现在你可以使用 Homebrew 来安装 mackup：\n$ brew install mackup 你还可以使用 pip 来安装:\n$ pip install mackup 如果 pip 不可用，你可能需要使用 brew 来安装 python，它默认集成 pip。\n默认情况，mackup 将你的配置文件备份到你的 Dropbox 中，因此你需要实现安装 Dropbox。一旦你的 Dropbox 被安装，备份工作将会是非常简单的：\nmackup backup 这个命令将会匹配查找你安装的应用，并拷贝到 ~/Dropbox/Mackup 目录下。\n通过下述命令还原你的配置到制定的 Mac 中：\nmackup restore 使用 osx-for-hacker.sh 修改 Mac 的默认配置 # osx-for-hackers.sh 由 Brandon Brown 基于 Mathias Bynens 著名的 dotfile 修改的。\n脚本作者的初衷是优化 Mac 的默认配置，禁用了一些不需要的配置，加速键盘反应和桌面效果。\n但是有人反应使用完导致电池续航能力下降，禁用了某些文件的默认备份，可能会导致重要信息丢失。\n介于对其中一些配置以及 Applescript 不是很熟悉，这里就不多加评论。执行前，务必实现对代码细节了解清楚。\n使用 dots 将它们整合在一起 # dots 是将上述的所有操作配置整合到一块。dots 的开发者的想要把 dots 打造成 Mac 和 ubuntu 的初始化工具。它不对外部依赖，所以它能在很多分发版本中运行。使用下述命令安装：\n(mkdir -p /tmp/dots \u0026amp;\u0026amp; cd /tmp/dots \u0026amp;\u0026amp; curl -L# https://github.com/matthewmueller/dots/archive/master.tar.gz | tar zx --strip 1 \u0026amp;\u0026amp; sh ./install.sh) dots boot osx 结语 # 通过自动化配置，你可以迅速让你的新 Mac 跑起来。请及时更新最新的补丁，降低与你工作伙伴的环境差异。\n","date":"2014-10-05","permalink":"/n3xtchen/2014/10/05/hack-mac---automatic-setting/","section":"时间线","summary":"译自 Hacker\u0026rsquo;s Guide to Setting up Your Mac","title":"Mac 骇客指南 - 自动化配置"},{"content":"当我们学习很多东西的时候，大家都很想了解它的底层具体发生的事情。比如，“真的“ 和 ”为什么” 这些词经常会存在对话中 —— “当我运行一个列表，实际发生了什么？”，“为什么函数调用需要考虑成本？”。如果看完下面的部分，你就知道我有多喜欢挖掘 Python 的底层，并且我永远喜欢和别人分享这些。\n为什么要深挖底层？ # 首先，不要以为了解 Python 的底层，就可以成为好的 Python 开发。很多你学习的东西并不能帮护你编写更好的代码。底层结构非常的特别，也很似是而非 —— 为什么只在 Python 底层止步呢？你能知道一些 C，会更完美，如 C 编译器，汇编语言，……\n换句话说，我认为你应该更加了解 Python —— 她有时会帮助你写出更好的 Python，如果你想贡献代码，这也会为你打好基础；更重要的是，她真的很有趣。\n安装 # 首选你得先有一个 Python，会看这篇文章的人，应该都有环境了。\n策略 # 1. 自然主义 # Peter Seibel 有一个很好的关于 读代码的博客。他认为“读”并不是与代码最好的交互方式 —— 相反，他们解剖他。\n2. 科学 # 我是假说学的大粉丝 —— 调试驱动，这个思想也可以用于探索 Python。我觉得你不应该坐下来随便读几段代码。相反，你要带着问题和假设进入代码库。没意见事情你都需要抱着怀疑的态度，猜测它如何实现，最后验证或者驳斥你的假设。\n3. 向导式的探索 # 按照一步一步的指导，以不断的调试和理解代码。推荐大家看一下 Amy Hanlon 的博客（其中讲到如何修改 Python 内置函数的语句）和 Eli Bendersky 的博客（其中讲到如何添加自己的方法在 Python 内置函数中）；这样我们可以轻易地追踪代码，了解代码具体实现和运行轨迹。（接下来也会翻译这两篇文章）\n4. 推荐你最喜欢的模块实现 # 如果你读到觉得实现很棒的模块，你应该毫不客气地推荐给大家。如果你已经有自己觉得很棒的实现，你可以 @我（ @n3xtm3）。下面 akaptur 推荐的两个模块：\ntimeit in Lib/timeit.py namedtuple in Lib/collections.py 5. 写博客和演讲 # 你学到那些有趣的东西了吗？把它写下来并分享它，或者在本地社区交流会中演讲它！你可能感觉到其他人已经知道你所知道的一切，没什么好分享的，但是相信我，这不是真的，你所分享总会帮助到别人的，总有人不懂的。\n6. 重构 # 当你阅读代码实现之前，你可以尝试按自己理解先实现一个。这样可以加深你的印象。\n工具 # 1. Ack # 我之前使用 grep 搜索 CPython 代码库，但是它真的非常慢。后来我使用 ack 后就喜欢上它了。\n如果你是 Mac 用户，使用 Homebrew，你可以用 brew install ack 安装它；你只需要简单地 ack 你想要查找地字段，Ack 会输出非常棒地结果。\n2. timeit # 运行时间和效率问题是使用上文提到 “科学“ 策略最好地方。你可能会有疑问 “X 和 Y，哪一个更快？”例如，想要证明两个赋值语句（一个两个赋值语句和一个元组赋值方式）哪一个快？我首先猜测元组赋值（tuple-unpacking）是更慢，因为多了一个解包地步骤。让我们来验证它。\nn3xtchen$ python -m timeit \u0026quot;x = 1; y = 2\u0026quot; 10000000 loops, best of 3: 0.0323 usec per loop n3xtchen$ python -m timeit \u0026quot;x, y = 1, 2\u0026quot; 10000000 loops, best of 3: 0.036 usec per loop 猜对了。\n身边地很多人都喜欢使用 IPython。Ipython 可以使用 pip 安装，而且它安装非常简单。在 IPython REPL 模式中，你可以使用 %timeit 来计时。关于时间还有一个问题 —— 上述例子只是为了加深你的理解，除非你真的有性能问题。\nIn [1]: %timeit x = 1; y = 2; 10000000 loops, best of 3: 33.3 ns per loop In [2]: %timeit x, y = 1, 2 10000000 loops, best of 3: 38.9 ns per loop 3. 解剖 # Python 编译成字节码，用于 Python 虚拟机的代码中间表现。你使用 dis 模块，有时可以很清晰看到它的执行，非常的有趣的。\nIn [1]: def one(): ...: x = 1 ...: y = 2 ...: In [2]: def two(): ...: x, y = 1, 2 ...: In [3]: import dis In [4]: dis.dis(one) 2\t0 LOAD_CONST 1 (1) 3 STORE_FAST 0 (x) 3 6 LOAD_CONST 2 (2) 9 STORE_FAST 1 (y) 12 LOAD_CONST 0 (None) 15 RETURN_VALUE In [5]: dis.dis(two) 2 0 LOAD_CONST 3 ((1, 2)) 3 UNPACK_SEQUENCE 2 6 STORE_FAST 0 (x) 9 STORE_FAST 1 (y) 12 LOAD_CONST 0 (None) 15 RETURN_VALUE 变量的实现在 Python/ceval.c，你可以去看下。\n4. Inspect/cinspect # 你应该有经常使用 inspect，来查看任何你好奇的源代码实现：\nIn [11]: import inspect In [12]: import collections In [13]: print inspect.getsource(collections.namedtuple) def namedtuple(typename, field_names, verbose=False, rename=False): \u0026quot;\u0026quot;\u0026quot;Returns a new subclass of tuple with named fields. ... 然而，inspect 只能查看使用 Python 编写的源代码，有时可能会很迷惑：\nIn [14]: print inspect.getsource(collections.defaultdict) --------------------------------------------------------------------------- IOError Traceback (most recent call last) \u0026lt;ipython-input-14-71c51c86f80e\u0026gt; in \u0026lt;module\u0026gt;() ... 为了解决这个问题， Puneeth Chaganti 编写一个 cinspect 来拓展 inspect，可以用来查看 C 实现的部分。\n开始你的探索 PYTHON 世界的征程把 # CPython 是非常巨大的代码库，你想要为它建立一个心智模型需要花费很长的时间。现在下载源码，到处看看，如果对某一个东西感兴趣，可以花上 5 到 10 分钟探索它。日积月累，你就会变得更快，更严谨，处理流程更简单。\n如果你有更好的探索 Python 的测试，请让我知道！\n参考\nGetting Started With Python Internals ","date":"2014-08-26","permalink":"/n3xtchen/2014/08/26/python-internal/","section":"时间线","summary":"当我们学习很多东西的时候，大家都很想了解它的底层具体发生的事情。比如，“真的“ 和 ”为什么” 这些词经常会存在对话中 —— “当我运行一个列表，实际发生了什么？”，“为什么函数调用需要考虑成本？”。如果看完下面的部分，你就知道我有多喜欢挖掘 Python 的底层，并且我永远喜欢和别人分享这些。","title":"如何探索 Python？"},{"content":"","date":"2014-08-25","permalink":"/n3xtchen/tags/jawbone/","section":"标签","summary":"","title":"Jawbone"},{"content":"","date":"2014-08-25","permalink":"/n3xtchen/categories/jawbone/","section":"分类页","summary":"","title":"Jawbone"},{"content":"今年 5 月份入手 UP24，我的生日礼物哦，羡慕嫉妒恨去吧！使用了 3 个多月，这里对 UP24 的一些心得体会进行了一下总结。\n既然选择了她，自然是因为她拥有其他腕带没有的优点（对于我来说），那就先来谈谈她的优点把！（我这人比较懒，有空再补图把！）\n我为什么选择 Jawbone UP24？ # 外观：个人感觉是当前这么多款腕带中最炫的了，也是我选择她的原因（购买的时候，我还是做过功课的）； APP：相比其他腕带的应用，APP 的界面可谓制作精良，尤其前段时间又改版，更现代感，图表做的也很酷炫；数据可视化是亮点，清晰地看出自身的运动频率和强度； 建议推送：UP24 会根据你上一天的运动睡眠情况，对今天给出建议： 每天达到某个步数，你可能会给你发任务，让你今天尝试更大一点的运动量； 每天的运动量不足，则会提醒你； 睡眠不足，就是给你睡眠时间任务，还会经常给出一些生活小贴士（我还学到不少），如果昨晚睡眠不足，可能会给出睡不好造成的伤害，“吓吓“你，或者个告诉你良好睡眠的好处；真是威逼利诱啊！有时对她的提醒感觉好人性化，看来 Jawbone 的 AI 程度相当高！ 智能闹钟：现在已经没什么优势（三个月前还是一家独有呢！）了，不过这是 UP24 的独创，后来者都模仿这个功能！ 数据准确： 记步的误差很小（1000步有 0.5% 误差，无聊测过）； 睡眠时间，我是无法测得（总不能叫别人盯着我睡觉来测试把），不过据说 UP24 的数据团队非常的牛逼，在睡眠检测方面，应该是业界最好的。 提供 API：这个很重要，Jawbone 团队高明的地方（貌似没发现其他团队有对外提供 API），对于极客用户有很大的诱惑（可以根据需求，来开发功能）；对于 Jawbone 本身，会更加丰富自己的平台，目前有 10 来款第三方应用，比较有趣的要数控制飞利浦的 HUE，在切换成 Sleep 状态的时候，会自动变暗，可以作为家庭物联网的一环，比如如果你跑步，可能会自动开启家里的热水器，回来就有热水洗澡了，发挥你的想像，还是有很多事情可做的。 智能提醒：你可以设置你每周每天的运动量，饮食时间和睡眠提醒，生病时还有吃药提醒。 清闲时提醒：如果你保持长期不动，她会震动提醒你，起来走走，自己设置时间间隔，大家都知道，久坐对身体不好！ 材质：手感非常好，没有任何的不适，现在脱掉反而感觉不适应了，^_^ 人无完人…… 也要吐槽下她的缺点，希望 UP24 能做的更好！ # APP 被动同步：当手机打开应用时，应用中的数据都不是最新的，必须等待“漫长“的同步！应该定时同步信息比较好，等待可是非常不好的用户体验，据说 IPhone 5S 同步速度会快点，但是定时同步还是会提升用户体验的； 饮食功能：在国内几乎废掉，除非你每顿都去沃尔玛吃面包=_=，在普通饭馆吃饭无法通过扫描条形来获取相应的热量卡路里，建议：比如统计市面几葷几素的大概的卡路里供用户选择，来统计大概的饮食卡路里，不准确，至少减少误差； 传感器过于单一：只能统计睡眠和运动，如果能识别心跳脉搏等感应来判定睡眠或者身体异常那就更好，毕竟 1k 多价位应该值得拥有更多的功能； 需要状态切换：比如睡眠和运动是需要手动切换，而不能自动识别来切换状态，有时还忘记切换成睡眠状态，还好 UP24 有补记功能防止数据丢失，数据保存 90 天，不过需要多余的操作，导致用户体验下降。像我睡眠时间不规律，经常需要补登睡眠时间； 打盹功能：要切换状态，太不方便，几乎不实用，操作完，估计就不想睡。我一次都没用过； 电池容量：基本上一周一充，如果经常运动，大概5天一充，建议能提升到一个月一充就更加方便，偶尔一两次出现忘记充电，导致丢失半天的数据。 APP 本土化：由于我使用英文版的，所以每天都能接收 Jawbone 根据我的状态，给我的后续提供建议，前文提到过，但是中文版的用户就没那么幸运了，中文本土化做的不过； 材质弹性：使用了三个月，感觉腕带有点松了； 秒表功能不够纯粹：只能精确到分钟，短跑的话，数据就不够准确了； 运动轨迹：应用不能记录用户的运动轨迹，由于 UP24 提供 API，可以使用第三方应用记录，但是个人认为这是很基本的功能，其他腕带都有提供； 无痛起床：如果把你叫醒后，你按掉又去睡，那 UP24 就显得很无力，应该继续检测睡眠，还在睡觉，再进行提醒，至少提供延迟提醒功能； 不够防水：洗澡时要脱下来，还是不方便； 中文社交功能缺失，游戏性不够：用户可以通过 Tweet 和 FB 来和朋友做互动，只是仅仅的比步数和睡眠时间，中规中矩，虽然提供第三方 API（我就利用 API 开发，自动发微博的功能），你可以拓展，如果集成到 APP 去可能会更好，用户毕竟都很懒，减少操作肯定能提升不少用户体验； LED 更多内容显示：UP24 只能显示腕带状态，稍微丰富下显示可能会带来更多的用户，比如时间，或者当前进度，有时去打球，可能会把手机放起来，离开手机 UP24 就只能是普通的腕带了，比如时间（当然你可以使用 reminder 来提醒你，但是毕竟这样的控制很 low） 我提到这些建议，可能也是广大 UP24 用户的心声把！\n","date":"2014-08-25","permalink":"/n3xtchen/2014/08/25/jawbone-up24----experience/","section":"时间线","summary":"今年 5 月份入手 UP24，我的生日礼物哦，羡慕嫉妒恨去吧！使用了 3 个多月，这里对 UP24 的一些心得体会进行了一下总结。","title":"Jawbone UP24 - 3 个月使用体验"},{"content":"","date":"2014-08-23","permalink":"/n3xtchen/tags/javascript/","section":"标签","summary":"","title":"javascript"},{"content":" 1. 介绍 # 在这篇文章中，我们将要对比当今最流行 MV* 前端框架： Backbone VS. AngularJS VS. Ember.js。选择正确的框架将对你和你的项目产生巨大的影响，无论从开发的时间，还是将来维护代码的成本来说。你可能会选择一款灵活稳定以及被人证实过的框架来搭建你的项目，但是有不想让你的选择受限制。Web 在近几年爆炸式的发展－新的技术不断地产生，以及旧的无效方法论很快地被淘汰。在这样的背景下，我们将要对这三个框架进行深度的对比。\n2. 初始框架 # 这些流行的框架拥有很多的共同点：他们都是开源的，使用宽容的 MIT 开源协议，并且使用 MV* 模式创建单页应用（Single Page Web Applications）来解决问题。 他们都有视图（Views），事件（Events），数据模型（Model）以及路由（Routing）的概念。我们将要先从他们的技术背景和历史介绍开始，然后在分开对比这三个框架。\nAngularJS # AngularJS 作为商业产品的一部分，诞生于 2009 年，那时称作为 GetAngular。不久的厚厚，Misko Hevery，GetAngular 的创始人之一，利用 GetAngular 仅使用 3 周的时间重建一个 Web 应用（它原本包含了 1 万 7千行代码，花费了 6 个月的开发时间）。由于将代码量缩减到了 1000 行，这点说服了 Google 捐助这个项目，最后就成为我们如今的开源 AngularJS。AngularJS 有如下特有的独创特征：\n双向绑定（Two-Way Data Bindings） 依赖注入（Dependency Injection） 易于测试（easy-to-test）的代码 使用指令（directives）拓展 HTML Backbone # Backbone.js 是一个轻量 MVC 框架。诞生于 2010 年，它由于作为全功能的 MVC 重框架 - ExtJS 的替代者，迅速走红。这导致很多服务应用依赖它，如 Pinterest, Flixster, AirBNB。\nEmber.js # Ember 的起源可以追溯到 2007 年。它的前身是 SproutCore MVC 框架，前期由 SproutIt 开发的，后来由 Apple 主导，在 2011 年被 Yehuda Katz（著名的 JQuery 和 Ruby On Rails 计划的核心贡献者之一） 分支出来 。Yahoo!，Groupon 和 ZenDesk 也是强有力的支持者。\n3. 社区 # 社区是影响我们选择框架的最重要因素之一。一个强大的社区意味着更多的答案被解答，更多的第三方模块以及更多 YouTube 教程…… 我将这些放到一个报表中，数据时间截止到 2014 年 8 月 16 日。Angular 占据绝对的优势，成为 GitHub 最受关注的第六名，在 StackOverflow 拥有更多的问题，比 Backbone 和 Ember 加起来都要多，报表如下：\n| 度量 | AngularJS | Backbone.js | Ember.js | | : \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; |: \u0026mdash;\u0026mdash;\u0026ndash; :|: \u0026mdash;\u0026mdash;\u0026mdash; :|: \u0026mdash;\u0026mdash; :| | GitHub 的星级（Stars） | 27.2K | 18.8K | 11K | | 第三方模块 | 800 | 236 | 21 | | StackOverflow 问题个数 | 49.5k | 15.9k | 11.2k | | YouTube 视频 | ~75k\t| ~16k | ~6k | | GitHub 贡献者 | 928\t| 230 | 393 | | Chrome 拓展用户 | 150k\t| 7k | 38.3k |\n所有的这些指标，仅仅是展示每一个框架的当前状态。这些框架的发展趋势也是非常有趣。幸运的是，我们可以从 Google 趋势中得到答案。\n4 框架大小 # 页面加载时间是衡量一个 Web 页面关键指标之一。用户并不总是很有耐性的 —— 因此在任何场景下，你应该让你应用载入的尽可能更快。当考虑框架在载入时间的影响时，一般有两个因素来考量：框架的尺寸以及它启动的时间。\nJavascript 代码一般需要压缩，因此我们将要比较他们被压缩的最小版本。然而仅仅看这个是不够的。Backbone.js 最小只有 6.5 KB，一般需要依赖 Underscore.js(5 KB) 和 JQuery（32 KB）或者 Zepto（9.1 KB），所以你需要加入一些第三方插件，进行对比\n| 框架 | 净容量 | 带上必要的依赖 | |: \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; |: \u0026mdash;\u0026mdash;\u0026mdash; :|: \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- :| | AngularJS 1.2.22\t| 39.5kb\t|39.5kb | | Backbone.js 1.1.2 |\t6.5kb\t|43.5kb (jQuery + Underscore) 20.6kb (Zepto + Underscore) | | Ember.js 1.6.1 |\t90kb\t|136.2kb (jQuery + Handlebars)|\n5 模版 # Angular 和 Ember 都包括模版引擎。Backbone 留给使用其他模版的选择。体验不同模版引擎的最好方法就是代码样例，因此让我们分开对比。我们将展示格式化列表的例子。\n5.1 AngularJS # AngularJS 模版引擎只是简单将表达式绑定到 HTML 上。绑定的表达式被双花括号保卫。\n{% raw %} \u0026lt;ul\u0026gt; \u0026lt;li ng-repeat=\u0026quot;framework in frameworks\u0026quot; title=\u0026quot;{{framework.description}}\u0026quot;\u0026gt; {{framework.name}} \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; {% endraw %} 5.2 Backbone.js # 虽然 Backbone 可以整合很多第三方模版引擎，默认的选择是 Underscore 模版。由于 Underscore 是 Backbone 的依赖包，所以你已经拥有它，你可以简单地利用它，而不需要添加另外地依赖。弱点就是，Underscore 的模版引擎非常基础，你一般需要混入一些 javascript 脚本，下面就是例子：\n\u0026lt;ul\u0026gt; \u0026lt;% _.each(frameworks, function(framework) { %\u0026gt; \u0026lt;li title=\u0026quot;\u0026lt;%- framework.description %\u0026gt;\u0026quot;\u0026gt; \u0026lt;%- framework.name %\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;% }); %\u0026gt; \u0026lt;/ul\u0026gt; 5.3 Ember.js # Ember 目前使用把手（Handlebars）模版引擎，它是著名的模版引擎，Mustache，的拓展。有一个新的把手变种，称作 HTMLBars，目前能很好的运行。Handlebars 不理解 DOM —— 它所做的一切只是简单的字符串变形。HTMLBars 可以理解 DOM，因此变量注入是上下文相关的。由于目前 HTMLBars 还未在生产环境应用，因此我们将使用 Handlebars 答应框架列表：\n{% raw %} \u0026lt;ul\u0026gt; {{#each frameworks}} \u0026lt;li {{bind-attr title=description}}\u0026gt; {{name}} \u0026lt;/li\u0026gt; {{/each}} \u0026lt;/ul\u0026gt; {% endraw %} AngularJS # 6.1 优点 # Angular 已经为前端开发引入了很多独创的概念。双向数据绑定（Two-way data binding）保存很多样板代码。使用 JQuery 代码使用：\n$('#greet-form input.user-name').on('value', function() { $('#greet-form div.user-name').text('Hello ' + this.val() + '!'); }); 多亏了 Angular 的双向数据绑定（Two-way data binding），你不再需要自己编写代码了。而是简单的声明：\n\u0026lt;input ng-model=\u0026quot;user.name\u0026quot; type=\u0026quot;text\u0026quot; /\u0026gt; Hello {{user.name}}! Promises（异步编程模式） 在 Angular 中扮演着重要的角色。Javascript 是一个单线程(single-thread)，事件循环（event-loop）的语言，这意味这很多操作将采用异步行为（就像网络传输）。异步 Javascript 代码由于通过嵌套回调的编码方式很容易产生意大利面条式的代码，比如臭名昭著的“死亡金字塔（Pyramid Code）“或者“回调地狱（Callback Hell）“。\n}); }); }); }); }); }); }); }); }); }); // 这就是回调地狱 Angular 不仅拥有比其他两个框架更大的社区，更多的在线内容，而且还有 Google 作为强力的后盾。因此，核心团队不断地壮大，导致更多的革新和工具来提高开发效率： Protractor， Batarang， ngmin 和 Zone.js, 只列举了一些。另外，团队合作以设计为导向。比如，Angular 2.0 所有的设计文档都在 这里，每一个人都可以直接提出设计建议。\nAngular 自动会把你的代码分类到不同的应用组件类型中：Controllers， Directives， Factories， Filters， Services and Views (templates)。 她们将会组织到模块中，并相互依赖。构建模块的每一个类型都扮演着不同的角色。View 做 UI，Controller 充当 UI 之后的逻辑，Services 负责与后端的交互，组合在一起就构成通用相关的功能组件，而 Directives 使得代码复用，HTML 元素，属性和行为拓展变得更简单。\n自动脏代码检查（Dirty Checking）意味着你不需要通过 getter 和 setter 访问你的数据模型 —— 你可以对任意对象进行修改，Angular 将会自动检测改变和提醒所有的观察者。\n“Angular is written with testability in mind.”\n这句话出自 AngularJS 的单元测试指南，意思是编写 AngularJS 代码时，需要考虑可测试性 —— Angular 确实在构建内置服务时（例如 $http 和 $timeout）很注重概念分离，单元隔离以及提供可用性高，强大的模拟对象（Mocks）。\n6.2 痛点 # Angular 紧张由于 Directives 接口负责度，为人所诟病。转置（Transclusion），这个概念让无数开发者云里雾里，例如编译（Compile）函数，前后置链接函数（pre/post linking functions）,错综复杂的作用域类型（transclusion/isolate/child）以及其他配置设置等等。\nAngular 的作用域继承使用原型继承，对于来自对象编程开发者来说，需要花时间来掌握的的新概念。难以理解的作用域继承将是导致迷惑开发者的主要元凶。\nAngular 表达式（Expressions）被用于扩展 Angular 的视图层。这个表达式语言是非常强大，甚至强大过头了。它让开发者使用复杂的逻辑来实现最简单的赋值和计算。将逻辑放在模版层将导致难以测试，甚至不可测试。下面的例子就是展示如何简单的把表达式语言滥用。\n\u0026lt;button ng-click=\u0026quot;(oldPassword \u0026amp;\u0026amp; checkComplexity(newPassword) \u0026amp;\u0026amp; oldPassword != newPassword) ? (changePassword(oldPassword, newPassword) \u0026amp;\u0026amp; (oldPassword=(newPassword=''))) : (errorMessage='Please input a new password matching the following requirements: ' + passwordRequirements)\u0026quot;\u0026gt; Click me\u0026lt;/button\u0026gt; 在很多情况下，例如指令名的误拼或者调用为定义的域函数的错误只能无情地被忽视了，尤其是你把指令接口的复杂性与与继承混合的时候，查找将非常有挑战性。我曾是发现一些开发者花费大量时间从头到尾检查代码，只是为了查出一个被绑定的事件为什么在某个域中被触发，结果是因为它使用驼峰规则代替下划线分割时，拼错了属性名，这可是血的教训啊。\n最后，Angular 的对象轮询机制（Digest Cycle ），收到了特殊的脏数据检查，也令开发者瞠目结舌。当运行非 Angular 上下文时，非常容易忘记带调用 $digest()。另一方面，你需要非常小心避免造成缓慢执行或者无限轮询。通常，每个页面都有很多的交互，Angular 将会变得很慢。一个好的标准就是不要在同一个页面绑定2000 个活动。\n7 Backbone.js # 7.1 优点 # Backbone 以轻量级，快速和低内存著称。学习曲线非常平缓，需要掌握的概念也不适很多（Models/Collections，Views 和 Routes）。她拥有很棒的文档，简单的代码和重度的注释，甚至使用注释解释用法。你可以使用一个小时的事件通读代码并熟悉她。\n由于她足够小和简单，因此 Backbone 很适合作为构建自己框架的底层。基于 Backbone 作为底层的框架有很多，比如 Aura，Backbone UI，Chaplin，Geppetto，Marionette，LayoutManager，Thorax，Vertebrae。而 Angular 和 Ember 则走不同的路线，编码中充分体现框架构建者的意识，这可能不适和你的项目需要或者不符合你个人风格。Angular 2.0 承诺改变这一现状，由更小独立的模块构成，因此你可以选择和组合。我们将赤目以待。\n7.2 痛点 # Backbone 不提供结构。她甚至不提供构建结构的基本工具，交给开发者决定如何构建自己的应用，存在很多空白需要填补，甚至连内存管理都需要小心的考虑。由于视图生命周期管理的缺失使得 route/state 长期占据内存，除非你小心释放无用的对象。\n由于很多函数不是 Backbone 本身提供，这块就交给第三方插件来填充空白，这意味着构建应用时面临许多的选择。例如，内置的 Models 可以由 Backbone 提供；你还可以选择Document model，BackBone.NestedType，Backbone.Schema，Backbone-Nested， backbone-nestify， 这里只列举了一些。 你需要调研决定哪一个最适合你的项目，这需要花费事件 —— 而框架最主要目的之一就是节省你的事件。\nBackbone 不支持双向数据绑定，意味着你需要写很多的模版在模型改变时更新视图，视图改变时改变模型。看看上面给的例子，Angular 展示了如何双向数据绑定来减少代码。\nBackbone 中的视图是直接操纵 DOM 的，这样真的很难进行单元测试，更加碎片化和更难复用。\n8. Ember.js # 8.1 优点 # Ember.js 是约定优于配置（Convention over Configuration）的拥护者。这意味着替代编写更多的模版代码，Ember 可以自动推测大部分配置，例如当决定路由资源时会自动决定路由和控制器名称。如果没有自己定义控制器的时候，Ember 会自动为你创建一个控制器。\nEmber 本身包含很棒的路由和可选的数据层（称为 ember data）。不像之前两个框架，只有非常轻的数据层（Backbone 的 Collection/Model 和 Angular 的 $resource），Ember 拥有全功能的数据模块（可以和 Ruby On Rails 完美整合或者其他符合约定的 JSON 接口）。她还可以设置基境（Fixture）为开发提供测试和模拟环境。\n性能是 Ember.js 设计中最重要的目标。例如 Run Loop 的概念，为了确保更新的数据只造成一次 DOM 更新，即便同一块数据被更新了几次，还有计算指标的缓存机制，以及在编译事件或者你的服务器上有预编译 HandleBars 模版的能力，帮助你的应用载入和运行的很快。\n8.2 痛点 # Ember 的 API 在她稳定之前改变了很多；导致了很多过期的内容，很多例子都不能使用，让第一次接触框架的开发感觉到迷惑。如果你看过 Ember Data 的变更日志，你就会明白我的意思了。这么多的大变动，这导致了很多 Stackflow 的问题和一些编码教程变得过时。\n为了保持模版的数据实时更新，DOM 会被 Handlebars 使用的 script 标签。这会随着 HTMLBars 的转变，随着时间推移，你的 DOM 树会被很多 script 标签填充，以致你难以辨认自己的代码。最坏的情况是，她会弄乱 CSS 样式，以及破坏与其他框架的整合。\n9 总结 # 我们已经看过了这三个框架的优缺点。Ember 整体方法论强调 MVC 结构，对于有 MVC 开发背景的人来说，容易适应，如 Ruby，Python，Java，C# 等等其他面向对象的语言。Ember 还提升了应用的性能，通过支持约定优于配置很大程度上为你节省了你自己编写模版的时间。\nBackbone 强调极度抽象。她小，快以及易于学习，提供你需要最小的代码（甚至比最小还要少）。\nAngular 的拓展 HTML 的独创方法与 Web 开发者产生了很多共鸣。强大的社区支持和强有力的 Googld 作为后盾，目前得到了很快速和发展，她同时可以适应快速构建原型项目以及大型的应用产品。\n参考：\nJavascript Framework Comparison 逃离回调地狱 ","date":"2014-08-23","permalink":"/n3xtchen/2014/08/23/javascript---backbone-vs-angularjs-vs-emberjs/","section":"时间线","summary":"1.","title":"Javascript MVC 框架大 PK - Backbone, AngularJS 和 Ember.js"},{"content":"","date":"2014-08-21","permalink":"/n3xtchen/tags/analytics/","section":"标签","summary":"","title":"analytics"},{"content":"Toolz 可以用于编写分析大型数据流脚本，它支持通用的分析模式，如通过纯函数来对数据进行筛选（Selection），分组（Grouping），化简（Reduction）以及连表（Joining）。这些函数通常可以模拟类似其他数据分析平台（如 SQL 和 Panda）的类似操作行为。\n我将使用下面简单的数据集作为演示数据，贯穿全文\n\u0026gt;\u0026gt;\u0026gt; # id, name, balance, gender \u0026gt;\u0026gt;\u0026gt; accounts = [(1, 'Alice', 100, 'F'), ... (2, 'Bob', 200, 'M'), ... (3, 'Charlie', 150, 'M'), ... (4, 'Dennis', 50, 'M'), ... (5, 'Edith', 300, 'F')] 使用 Map 和 Filter 来筛选数据 # 通过标准函数 map 和 filter 能够完成对列表简单的映射和筛选\nSELECT name, balance FROM accounts WHERE balance \u0026gt; 150; 下面的函数能够满足 SQL 的 SELECT 和 WHERE 需求\n\u0026gt;\u0026gt;\u0026gt; from toolz.curried import pipe, map, filter, get \u0026gt;\u0026gt;\u0026gt; pipe(accounts, filter(lambda (id, name, balance, gender): balance \u0026gt; 150), ... map(get([1, 2])), ... list) 它使用了 map 和 reduce 的加里化（curried）版本。\n当然，这些操作也能很好的支持标准的列表（List）和生成器（Generator）的组合语法。这个语法会经常被使用，并通常被认为非常的 Pythonic\n\u0026gt;\u0026gt;\u0026gt; [(name, balance) for (id, name, balance, gender) in accounts ... if balance \u0026gt; 150] 使用 groupby 和 reduceby 完成 Split-apply-combine # 我们把 Split-apply-combine 拆分成下面两个概念：\n根据一些特征将数据拆分到不同组中 使用聚合函数对每一个分组进行化简 Toolz 支持这种工作流：\n简单的内存计算方案 更复杂的流式计算方案 使用内存计算进行 Split-apply-combine # 内存计算方案使用 groupby 进行分割数据（Split），和 valmap 来应用／组合（apply/combine）\nSELECT gender, SUM(balance) FROM accounts GROUP BY gender; 我们首先使用 groupby 和 valmap 来分别展示中间结果\n\u0026gt;\u0026gt;\u0026gt; from toolz import groupby, valmap, compose \u0026gt;\u0026gt;\u0026gt; from toolz.curried import get, pluck \u0026gt;\u0026gt;\u0026gt; groupby(get(3), accounts) {'F': [(1, 'Alice', 100, 'F'), (5, 'Edith', 300, 'F')], 'M': [(2, 'Bob', 200, 'M'), (3, 'Charlie', 150, 'M'), (4, 'Dennis', 50, 'M')]} \u0026gt;\u0026gt;\u0026gt; valmap(compose(sum, pluck(2)),\t... _) {'F': 400, 'M': 400} 然后我们把他们组合在一起\n\u0026gt;\u0026gt;\u0026gt; pipe(accounts, groupby(get(3)), ... valmap(compose(sum, pluck(2)))) {'F': 400, 'M': 400} 使用流式计算进行 Split-apply-combine # groupby 使用内粗将所有的数据实体存储到字典中。虽然方便，然而它不会流式的，因此这种方法受限于机子的内存。\nToolz 通过 reduceby 来实现流式的 Split-apply-combine，就像元素流入一样，它并行对每一个分组进行化简处理。为了理解这个概念，你首先应该熟悉内置的 reduce 函数。\nreduceby 操作接受一个获取键的函数（如 get(3) 或者 lambda x: x[3]）, 和一个二元运算符（如 add 或者 lesser = lambda acc, x: acc if acc \u0026lt; x else x）。它可以连续将获取键的函数应用到每一个项中，通过使用二元运算符将之前的总数结合新的值，为每一个键汇总出总数。由于需要一次性反问全部分组，它不能接受全化简操作（如 sum 和 min）。这里是一个简单的例子：\n\u0026gt;\u0026gt;\u0026gt; from toolz import reduceby \u0026gt;\u0026gt;\u0026gt; def iseven(n): ... return n % 2 == 0 \u0026gt;\u0026gt;\u0026gt; def add(x, y): ... return x + y \u0026gt;\u0026gt;\u0026gt; reduceby(iseven, add, [1, 2, 3, 4]) {True: 6, False: 4} 偶数会被加到 True 的分组中（2 + 4 =6），奇树会被加到 False 的分组中（1 + 3 = 4）。\n注意，我们已经使用二元运算符 add 替换化简函数 sum。 但新的值被传入，add 的渐进性允许我们做汇总操作。二元操作符（如 add ）比全化简操作（如 sum）更能处理大型数据流。\n使用 reduceby 的挑战主要在于构建合适的二元操作符。这里有一个解决方案来解决汇总每一个分组的收支\n\u0026gt;\u0026gt;\u0026gt; binop = lambda total, (id, name, bal, gend): total + bal \u0026gt;\u0026gt;\u0026gt; reduceby(get(3), binop, accounts) {'F': 400, 'M': 400} 这个构造器可以支持比可用内存大很多的数据集合。不过输出必须符合内存问题，不过即使是在非常大的 split-apply-combine 的计算中，这个问题很少见。\n伪流式 join # 我们通过 Join 将多个数据集整合在一起。假设，第二个数据用来存储地址，并给予主键 ID\n\u0026gt;\u0026gt;\u0026gt; addresses = [(1, '123 Main Street'), # id, address ... (2, '5 Adams Way'), ... (5, '34 Rue St Michel')] 我们可以将它与我们的账户数据连表，通过指定连接两张表的共同键；在这个例子中，他们共同字段是 ID 字段\nSELECT accounts.name, addresses.address FROM accounts, addresses WHERE accounts.id = addresses.id; \u0026gt;\u0026gt;\u0026gt; from toolz import join, first, second \u0026gt;\u0026gt;\u0026gt; result = join(first, accounts, ... first, addresses) \u0026gt;\u0026gt;\u0026gt; for ((id, name, bal, gender), (id, address)) in result: ... print((name, address)) ('Alice', '123 Main Street') ('Bob', '5 Adams Way') ('Edith', '34 Rue St Michel') Join 需要传入四个参数，获取左右键函数和左右数据流。它返回包含多对匹配项的序列。在我们例子中，\u0026lsquo;Join` 的返回值是一个一对第一个元素（即 ID）匹配的项的列表。\n可以 join 任意的函数和数据 # 它类似于 SQL，习惯于对列的 Join。无论如何，函数式的 Join 比它更加的通用；它不需要操作数组，键函数不需要获取特定的列。在下面的例子中，我们匹配两个数组的数字，一个奇偶数对。\n\u0026gt;\u0026gt;\u0026gt; def iseven(n): ... return n % 2 == 0 \u0026gt;\u0026gt;\u0026gt; def isodd(n): ... return n % 2 == 1 \u0026gt;\u0026gt;\u0026gt; list(join(iseven, [1, 2, 3, 4], ... isodd, [7, 8, 9])) [(2, 7), (4, 7), (1, 8), (3, 8), (2, 9), (4, 9)] 伪流式 Join # Toolz Join 操作是将左边的序列保存在内存中，流式处理右边的序列；因此，如果想要更接近流式处理，传入的两个序列中最大的那个应该放在 Join 右边。\njoin(func1, smaller, func2, larger) 具体算法 # Toolz Join 的伪流式操作已经接近最优。 计算速度和输入输出的容量大小程线性相关。左边的序列必须装进内存（换句话说，就是受限于你的内存），而右边采用流式，几乎不受限于你的内存容量。\n不像 SQL 那样一定要范式化，结果是允许重复的值。如果需要范式化，考虑使用 unique 函数（注意，它不是完全流式化的）。\n更复杂的例子 # 上面例子账户的例子的 accounts 和 addresses 是一一对应的关系；每个 ID 一个任命，每个 ID 一个地址。但现实生活中并不是这样的。Join 需要足够的灵活来处理一对多甚至多对多的关系。下面的例子找出城市/人的对应关系；一个人有一个朋友，这个朋友在市区有一个住所。这是一个多对多关系例子，因为一个人会有多个朋友，一个朋友会有多个住所。\n\u0026gt;\u0026gt;\u0026gt; friends = [('Alice', 'Edith'), ... ('Alice', 'Zhao'), ... ('Edith', 'Alice'), ... ('Zhao', 'Alice'), ... ('Zhao', 'Edith')] \u0026gt;\u0026gt;\u0026gt; cities = [('Alice', 'NYC'), ... ('Alice', 'Chicago'), ... ('Dan', 'Syndey'), ... ('Edith', 'Paris'), ... ('Edith', 'Berlin'), ... ('Zhao', 'Shanghai')] \u0026gt;\u0026gt;\u0026gt; # Vacation opportunities \u0026gt;\u0026gt;\u0026gt; # In what cities do people have friends? \u0026gt;\u0026gt;\u0026gt; result = join(second, friends, ... first, cities) \u0026gt;\u0026gt;\u0026gt; for ((name, friend), (friend, city)) in sorted(unique(result)): ... print((name, city)) ('Alice', 'Berlin') ('Alice', 'Paris') ('Alice', 'Shanghai') ('Edith', 'Chicago') ('Edith', 'NYC') ('Zhao', 'Chicago') ('Zhao', 'NYC') ('Zhao', 'Berlin') ('Zhao', 'Paris') Join 具有强大的计算能力：\n它在涵盖大量的分析操作上表现力丰富。 它的执行时间和输出输入的容量大小呈线性关系。 只有左边的序列需要放在内存中。 结语 # Toolz 为扁平的 Python 结构进行数据分析时，提供更紧凑的类型。CyToolz 通过 Cython 加速了整个工作流。这种方法使用起来低技术含量，并通过流媒体来支持大数据处理。\n同时，Toolz 是一个通用的函数式标准库，并不是仅仅为了数据分析。然而，对于对数据分析感兴趣的用户，有明显的优势（流式化，组合等等），可能在某些场景比使用专门用于分析的项目（如 Pandas 和 SQLAlchemy）。\n参考：\n* http://matthewrocklin.com/blog/work/2014/07/04/Streaming-Analytics/\n","date":"2014-08-21","permalink":"/n3xtchen/2014/08/21/python---toolz/","section":"时间线","summary":"Toolz 可以用于编写分析大型数据流脚本，它支持通用的分析模式，如通过纯函数来对数据进行筛选（Selection），分组（Grouping），化简（Reduction）以及连表（Joining）。这些函数通常可以模拟类似其他数据分析平台（如 SQL 和 Panda）的类似操作行为。","title":"Python - Toolz - 流式分析（Streaming Analytics）工具"},{"content":"","date":"2014-08-21","permalink":"/n3xtchen/tags/toolz/","section":"标签","summary":"","title":"toolz"},{"content":"心血来潮，当我在构思不定参数文档的时候，想到了用 Map 作为例子，萌生我实现 MapReduce 机制的想法!\n如果我不使用循环语句，我还能用什么？\n毫无疑问，使用 递归！\n实现 MAP # def myMap(f, *args): \u0026quot;\u0026quot;\u0026quot; 实现 map 函数 \u0026quot;\u0026quot;\u0026quot; def getEachArgs(f1, args): \u0026quot;\u0026quot;\u0026quot; 获取每一个参数表 \u0026quot;\u0026quot;\u0026quot; if args==[]: return [] else: return [f1(args[0])] + getEachArgs(f1, args[1:]) if list(args)[0]==[]: return [] else: return [f(*(getEachArgs(lambda x: x[0], list(args))))] \\ + myMap(f, *(getEachArgs(lambda x: x[1:], list(args)))) \u0026gt;\u0026gt;\u0026gt; print myMap(lambda *args: (args), [1, 2], [3, 4]) [(1, 3), (2, 4)] 本来想用尾递归来实现，不过遇到点问题，稍后调试好了，把代码贴出来\n实现 FILTER # def myFilter(func, seq): \u0026quot;\u0026quot;\u0026quot; 实现 filter 函数 \u0026quot;\u0026quot;\u0026quot; if seq == []: return [] else: return [seq[0]] if func(seq[0]) else [] + \\ myFilter(func, seq[1:]) \u0026gt;\u0026gt;\u0026gt; print myFilter(lambda x: x\u0026gt;=5, [1, 5, 10]) [5, 10] 实现 REDUCE # 这里使用尾递归的方式实现：\ndef myReduce(func,seq,initial=0): \u0026quot;\u0026quot;\u0026quot; 实现 reduce 函数 \u0026quot;\u0026quot;\u0026quot; if(len(seq)==0): return initial return myReduce(func,seq[1:], func(initial, seq[0])) \u0026gt;\u0026gt;\u0026gt; print myReduce(lambda x,y: x+y, [1, 2, 4, 1]) 8 如果你使用 For 循环来实现，那代码将会变成什么样的呢？ # ","date":"2014-08-12","permalink":"/n3xtchen/2014/08/12/python---mapfilterreduce/","section":"时间线","summary":"心血来潮，当我在构思不定参数文档的时候，想到了用 Map 作为例子，萌生我实现 MapReduce 机制的想法!","title":"使用 python - 实现 Map，Filter 以及 Reduce"},{"content":" 写在前面的话 # 传递参数的行为对于现在编程语言来说，再寻常不过的概念\n参数（英语：parameter）是使用通用变量来建立函数和变量之间关系（当这种关系很难用方程来阐述时）的一个数量。 - 来自 wikipedia\n先来看一个例子：\n# 来源于 https://docs.python.org/2/library/itertools.html def chain(*iterables): for it in iterables: for element in it: yield element 大家可能注意到 *iterables 了，对了，就是他， 不定参数。\n\u0026gt;\u0026gt;\u0026gt; from itertool import chain \u0026gt;\u0026gt;\u0026gt; chan([1,2], [2, 3])\t# 你可以这么用 [1, 2, 2, 3] \u0026gt;\u0026gt;\u0026gt; chan([1,2], [2, 3], [4, 5])\t# 你还可以这么用 [1, 2, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; chan([1,2], [2, 3], [4, 5])\t# 你也可以这么用 [1, 2, 2, 3, 4, 5] ...\t// 随心所欲的加参数 很神奇把，(^_^)v，来讲讲枯燥的概念把！\n可能很多人用了几年的 Python 都没真正使用过可变参数，就比如我，为了学写通用模块，就会对它有需求；或许你经常看 Python 模块库代码，会发现很多函数的参数定义，都会跟上 *args 和 **kwargs（不定参数的另一种形式，后面会讲到）。\n在计算机程序设计，一个可变参数函数是指一个函数拥有不定引数，即是它接受一个可变量目的参数。 - 来自 wikipedia\n通俗的说就是，函数可以处理不同数量的参数。\n在我看来，几乎80%的使用可变参数列表的场景，都可以使用数组和字典来解决。但是使用可变参数列表的函数可以提供一种数组和字典无法提供的东西：优雅。\n*args # def argsFunc(a, *args): print a print args \u0026gt;\u0026gt;\u0026gt; argsFunc(1, 2, 3, 4) 1 (2, 3, 4) argsFunc 中匹配完定义好的参数，剩余的参数以元组的形式存储在 args（args 名称你可以自行定义），因此在上述程序中只要你传入不小于 1 个参数，该函数都会接受，当然你也可以直接定义只接受可变参数，你就可以自由传递你的参数:\ndef argsFunc(*my_args): print my_args \u0026gt;\u0026gt;\u0026gt; argsFunc(1, 2, 3, 4) (1, 2, 3, 4) \u0026gt;\u0026gt;\u0026gt; argsFunc() () 很简单把，现在来将另一个种不定参数形式\n**kwargs # 形参名前加两个*表示，参数在函数内部将被存放在以形式名为标识符的 dictionary 中，这时调用函数的方法则需要采用 arg1=value1,arg2=value2 这样的形式。\n为了区分，我把 *args 称作为数组参数，**kwargs 称作为字典参数\n\u0026gt;\u0026gt;\u0026gt; def a(**x):print x \u0026gt;\u0026gt;\u0026gt; a(x=1,y=2,z=3) {'y': 2, 'x': 1, 'z': 3} #存放在字典中 不过，有个需要注意，采用 **kwargs 传递参数的时候，你不能传递数组参数\n\u0026gt;\u0026gt;\u0026gt; a(1,2,3) #这种调用则报错 Traceback (most recent call last): File \u0026quot;\u0026lt;stdin\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; TypeError: a() takes exactly 0 arguments (3 given) 同样很简单，但是我们什么时候可以用到他呢？\nimport mysql.connector db_conf = { user='xx', password='yy', host='xxx.xxx.xxx.xxx', database='zz' } cnx = mysql.connector.connect( user=db_conf['user'], password=db_conf['password'], host=db_conf['host'], database=db_conf['database'] ) ... 相比，使用 Mysql Python 库时候，经常看到这个样子的代码，db_conf 一般都从配置文件读取，这是优雅的不定字典参数就派上用途了！\nimport mysql.connector db_conf = { user='xx', password='yy', host='xxx.xxx.xxx.xxx', database='zz' } cnx = mysql.connector.connect(**db_conf) ... 怎样，是不是顺眼多了，代码也省了不少！^_^\n今天就到这里了，很早就开始写这一篇了，不想像网路上的大部分博客，只是写一个使用文档类型的教程，看完就忘了。\n适当的考虑应用场景，希望能印象深刻。学会，就尽可能的使用它；再优雅的概念，不用也是百搭。\nThe End # 参考 * Python中函数的参数定义和可变参数 * Python函数中定义参数的四种方式 * wiki - 参数 * 【C/C++ 语法备忘】4、可变参数列表\n","date":"2014-08-08","permalink":"/n3xtchen/python/2014/08/08/python-args-and-kwargs/","section":"时间线","summary":"写在前面的话 # 传递参数的行为对于现在编程语言来说，再寻常不过的概念","title":"Python 优雅的使用参数 - 可变参数（*args \u0026 **kwargs)"},{"content":"","date":"2014-08-04","permalink":"/n3xtchen/tags/funcitonal-programming/","section":"标签","summary":"","title":"Funcitonal Programming"},{"content":" 关于函数式 # Imperative Programming 指令式编程（C/C++，Java）\nDeclarative Programming 声明式编程）\nFunctional programming 函数式编程（Haskel， Scheme，OCaml） Logic programming 逻辑编程（Prolog，Clojure.core.logic） IP = 通过执行改变程序状态的语句进行计算\nFP = 使用数学函数计算求值，避免状态和改变数据\nFP 的特性： # Avoid State 避免状态 Immutable data 不可变的数据 First Class Functon：函数是第一类型 High Ordered Function 高阶函数 Pure Function 纯函数 Recursion，Tail Recursion 递归，尾递归 惰性求值 Partial Function Application(偏函数)和 Currying(函数加里化) 迭代器，队列，模式匹配。。。 命令行执行风格 # 指令式 # $ ./program1 $ ./program2 --param1=1 $ ./program3 函数式(Pipeline) # $ ./program1 | ./program2 --param1=1 | ./program3 简单的例子 # 计算部分无效字符串的数据计算：\n\u0026quot;28+32+++32++39\u0026quot; 指令式 # Imperative Style ＝ 执行动作，从初始状态转变成结果\nexpr, res = \u0026quot;28+32+++32++39\u0026quot;, 0 for t in expr.split(\u0026quot;+\u0026quot;): if t != \u0026quot;\u0026quot;: res += int(t) print res 执行路径：\n\u0026quot;28+32+++32++39\u0026quot;, 0 \u0026quot;28\u0026quot;, 0 \u0026quot;32\u0026quot;, 28 \u0026quot;\u0026quot;, 60 \u0026quot;\u0026quot;, 60 \u0026quot;32\u0026quot;, 60 \u0026quot;\u0026quot;, 92 \u0026quot;39\u0026quot;, 92 131 函数式 # Functional Style = 应用转换和组合\nfrom operator import add expr = \u0026quot;28+32+++32++39\u0026quot; print reduce(add, map(int, filter(bool, expr.split(\u0026quot;+\u0026quot;)))) 执行路径：\n\u0026quot;28+32+++32++39\u0026quot; [\u0026quot;28\u0026quot;,\u0026quot;32\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;32\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;39\u0026quot;]\t# split [\u0026quot;28\u0026quot;,\u0026quot;32\u0026quot;,\u0026quot;32\u0026quot;,\u0026quot;39\u0026quot;]\t# filter [28,32,32,39]\t# int 131\t# reduce MAP/REDUCE/FILTER # 可读性 VS 简洁 技术面 VS 操作物质 代码复用（热拔插） 函数式编程－函数是第一类型和高阶函数 # 第一类型 # 换句话说，你的函数可以像变量一样来使用！\n它可以作为变量：\nadd = lambda a,b: a + b 它也可以作为函数的返回值\ndef calculations(a, b): def add(): return a + b return a, b, add 高阶函数（High Ordered Function） # 在数学和计算机科学中，高阶函数是至少满足下列一个条件的函数:\n接受一个或多个函数作为输入，即作为参数 输出一个函数，即作为返回值 在数学中它们也叫做运算符或范式函数。微积分中的导数就是常见的例子，因为它映射一个函数到另一个函数。 例如:函数f(x) = x2;函数f(x)的导数为2x;2x也为一个函数，所以导数是高阶函数。\n作为参数传递给函数：\nmap(lambda x: x^2, [1,2,3,4,5]) 作为返回值\ndef speak(topic): print \u0026quot;My speach is \u0026quot; + topic def timer(fn): def inner(*args, **kwargs): t = time() fn(*args, **kwargs) print \u0026quot;took {time}\u0026quot;.format(time=time()-t) return inner speaker = timer(speak) speaker(\u0026quot;FP with Python\u0026quot;) # 好可以这么用 @timer def speak(topic): print \u0026quot;My speach is \u0026quot; + topic\tspeak(\u0026quot;FP with Python\u0026quot;) 你可能对第一类型和高阶函数感觉到迷糊，下面是 StackOverflow 的答案：\n第一类型和高阶函数的区别 http://stackoverflow.com/questions/10141124/any-difference-between-first-class-function-and-high-order-function 看完估计还是很晕，不过\n纯函数（Pure Function） # PURE # def is_interesting(topic): return topic.contains(\u0026quot;FP\u0026quot;) NOT PURE # def speak(topic): print topic PURE？？ # def set_talk(speaker, topic): speaker[\u0026quot;talk\u0026quot;] = topic return speaker 纯函数是何方神圣？ # 纯函数（Pure Function）是这样一种函数——输入输出数据流全是显式（Explicit）的。\n显式（Explicit）的意思是，函数与外界交换数据只有一个唯一渠道——参数和返回值；函数从函数外部接受的所有输入信息都通过参数传递到该函数内部；函数输出到函数外部的所有信息都通过返回值传递到该函数外部。\n有什么好处？ # 纯函数的好处主要有几点：\n无状态。线程安全。不需要线程同步。 纯函数相互调用组装起来的函数，还是纯函数。 应用程序或者运行环境（Runtime）可以对纯函数的运算结果进行缓存，运算加快速度。 使用 stackoverflow 的例子：\nfor (int i = 0; i \u0026lt; 1000; i++){ printf(\u0026quot;%d\u0026quot;, pureFun(10)); } 使用纯函数，编译器可以知道只需要执行 pureFun(10)，仅需一次，而不是 1000次，然后读入缓存。对于复杂的函数，性能可想而知。。。\nSO，函数还是纯的好！ # 可以使用 pmap 更少的 BUG 更容易测试 最大程度复用 我们能避免轮询吗？- 递归 # \u0026gt;\u0026gt;\u0026gt; name = None \u0026gt;\u0026gt;\u0026gt; while name is None: ... name = raw_input() ... if len(name) \u0026lt; 2: ... name = None 使用递归调用 # def get_name(): name = raw_input() return name if len(name) \u0026gt;= 2 else get_name() 递归最大的好处就简化代码，他可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓。\n** Note **: Python 对递归次数有限制的，当递归次数超过1000次的时候，就会抛出“RuntimeError: maximum recursion depth exceeded”异常。\n函数式编程－尾(递归)调用 # 在计算机科学里，尾调用是指一个函数里的最后一个动作是一个函数调用的情形：即这个调用的返回值直接被当前函数返回的情形。\n我们知道递归的害处，那就是如果递归很深的话，栈受不了，并会导致性能大幅度下降。所以，我们使用尾递归优化技术——每次递归时都会重用栈，这样一来能够提升性能，当然，这需要语言或编译器的支持。\nERLANG # fib(N) -\u0026gt; fib(0,1,N). fib(_,S,1) -\u0026gt; S; fib(F,S,N) -\u0026gt; fib(S,F+S,N-1). PYTHON # Sorry，Python 目前不支持这一特性，下面代码演示，Python 对尾递归算法的实现\ndef tailrecsum(x, running_total=0): if x == 0: return running_total else: return tailrecsum(x - 1, running_total + x) ** Note **: 这只是个算法实现，由于 Python 本身不支持为尾递归，而且实际测试结果中，写成尾递归的方式会比会比普通方式占用多那么多内存，所以应该在实际的代码使用尾递归，后续将会专门的章节介绍。\n惰性求值（Lazy Evaluation） # 它的目的是要最小化计算机要做的工作。\n先来看个例子：\nfor num in range(1, 20): yield num 我们动用了Python的关键字 yield，这个关键字主要是返回一个Generator，yield 是一个类似 return 的关键字，只是这个函数返回的是个Generator-生成器。所谓生成器的意思是，yield返回的是一个可迭代的对象，并没有真正的执行函数。也就是说，只有其返回的迭代对象被真正迭代时，yield函数才会正真的运行，运行到yield语句时就会停住，然后等下一次的迭代。（这个是个比较诡异的关键字）这就是lazy evluation。\n惰性求值有显著的优化潜力。惰性编译器看函数式代码就像数学家面对代数表达式——可以消去一部分而完全不去运行它，重新调整代码段以求更高的效率，甚至重整代码以降低出错，所有确定性优化（guaranteeing optimizations）不会破坏代码。这是严格用形式原语描述程序的巨大优势——代码固守着数学定律并可以数学的方式进行推理。\n考虑一个 Fibonacci 数列，显然我们无法在有限的时间内计算出或在有限的内存里保存一个无穷列表。在严格语言如 Java 中，只能定义一个能返回 Fibonacci 数列中特定成员的 Fibonacci 函数，在 Haskell 中，我们对其进一步抽象并定义一个关于 Fibonacci 数的无穷列表，因为作为一个惰性的语言，只有列表中实际被用到的部分才会被求值。这使得可以抽象出很多问题并从一个更高的层次重新审视他们（例如，我们可以在一个无穷列表上使用表处理函数）。\n函数式编程－偏函数应用（Partial Function Application） # 通过固定函数的一个或多个参数，生成一个新的更少元数函数。\n固定参数是偏函数应用的函数。\ndef log(level, message): print level + \u0026ldquo;: \u0026quot; + message\nlog(\u0026ldquo;Warning\u0026rdquo;, \u0026ldquo;this is one warning message\u0026rdquo;) log(\u0026ldquo;Error\u0026rdquo;, \u0026ldquo;this is one error message\u0026rdquo;)\ndef logWarning(message): log(\u0026ldquo;Warning\u0026rdquo;, message)\ndef logError(message): log(\u0026ldquo;Error\u0026rdquo;, message)\nlogWarning(\u0026ldquo;this is one warning message\u0026rdquo;) logError(\u0026ldquo;this is one error message\u0026rdquo;)\n上面的函数实现，相信代码都不会陌生把，logWarning 和 logError 就叫做函数应用。\n在 Python 中可以借助 functools 模块来完成偏函数应用，而不用手工编写代码。\n看看 Demo：\n\u0026gt;\u0026gt;\u0026gt; from functools import partial \u0026gt;\u0026gt;\u0026gt; a = lambda x, y, z: x+y+z \u0026gt;\u0026gt;\u0026gt; bind_x = partial(a, 1) \u0026gt;\u0026gt;\u0026gt; bind_x(2, 3) 6 上述例子我们可以使用简单类型λ演算来表示\npapply : λ(f, x). λy. f (x, y) 函数式编程－加里化（Currying） # 是一种将带多个参数的函数转化成每次传入一个参数的函数链调用\nCurrying：因为是美国数理逻辑学家哈斯凯尔·加里(Haskell Curry)发明了这种函数使用技巧，所以这样用法就以他的名字命名为Currying，中文翻译为“加里化”。\n先看一个简单的例子：\ndef hiCurry(x): return lambda z: lambda y: x + y + z \u0026gt;\u0026gt;\u0026gt; hiCurry(1)(2)(3) 6 上述例子我们可以使用简单类型λ演算来表示\ncurry: λx. λy. λz. (x+y+z) 常规的函数 # def simple_sum(a, b): return sum(range(a, b+1)) \u0026gt;\u0026gt;\u0026gt; simple_sum(1, 10) 55 def square_sum(a, b): return sum(map(lambda x: x**2, range(a,b+1))) \u0026gt;\u0026gt;\u0026gt; square_sum(1,10) 385 使用 Currying # def fsum(f): def apply(a, b): return sum(map(f, range(a,b+1))) return apply log_sum = fsum(math.log) square_sum = fsum(lambda x: x**2) simple_sum = fsum(int) ## fsum(lambda x: x) \u0026gt;\u0026gt;\u0026gt; fsum(lambda x: x*2)(1, 10) 110 \u0026gt;\u0026gt;\u0026gt; import functools \u0026gt;\u0026gt;\u0026gt; fsum(functools.partial(operator.mul, 2))(1, 10) 110 使用闭包封装一个通用 Currying 的函数(来源 Cookbook) # def currying(f, *a, **kw): def curried(*more_a, **more_kw): return f(*(a+more_a), **dict(kw, **more_kw)) return curried \u0026gt;\u0026gt;\u0026gt; a = lambda a, b, c: a+b+c \u0026gt;\u0026gt;\u0026gt; fillA = currying(a, 1) \u0026gt;\u0026gt;\u0026gt; fillB = currying(fillA, 2) \u0026gt;\u0026gt;\u0026gt; fillC = currying(fillC, 3) \u0026gt;\u0026gt;\u0026gt; fillC() 6 标准库中利用 Currying 技术： # \u0026gt;\u0026gt;\u0026gt; from operator import itemgetter \u0026gt;\u0026gt;\u0026gt; itemgetter(3)([1,2,3,4,5]) 4 \u0026gt;\u0026gt;\u0026gt; from operator import attrgetter as attr \u0026gt;\u0026gt;\u0026gt; class Speaker(object): ... def __init__(self, name): ... self.name = \u0026quot;[name] \u0026quot; + name ... \u0026gt;\u0026gt;\u0026gt; alexey = Speaker(\u0026quot;Alexey\u0026quot;) \u0026gt;\u0026gt;\u0026gt; attr(\u0026quot;name\u0026quot;)(alexey) '[name] Alexey' \u0026gt;\u0026gt;\u0026gt; from operator import methodcaller \u0026gt;\u0026gt;\u0026gt; methodcaller(\u0026quot;__str__\u0026quot;)([1,2,3,4,5]) '[1, 2, 3, 4, 5]' \u0026gt;\u0026gt;\u0026gt; methodcaller(\u0026quot;keys\u0026quot;)(dict(name=\u0026quot;Alexey\u0026quot;, topic=\u0026quot;FP\u0026quot;)) ['topic', 'name'] \u0026gt;\u0026gt;\u0026gt; values_extractor = methodcaller(\u0026quot;values\u0026quot;) \u0026gt;\u0026gt;\u0026gt; values_extractor(dict(name=\u0026quot;Alexey\u0026quot;, topic=\u0026quot;FP\u0026quot;)) ['FP', 'Alexey'] \u0026gt;\u0026gt;\u0026gt; methodcaller(\u0026quot;count\u0026quot;, 1)([1,1,1,2,2]) \u0026gt;\u0026gt;\u0026gt; # same as [1,1,1,2,2].count(1) 3 程序就要小而美 # BAD # \u0026gt;\u0026gt;\u0026gt; ss = [\u0026quot;UA\u0026quot;, \u0026quot;PyCon\u0026quot;, \u0026quot;2012\u0026quot;] \u0026gt;\u0026gt;\u0026gt; reduce(lambda acc, s: acc + len(s), ss, 0) 11 NOT BAD\u0026hellip; # \u0026gt;\u0026gt;\u0026gt; ss = [\u0026quot;UA\u0026quot;, \u0026quot;PyCon\u0026quot;, \u0026quot;2012\u0026quot;] \u0026gt;\u0026gt;\u0026gt; reduce(lambda l,r: l+r, map(lambda s: len(s), ss)) 11 GOOD # \u0026gt;\u0026gt;\u0026gt; ss = [\u0026quot;UA\u0026quot;, \u0026quot;PyCon\u0026quot;, \u0026quot;2012\u0026quot;] \u0026gt;\u0026gt;\u0026gt; reduce(operator.add, map(len, ss)) 11 结语 # 系统地总结下自己对函数式编程的理解，没想到，收获颇丰！千万不要被外界评论给吓倒，绝对是一门有意思的学问。\n发现通过小小一个篇幅的博客，根本无法把函数式编程的全部概念都讲清楚，只能慢慢来了，越深挖，内容也多，来日方长，只能慢慢来了，就怕大脑不够用！已经被忽悠去学习 λ 演算！\nFP 哪里是编程，骨子里就是透着都是数学公式。\n未完待续 # 参考：\n* FUNCTIONAL PROGRAMMING WITH PYTHON * Pure Function\nCoolShell 函数式编程 函数副作用 * Higher-order function and First-class object * 函数式编程里的惰性求值 * 函数加里化(Currying)和偏函数应用(Partial Application)的比较 * Currying in Python * Recipe 16.4. Associating Parameters with a Function (Currying) ","date":"2014-08-04","permalink":"/n3xtchen/2014/08/04/python---functional-programming/","section":"时间线","summary":"关于函数式 # Imperative Programming 指令式编程（C/C++，Java）","title":"函数式编程 \u0026\u0026 Python - 第一部分"},{"content":"","date":"2014-07-25","permalink":"/n3xtchen/tags/coverage/","section":"标签","summary":"","title":"coverage"},{"content":"","date":"2014-07-25","permalink":"/n3xtchen/tags/doctest/","section":"标签","summary":"","title":"doctest"},{"content":"","date":"2014-07-25","permalink":"/n3xtchen/tags/nose/","section":"标签","summary":"","title":"nose"},{"content":"老是把 TDD 挂在嘴边，周边的人也不为之所动，终于下决心整理下文档，也顺便重温下学过的知识。\n我们将会提到两种 Python 的测试方式：doctest 和 unittest，还涵盖了测试覆盖率评定的库 nose 和 coverage\n准备你的要使用的工具 # 前提是你的环境至少 Python 2.7 以上，这里假定你已经安装了 pip (如果你没有的话，可以参照 pip 自行安装)；\ndoctest 是一个 Python 原生自带的标准模块，因此不需要安装；\n安装 Unittest2:\n$ pip install unittest2 安装 nose 和 coverage\n$ pip install nose $ pip install coverage 先从简单的 Doctest 开始 # Doctest 是 Python 中最简单的测试工具；他们主要有三个好处：\n帮助注释如何使用该方法或函数 如果你变更你的接口，它将提醒你更新你的 docstring 进行简单的测试的同时也不会打断你工作 值得一提的是， doctest 并不打算成为一个完整功能的单元测试框架。他们的哲学就是为每一个公共的方法使用一两个例子注释说明，然后 move on；但是需要提醒的是不要使用完整的测试来污染你的文档，而且 doctest 也会很慢。\n先来看一个简单的例子：\n# filename: module_demo.py def add(arg): \u0026quot;\u0026quot;\u0026quot; Perform an int type-conversion on arg and add 4 Example ------- \u0026gt;\u0026gt;\u0026gt; add('12') 16 \u0026quot;\u0026quot;\u0026quot; return int(arg)+4 if __name__=='__main__': import doctest doctest.testmod() 通过在底部包含 doctest.testmod() ，简单的运行它：\n$ python module_demo.py $ No news is good news;(没有新闻就是最好的新闻)，如果 doctest 通过测试，你将看不到任何信息。\n让我们改下返回值来演示下 doctest 测试失败的输出（将 16 改称 14），执行的结构如下：\n$ python module_demo.py ********************************************************************** File \u0026quot;module_demo.py\u0026quot;, line 21, in __main__.add Failed example: add('12') Expected: 16 Got: 14 ********************************************************************** 1 items had failures: 1 of 1 in __main__.add ***Test Failed*** 1 failures. 让我们还原成通过测试的值，然后接下来学习 uinttest\n使用 unittest2 # unittest 会执行名称包含 test 的类／方法，整体上必须符合下面两个标准：\n类必须继承 unittest.TestCase； 类和方法必须在它的名称上包含 test 字样； 使用 unittest2 的规则：\n每个测试用例的测试应该重点放在少数代码中，避免测试太多代码，因为一个用例很难覆盖一个复杂的类 每个测试之间必须是相互独立（因为 set-up/tear-down 在执行每一个用例都会被触发） 失败不能阻塞不相关测试的运行 需要测试的项目\n类／方法的输入 标准值 极值测试（Cornor Case） 上面情形的组合 负面测试（Nagative Tests） 错误的类型 错别字（Typos）／操作失误（fat-fingers）／大脑短路（brain-fart）的条件 预期的错误条件 日志 API 的一致性 迭代测试某类的子类，确保预期的方法可访问 一个简单的 unitest2 的例子 # 第一个例子，我们将抛出几个测试用例\n#! /usr/bin/env python # -*- coding: utf-8 -*- # filename: test_demo.py # 载入 unittest 库 try: import unittest2 as unittest except ImportError: import unittest # 导入要测试的目标代码 import module_demo add = module_demo.add # 名称必须包含 test, 必须继承 unittest.TestCase class test_Add_Args(unittest.TestCase): \u0026quot;\u0026quot;\u0026quot; test arguments sent to add() \u0026quot;\u0026quot;\u0026quot; def test_inputs(self): self.assertEqual(add(12), 16) self.assertEqual(add(12.0), 16) self.assertEqual(add('12.0'), 16) self.assertEqual(add('-12.0'), -8) self.assertEqual(add(-12.0), -8) if __name__ == '__main__': unittest.main() 当你执行它的时候，程序将会报错\n$ python test_demo.py E ====================================================================== ERROR: test_inputs (__main__.test_Add_Args) ---------------------------------------------------------------------- Traceback (most recent call last): File \u0026quot;test_demo.py\u0026quot;, line 24, in test_inputs self.assertEqual(add('12.0'), 16) File \u0026quot;/Users/ichexw/Dev/python/TDD/module_demo.py\u0026quot;, line 24, in add return int(arg)+4 ValueError: invalid literal for int() with base 10: '12.0' ---------------------------------------------------------------------- Ran 1 test in 0.001s FAILED (errors=1) 我们刚捕获了我们的第一个 bug；我不能预料到 int('12.0') 的问题，因此我们需要修改 add() 为 `return int(float(arg))+4。修改后，我们再次执行它：\n$ python test_demo.py . ---------------------------------------------------------------------- Ran 1 test in 0.000s OK 整合 Doctest 和 Unittest2 # 假设你想要让 doctest 和 unittest2 一块执行：\n我们仍然继续执行 simple_test.py 模块的方法 同时还需要测试在定义在 module_demo.py 中的 doctest 我们把下面的代码整合到 test_demo.py (即放在执行 unittests 的代码中)： 代码如下：\n#! /usr/bin/env python # -*- coding: utf-8 -*- # filename: test_demo.py # 载入 unittest 库 try: import unittest2 as unittest except ImportError: import unittest # 导入要测试的目标代码 import module_demo import doctest def load_tests(loader, tests, ignore): \u0026quot;\u0026quot;\u0026quot;Run doctests from this.py\u0026quot;\u0026quot;\u0026quot; tests.addTests(doctest.DocTestSuite(module_demo)) return tests add = module_demo.add # 名称必须包含 test, 必须继承 unittest.TestCase class test_Add_Args(unittest.TestCase): \u0026quot;\u0026quot;\u0026quot; test arguments sent to add() \u0026quot;\u0026quot;\u0026quot; def test_inputs(self): self.assertEqual(add(12), 16) self.assertEqual(add(12.0), 16) self.assertEqual(add('12.0'), 16) self.assertEqual(add('-12.0'), -8) self.assertEqual(add(-12.0), -8) if __name__ == '__main__': unittest.main() 现在，我们的 doctests 和 unittests 将整合在一块运行。\n查看代码覆盖率 # 如果你使用 nose 插件，那 coverage 可以为你代码的覆盖率测试生成一些漂亮的报告。\n$ coverage erase $ nosetests --with-coverage module_test . Name Stmts Miss Cover Missing ------------------------------------------- module_demo 6 2 67% 26-27 ---------------------------------------------------------------------- Ran 1 test in 0.017s OK 67% 的覆盖率太低了，所以我们需要修复它。通过分析代码，我们发现 coverage 把我们的 doctest 运行器当成 module_demo 的一部分。幸运的是，coverage 提供了一种方式来解决这类问题。我们只需要 if 语句后面添加 # pragma: no cover:\nif __name__=='__main__': # pragma: no cover import doctest doctest.testmod() 然后我们再次运行测试，100％ 的覆盖率 ^_^：\n$ nosetests --with-coverage module_demo Name Stmts Miss Cover Missing ------------------------------------------- module_demo 3 0 100% ---------------------------------------------------------------------- Ran 0 tests in 0.000s OK 结语 # 单元测试的工具越来越多，让测试驱动开发（TDD）变得更加的简便！但是由于大部分开发人员对测试不是很重要，甚至被作为额外无用的工作量！在长期的迭代开发过程中，有效的单元测试帮助我们的团队规避很多风险，把更安全地使用和修改过去写过的库或代码！所以思维的转变非常重要，在今后，我会分享更多的测试驱动开发的内容，希望对大家能有所帮助。\n引用 http://bucksnort.pennington.net/blog/post/python-unittest2-doctest/，\n","date":"2014-07-25","permalink":"/n3xtchen/2014/07/25/python-tdd---doctest-unittest2-coverage/","section":"时间线","summary":"老是把 TDD 挂在嘴边，周边的人也不为之所动，终于下决心整理下文档，也顺便重温下学过的知识。","title":"Python 测试驱动开发 - TDD初阶"},{"content":"","date":"2014-07-25","permalink":"/n3xtchen/tags/unittest2/","section":"标签","summary":"","title":"unittest2"},{"content":"PyMySQL 是由纯 Python 实现的 MySQL 客户端库；他的目标是为了作为 MySQLdb 的替代品，可以在 CPython，PyPy，IronPython 和 Jython 上运行。\n虽然由 C 实现的 MySQLdb 速度要快，但是 很遗憾 MySQLdb 是阻塞的，而 PyMySQL 和 gevent 可以很好地解决 MySQL 阻塞地问题。（我还发现另一个纯 Python 实现的 MySQL 库 - mysql-connector(MySQL 官方提供的) 同样也可以解决这个问题，有空再介绍）。\n进入正题，开始介绍：\n安装 PyMySQL # 使用 Pip 安装\npip install PyMySQL\n从源码安装\n从 https://pypi.python.org/pypi/PyMySQL 下载安装包 解压到指定的路径， 在命令行模式下进入安装目录，执行 python setup.py install 命令 使用 PyMySQL # 导入模块 # In [1]: import pymysql 连接数据库 # In [2]: conn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='', db='test') In [7]: c = conn.cursor() 执行查询 # 为了演示 PyMySQL 的多种数据类型的操作，创建一个拥有所有类型的表。\n1. 创建表 # In [8]: c.execute(\u0026quot;create table test_datatypes (b bit, i int, ....: l bigint, f real, s varchar(32), u varchar(32), bb blob, ....: d date, dt datetime, ts timestamp, td time, t time, st datetime)\u0026quot;) Out[8]: 0 2. 插入数据 # In [10]: import datetime, time In [11]: v = (True, -3, 123456789012, 5.7, \u0026quot;hello'\\\u0026quot; world\u0026quot;, ....: u\u0026quot;Espa\\xc3\\xb1ol\u0026quot;, \u0026quot;binary\\x00data\u0026quot;.encode(conn.charset), ....: datetime.date(1988,2,2), datetime.datetime.now(), ....: datetime.timedelta(5,6), datetime.time(16,32), time.localtime()) In [12]: c.execute(\u0026quot;insert into test_datatypes (b,i,l,f,s,u,bb,d,dt,td,t,st) values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\u0026quot;, v) Out[12]: 1 3. 查询数据(fetchOne) # In [13]: c.execute(\u0026quot;select b,i,l,f,s,u,bb,d,dt,td,t,st from test_datatypes\u0026quot;) Out[13]: 1 In [14]: r = c.fetchone() # 查询一条记录 In [15]: r Out[15]: ('\\x01', -3, 123456789012, 5.7, 'hello\\'\u0026quot; world', 'Espa\\xc3\\xb1ol', 'binary\\x00data', datetime.date(1988, 2, 2), datetime.datetime(2014, 7, 25, 1, 46, 6), datetime.timedelta(5, 6), datetime.timedelta(0, 59520), datetime.datetime(2014, 7, 25, 1, 46, 6)) 4. 删除数据 # In [23]: c.execute(\u0026quot;delete from test_datatypes\u0026quot;) Out[23]: 1 5. 查询多条数据（fetchAll） # In [19]: c.execute(\u0026quot;insert into test_datatypes (i, l) values (2,4), (6,8), (10,12)\u0026quot;) Out[19]: 3 In [20]: c.execute(\u0026quot;select l from test_datatypes where i in %s order by i\u0026quot;, ((2,6),)) Out[20]: 2 In [21]: r = c.fetchall() In [22]: r Out[22]: ((4,), (8,)) 6. 删除表 # In [16]: c.execute(\u0026quot;drop table test_datatypes\u0026quot;) Out[16]: 0 7. 批量插入数据(BulkInsert) # In [24]: c.execute( ....: \u0026quot;\u0026quot;\u0026quot;CREATE TABLE bulkinsert ....: ( ....: id int(11), ....: name char(20), ....: age int, ....: height int, ....: PRIMARY KEY (id) ....: ) ....: \u0026quot;\u0026quot;\u0026quot;) Out[24]: 0 In [25]: data = [(0, \u0026quot;bob\u0026quot;, 21, 123), (1, \u0026quot;jim\u0026quot;, 56, 45), (2, \u0026quot;fred\u0026quot;, 100, 180)] In [28]: c.executemany(\u0026quot;insert into bulkinsert (id, name, age, height) values (%s,%s,%s,%s)\u0026quot;, data) Out[28]: 3 好迟了，就写到这里了，由于 PyMySQL 的文档非常匮乏，于是萌生了写这篇教程的想法。还会继续更新。\nGood Night！欢迎多多提 Bug！\n","date":"2014-07-23","permalink":"/n3xtchen/2014/07/23/python-pymysql-basic/","section":"时间线","summary":"PyMySQL 是由纯 Python 实现的 MySQL 客户端库；他的目标是为了作为 MySQLdb 的替代品，可以在 CPython，PyPy，IronPython 和 Jython 上运行。","title":"Python - PyMySQL 初探"},{"content":"在 Web 的发展史上， PHP，Python 和 Ruby 几乎是同时出现的，PHP 以其简单的语法，较低的入门门槛，得到了广泛的传播，也是各大与排行榜单前十的常客。与 PHP 发展迅速所不相对称的是，在包管理方面发展非常缓慢，不象 node 的 npm 和 ruby 的 bundler，幸亏有了 composer 的出现，很大程度上改变这样的局势，虽然发展相对还是不够完善，但是它的存在，为 PHP 的持续流行提供了强有力的推动力。有了 Composer，你可以：\na. 你有一个依赖N多库的项目。 b. 这些库中一些又依赖于其他的库。 c. 你声明你所依赖的库。 d. Composer找出哪些包的哪个版本将会被安装，然后安装它们（也就是把它们下载到你的项目中）。\n你从此不再放在各种依赖问题，而且大部分主流的 PHP 框架和工具库都已经入住 composer 的官方资源库 packgagist（你也可以把你的项目开源到 packagist 上面）。\n安装你的 composer： # $ curl -sS https://getcomposer.org/installer | php $ sudo mv composer.phar /usr/local/bin/composer\t// windows 你可以设置 PATH 使得 composer 全局能访问 下面是一个让你的快乐和让你程序很好工作的小技巧。\n1. 用 composer init 开始你的程序 # composer 包含一个用 init 命令创建一个 composer.json 文件的过程：\n$ mkdir happy_composer \u0026amp; cd happy_composer $ composer init Package name (\u0026lt;vendor\u0026gt;/\u0026lt;name\u0026gt;) [ichexw/happy_composer]: ichexw/happy_composer Description []: 描述你的应用 Author [n3xtchen \u0026lt;echenwen@gmail.com\u0026gt;]: Next Chen \u0026lt;echenwen@gmail.com\u0026gt; Minimum Stability []: dev License []: GPL-2.0 Define your dependencies. Would you like to define your dependencies (require) interactively [yes]? no Would you like to define your dev dependencies (require-dev) interactively [yes]? no { \u0026quot;name\u0026quot;: \u0026quot;ichexw/happy_composer\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;描述你的应用\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;GPL-2.0\u0026quot;, \u0026quot;authors\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Next Chen\u0026quot;, \u0026quot;email\u0026quot;: \u0026quot;echenwen@gmail.com\u0026quot; } ], \u0026quot;minimum-stability\u0026quot;: \u0026quot;dev\u0026quot;, \u0026quot;require\u0026quot;: { } } Do you confirm generation [yes]? yes 2. 声明依赖关系 # 使用 require 命令安装你程序的依赖包。越来越多的 composer 的包可以满足你的开发需求，除了一部分旧的模块需要 pecl 来安装你的包。\n$ composer require monolog/monolog Please provide a version constraint for the monolog/monolog requirement: 1.0.* 这是 monolog 将会安装在你的项目根目录的 vendor 目录下，并将包声明记录到 composer.json 中：\n\u0026quot;require\u0026quot;: { \u0026quot;monolog/monolog\u0026quot;: \u0026quot;1.0.*\u0026quot; } 并生成一个 composer.lock，锁定项目的特定版本，防止误更新带来项目的不稳定。\n3. 自动加载（AutoLoad） # Composer 里面自带 PSR-0 自动加载机制，在项目里面加入下面一行代码：\ninclude_once 'path/to/vendor/autoload.php'; 就可以直接使用 composer 安装的模块了。\n4. 创建测试目录： # $ mkdir Tests $ cat phpunit.xml \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;phpunit backupGlobals=\u0026quot;false\u0026quot; backupStaticAttributes=\u0026quot;false\u0026quot; colors=\u0026quot;true\u0026quot; convertErrorsToExceptions=\u0026quot;true\u0026quot; convertNoticesToExceptions=\u0026quot;true\u0026quot; convertWarningsToExceptions=\u0026quot;true\u0026quot; processIsolation=\u0026quot;false\u0026quot; stopOnFailure=\u0026quot;false\u0026quot; syntaxCheck=\u0026quot;false\u0026quot; bootstrap=\u0026quot;vendor/autoload.php\u0026quot; \u0026gt; \u0026lt;!-- 注意指定 boostrap 的路径，不然单元测试将无法引用 compsoser 引用的包 --\u0026gt; \u0026lt;testsuites\u0026gt; \u0026lt;testsuite name=\u0026quot;PHP Jawbone-Up \u0026quot;\u0026gt; \u0026lt;directory\u0026gt;./Tests/\u0026lt;/directory\u0026gt; \u0026lt;!-- 只是测试目录 --\u0026gt; \u0026lt;/testsuite\u0026gt; \u0026lt;/testsuites\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;whitelist\u0026gt; \u0026lt;directory\u0026gt;./\u0026lt;/directory\u0026gt; \u0026lt;exclude\u0026gt; \u0026lt;!-- 指定测试不覆盖的代码 --\u0026gt; \u0026lt;directory\u0026gt;./vendor\u0026lt;/directory\u0026gt; \u0026lt;directory\u0026gt;./Tests\u0026lt;/directory\u0026gt; \u0026lt;/exclude\u0026gt; \u0026lt;/whitelist\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/phpunit\u0026gt; 如果你还没有安装 phpunit 的，也可以使用 composer 来安装；对于 phpunit 这类全局的组建，你可以进行系统级的安装：\ncomposer global require \u0026quot;phpunit/phpunit=4.1.*\u0026quot; 请确保 path 变量中包含有 ~/.composer/vendor/bin/；这样你就可以直接使用 phpunit 了。\n结语 # 由于受到 《 快乐Node程序员的10个习惯》 启发，于是决定写下了这些，希望能对大家有所帮助，顺便宣传下 composer。\n","date":"2014-07-04","permalink":"/n3xtchen/2014/07/04/php---happy-composer/","section":"时间线","summary":"在 Web 的发展史上， PHP，Python 和 Ruby 几乎是同时出现的，PHP 以其简单的语法，较低的入门门槛，得到了广泛的传播，也是各大与排行榜单前十的常客。与 PHP 发展迅速所不相对称的是，在包管理方面发展非常缓慢，不象 node 的 npm 和 ruby 的 bundler，幸亏有了 composer 的出现，很大程度上改变这样的局势，虽然发展相对还是不够完善，但是它的存在，为 PHP 的持续流行提供了强有力的推动力。有了 Composer，你可以：","title":"做一个快乐的 PHP Composer"},{"content":"R语言，一种自由软件编程语言与操作环境，主要用于统计分析、绘图、数据挖掘。R本来是由来自新西兰奥克兰大学的 Ross Ihaka 和 Robert Gentleman 开发（也因此称为R），现在由“R开发核心团队”负责开发。R是基于S语言的一个GNU计划项目，所以也可以当作S语言的一种实现，通常用S语言编写的代码都可以不作修改的在R环境下运行。R的语法是来自Scheme。\nR 提供一个强大和漂亮的交互环境，用于探索数据。由于 R 是免费的，使它变得更加的流行。目前存在的一批数据分析工具（如 S，MATLAB，SPSS 和 SAS）非常的昂贵，而 R 确实是实现同样效果成本最低廉的一种方式；而且 R 的社区拥有大量的领域专家和开发者，其中不乏统计学家，数据科学家，他们贡献了很多非常有用的包，来帮助拓展 R 的能力。\nR的源代码可自由下载使用，亦有已编译的可执行文件版本可以下载，可在多种平台下运行，包括UNIX（也包括FreeBSD和Linux）、Windows和MacOS。\n目前的稳定版是 3.10\n安装 R # R 提供几种可选方式安装方式：\n1. 从 Cran 下载软件包 # R可以在CRAN(Comprehensive R Archive Network)上免费下载。Linux、 Mac OS X和Windows都有相应编译好的二进制版本。根据你所选择平台的安装说明进行安装即可。\n2. 使用 Linux 包管理器安装 # 基于 Redhat 的系统：\n$ yum install R 基于 Debian 的系统：\n$ sudo apt-get isntall r-base 3. 编译安装（不推荐） # 启动你的 R # R 由多个部分组成，其中包括一个美观的图形用户界面。在 OS X 和 Linux 下，你可以简简单单在终端的敲击 R 就可以启动它了：\n$ R 当然还有很多非官方的 R 图形编程环境，例如 R Commander 和 RStudio。RStudio 不仅提供了很漂亮的图像界面，而且还有服务器版本（RServer），让你在网页中编程和本地一样舒适。当然你也可以使用你最喜欢的自处理软件（如 Vim， Emacs 和 Sublime）。\nHello, R! # 我们先从简单的 R 终端开始把。\n\u0026gt; height \u0026lt;- c(58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72) 函数 c() 告诉我们创建的是一个名为 height 向量（Vector，一个有序的数字集合，这里的单位是 inch），\u0026lt;- 是 R 的赋值符号（我们也可以使用 =）。接下来，我们用同样的方法创建一个体重向量：\n\u0026gt; weight \u0026lt;- c(115, 117, 120, 123, 126, 129, 132, 135, 139, 142, 146, 150, 154, 159, 164) 现在我们有数据了，用它来做点东西。我们想要算出身高的平均值：\n\u0026gt; mean(height) [1] 65 答案是 65，你会注意到答案之前的 [1]。它代表返回值的第一行，如果你的结果有用多项，那它就会一直往下现实，并标出对应的行数。\n回到我们的例子。如果我们想求出体重的标准差（Standard Deviation），我们可以使用 sd() 函数：\n\u0026gt; sd(weight) [1] 15.49869 现在我们想找出女性的体重是否和身高相关。这时，我们使用 cor() 函数，它会帮我们找出女性的体重和升高的区别：\n\u0026gt; cor(weight, height) [1] 0.9954948 我们发现它们之间存在非常强的线性相关（linear correlation）；几乎 1 比 1 的关系。最后，我们可视化它们的相关性：\n\u0026gt; plot(weight, height) 它将通过散点图的方式为我们展示两个向量的之间的关系。\n工作空间（Workspace） # 工作空间(workspace)就是当前R的工作环境,它储存着所有用户定义的对象(向量、矩阵、函数、数据框、列表)。在一个R会话结束时,你可以将当前工作空间保存到一个镜像中,并在下次启动R时自动载入它。各种命令可 在R命令行中交互式地输入。使用上下方向键查看已输入命令的历史记录。命令的历史记录保存到文件.Rhistory 中,工作空间(包含向量x)保存到文件 .RData 中。 当前的工作目录(working directory)是R用来读取文件和保存结果的默认目录。我们可以使用函数 getwd() 来查看当前的工作目录,或使用函数 setwd() 设定当前的工作目录。如果需要读入一个不在当前工作目录下的文件,则需在调用语句中写明完整的路径。记得使用引号闭合这些目录名和文件名。\n执行（Source）文件和运行命令 # 上一个章节的所有代码都可以输入到 R 交互终端上执行。如果你已经把所有的代码都写到文本中，你可能对讲代码剪切到终端上没有兴趣。我们将提供两种方法运行文件上的代码。\n打开 R 终端，并使用 source() 命令。\n$ R \u0026gt; source('path/to/file.R') 使用 R 的批处理命令\n$ R CMD BATCH path/to/file.R 包（Packages） # 一个 R 包是一个相关函数和帮助文档的合集。它和 Ruby 的 Gem 或者 C/C++ 库很类似。正常情况，一个单独包中的所有函数应该是相关的：例如 stats 的包只包含统计分析的函数。和 Ruby 一样，R 包也有自己的公共源：\nCRAN（Comprehensive R Archive Network，http://cran.r-project.org）：目前最大的 R 综合资源网络，由 R 基金会所管理（同样也是 R 的主要开发者），目前拥有 5599 个包（2014-06-08），还在持续增长中；全世界拥有众多的镜像网站（国内拥有 cTex.com, 中国科技大学，北交大以及厦大）；\nBioconductor（http://www.bioconductor.org），旨在提供生物信息学的 R 分析包。然而并不以为者他们不适用于其他分析领域。目前拥有包数量 824个（2014-06-08）；\nR-Forge，一个用于R的协作开发环境。它基于 FusionForge，是 GForge（也是 RubyForge 的底层）的一个分支，目前拥有 1760 个项目。它不同于 Cran 和 Bioconductor 的是任何人都可以在 R-Froge 上设立项目，甚至不需要最终形成 R 包；\nGitHub，这个就多介绍了，现在很多新的 R 包项目建立在 Github 上，这个得益于 RStudio 首席科学家 Hadley Wickham 的 devtools(它的项目地址：https://github.com/hadley/devtools)，允许我们通过它来安装建立在 github 上的 R 包。\nPackage 的安装 # R 提供了几种安装方法：\n从 Cran 中安装：\n\u0026gt; install.packages( + c(\u0026quot;xts\u0026quot;, \u0026quot;zoo\u0026quot;), # 指定要安装的包 + lib = \u0026quot;some/other/folder/to/install/to\u0026quot;, # 指定安装位置 + repos = \u0026quot;http://www.stats.bris.ac.uk/R/\u0026quot; # 指定安装来源 + ) 离线安装：\n\u0026gt; install.packages( + \u0026quot;path/to/downloaded/file/xts_0.8-8.tar.gz\u0026quot;, + repos = NULL, #NULL repo 代表已经下载 + type = \u0026quot;source\u0026quot; # 直接安装 + ) 从 Github 中安装（依赖 devtools）：\n＃ 安装 devtools \u0026gt; install.packages(\u0026quot;devtools\u0026quot;) ＃ 使用方法 \u0026gt; library(devtools) \u0026gt; install_github(\u0026quot;knitr\u0026quot;, \u0026quot;yihui\u0026quot;) Package 的使用 # 你可以使用 library 函数载入你安装好的包：\n\u0026gt; library(package_name) ＃ 不用带引号 这个函数名让人感觉很怪异，如果叫 load_package 可能会更好理解，但是历史原因，很难改变了。我们可以把 package 当作 R 函数和数据集的集合，一个 library 用来保存这个 package 的文件。\n注意，传入的包名是没有引号的，如果你的包名是字符型，应该这么处理：\n\u0026gt; library(\u0026quot;package_name\u0026quot;, character.only =TRUE) 如果你使用的包没有安装，则会抛出一个异常，有时你需要处理这个情况：\n\u0026gt; if (!require(noExistedPackage)) + { + warning(\u0026quot;warn msg\u0026quot;); + } 查看已经载入的包\n\u0026gt; search() [1] \u0026quot;.GlobalEnv\u0026quot; \u0026quot;package:stats\u0026quot; \u0026quot;package:graphics\u0026quot; [4] \u0026quot;package:grDevices\u0026quot; \u0026quot;package:utils\u0026quot; \u0026quot;package:datasets\u0026quot; [7] \u0026quot;package:methods\u0026quot; \u0026quot;Autoloads\u0026quot; \u0026quot;package:base\u0026quot; 查看安装的包：\n\u0026gt; installed.packages() 查看 R 公共库的默认安装路径：\n\u0026gt; R.home(\u0026quot;library\u0026quot;) # 或者 \u0026gt; .library() [1] /path/to/r/3.0.2/R.framework/Versions/3.0/Resources/library 查看 R 用户库的安装路径：\n\u0026gt; path.expand(\u0026quot;~\u0026quot;) # 或者 \u0026gt; Sys.getenv(\u0026quot;HOME\u0026quot;) [1] /home/xx-user/ 查看所有库的路径，第一个是默认安装路径：\n\u0026gt; .libPaths() [1] /path/to/r/3.0.2/R.framework/Versions/3.0/Resources/library [2] /home/xx-user/ 维护 # # 更新包 \u0026gt; update.packages(ask = FALSE) # 删除包 \u0026gt; remove.packages(\u0026quot;packName\u0026quot;); Note：Os X 报错 fatal error: 'libintl.h' file not found 的解决方案： $ brew intall gettext\n$ find ./ -name \u0026ldquo;libintl.\u0026rdquo; ./0.18.3.2/include/libintl.h ./0.18.3.2/lib/libintl.8.dylib ./0.18.3.2/lib/libintl.a ./0.18.3.2/lib/libintl.dylib ./0.18.3.2/share/gettext/intl/libintl.rc $ ln -s /usr/local/Cellar/gettext/0.18.3.2/lib/libintl. /usr/local/lib/ $ ln -s /usr/local/Cellar/gettext/0.18.3.2/include/libintl.h /usr/local/include/libintl.h\n如何寻找帮助 # R提供了大量的帮助功能,学会如何使用这些帮助文档可以在相当程度上助力你的编程工作。\nhelp.start() | 入门帮助 help(\u0026quot;foo\u0026quot;) or ?foo | 获取 foo 帮助 help.search(\u0026quot;foo\u0026quot;) or ??foo | 搜索函数名带 foo 的帮助 example(\u0026quot;foo\u0026quot;) | 执行函数 foo 的用例，引号是可选的 RSiteSearch(\u0026quot;foo\u0026quot;) | 在线上文档或者邮件列表存档中搜索 foo apropos(\u0026quot;foo\u0026quot;, mode=\u0026quot;function\u0026quot;) | data() | 列出所有的可用数据集 vignette() | 列出所有的可用事例 vignette(\u0026quot;foo\u0026quot;) | 列出主题foo的事例 ","date":"2014-06-08","permalink":"/n3xtchen/2014/06/08/r-stats---intro/","section":"时间线","summary":"R语言，一种自由软件编程语言与操作环境，主要用于统计分析、绘图、数据挖掘。R本来是由来自新西兰奥克兰大学的 Ross Ihaka 和 Robert Gentleman 开发（也因此称为R），现在由“R开发核心团队”负责开发。R是基于S语言的一个GNU计划项目，所以也可以当作S语言的一种实现，通常用S语言编写的代码都可以不作修改的在R环境下运行。R的语法是来自Scheme。","title":"R Statistics- 入门"},{"content":" 安装环境： # 软件列表： # Java-1.6.0 或者更高的版本 ant（包含 ant-withouthadoop） 设置环境变量： # $ export JAVA_HOME=path/to/java/home 从源码中安装: # $ svn co http://svn.apache.org/repos/asf/pig/trunk /\u0026lt;my-path-to-pig\u0026gt;/pig-n.n.n $ export PATH=/\u0026lt;my-path-to-pig\u0026gt;/pig-n.n.n/bin:$PATH $ export PIG_HOME=/\u0026lt;my-path-to-pig\u0026gt;/pig-n.n.n/ 安装本地运行（Local）模式 # $ cd $PIG_HOME $ ant jar-withouthadoop 安装 Piggybank # $ cd $PIG_HOME $ cd contrib/piggybank/java/ $ ant 运行 Pig： # 交互模式：\n$ pig -x local ... grunt\u0026gt; 批处理模式：\n$ pig -x local xxx.pig ... 简单的配置 # $ cat $PIG_HOME/conf/pig.properties ... #exectype local|mapreduce, mapreduce is default exectype=local # 设置它的默认模式，你就不需要使用 -x local 参数 ... ... pig.logfile=/home/vagrant/pig/logs/ # 配置你日志搭默认路径，不设置日志将会在运行的当前目录 ... ","date":"2014-02-27","permalink":"/n3xtchen/2014/02/27/hadoop---data-pig---installation/","section":"时间线","summary":"安装环境： # 软件列表： # Java-1.","title":"Data Pig - 手动安装（Local 模式）"},{"content":"","date":"2014-02-27","permalink":"/n3xtchen/tags/hadoop/","section":"标签","summary":"","title":"hadoop"},{"content":"","date":"2014-02-27","permalink":"/n3xtchen/categories/hadoop/","section":"分类页","summary":"","title":"Hadoop"},{"content":"","date":"2014-02-27","permalink":"/n3xtchen/tags/pig/","section":"标签","summary":"","title":"pig"},{"content":"通过三个章节的铺垫，现在开始深入了解 Pig 专属语言的 Pig Latin； 再次强调下， Pig Latin 是一门数据流语言（dataflow language）。\n关系（relation）和字段（field）的命名 # 每一个操作步骤都会产生新的数据集，或者关系。举个例子，看下面的代码：\ninput = LOAD 'data'; input 是载入 data 数据的关系（relation）名。关系名看起来想一个变量；实际上， 他们并不是。一旦创建，这个赋值是永久的。但是关系名只是一个别名（aliAS），它可以 被重复赋值，会覆盖之前的关系，例如：\ninput = LOAD 'data' AS (id, amount); input = FILTER input BY amount \u0026gt; 0; 关系（relation）名和字段（field）都必须字母打头，然后之后可以是数字，字母或 者下划线的任意组合。所有名称的字符必须符合 ASCII 标准。\n大小写敏感（Case Sensitivity） # Pig Latin 中的的保留字（keywords）是大小写不敏感的，例如， LOAD 等同于 LOAD；\n但是，关系和字段名是大小写敏感的；用户自定义的函数（UDF）也是大小写敏感的；\n注释（Comments） # 单行注释：--；\n多行注释：\n/* 在这里注释 */\n输入（input）和输出（output） # 开搞前，你必须知道如何输出和输出数据。\n载入（LOAD） # 数据流的第一步就是指定你的输入。\ninput = LOAD '路径' [USING LoadFunction(args)]; 路径 # input = LOAD 'HDFS 中的绝对路径'; input = LOAD './相对路径'； Pig 默认是在你的 HDFS 的 Home 目录下，/users/youlogin，执行的；\ninput = LOAD '../pig_home/dataset'; 它将在你的 Home 目录的上一级目录的 pig_home 目录中的 datASet 文件中查找数据 ；当然让你改变你的目录位置，那所有的相对路径以那里作为基准。\nPig 还支持以 URL 路径来查找数据；\ninput = LOAD 'hdfs://nn.n3xt.com/data/examples/datASet' 除了指定文件路径，你还可以传入目录，这样 Pig 就会载入该目录的所有文件。\n加载函数（loading Function） # 默认情况下，LOAD 使用默认的加载函数 PigStorage 在 HDFS 中的制表符分隔 （tab-delimited）的文件中寻找数据的；\n实际上，你的大部分数据并不是以制表符分隔的，你也有可能载入 HDFS 之外的存储器 中的数据。Pig 允许你是用 USING 语句来指定载入函数，例如\nlog = LOAD '/path/to/log' USING PigStorage(','); -- 以逗号分隔的方式载入数据 log = LOAD '/path/to/log' USING HBaseStorage(); -- 使用 HBase Pig 有两个内建的加载函数，PigStorage 和 TextLOADer，来操作 HDFS 上的文件 ，并支持 glob（载入匹配模式的文件和目录）。使用 glob，你可以读取在不同目录下 的多个文件，或者同一目录的部分文件。\nGlob 匹配符：\n? ：任意单字符 * ：0到多个字符 [abc] ：匹配其中的一个字符 [a-z] ：匹配任意一个小写字母 [^abd] ：匹配任一不属于该字符集的字符 \\ ：转移特殊富豪 {ab,cd} ：匹配字符串集中的任意一个 定义数据模式（Scheme） # LOAD 可以使用 AS 来指定你的数据模式：\nlog = LOAD \u0026quot;path/to/log\u0026quot; AS (u_id, login_time); -- data 是关系名 模式的定义详见：()[]\n存储（STORE） # LOAD 的反向工作，语法如下：\nSTORE data INTO `/path/to/storage` [USING storageFunction()]; 几种用法的范例：\n-- 使用默认的 PigStorage 存储在 HDFS 上，并使用制表符分隔 STORE processed INTO '/data/examples/processed'; -- 使用 HBase 存储 STORE processed INTO 'processed' USING HBaseStorage(); -- 字段使用逗号分隔 STORE processed INTO 'processed' USING PigStorage(','); Note，上述的 processed 将会是一个文件夹，里面包含多个数据文件，而不是单个 文件。数据文件的个数取决于并行计算工作及它的工作并行程度。\nDUMP 输出 # 大部分情况，你是要把处理完的数据存储起来；但是有时你需要在屏幕看结果。这个在 调试和构建模型的过程时非常有用；\nDUMP processed; 关系操作（Relational Operations） # 关系操作是 Pig 操作数据的主要工具。它允许你通过排序，分组，连表，映射以及i\u0026gt; 筛选来转换你的数据。\n循环 FOREACH # FOREACH 可以使用表达式，并把它应用到数据管道中的每一条数据。从这个表达式中 生成新的数据并传递给下一个操作器。\nA = LOAD 'input' AS (user:chararray, id:long, phone:chararray); B = FOREACH A GENERATE user, id; FOREACH 中使用的表达式 # FOREACH 支持表达式数组；最简单的就是常量和字段索引。常量我们已经在之前数据类 型中讨论过了。字段索引可以通过字段名或者位置。位置索引（Positional references）是使用 $ 和从0开始的整数：\nprices = LOAD 'NYSE_daily' AS (exchange, symbol, date, open, high, low, close, volume, adj_close); gain = FOREACH prices GENERATE close - open; gain2 = FOREACH prices GENERATE $6 - $3; gain 和 gain2 中存储的数据是一样的。位置风格的索引在某种情况下是非常有用的， 例如不确定数据结构的情况下。\n字段集的操作：\nprices = LOAD 'NYSE_daily' AS (exchange, symbol, date, open, high, low, close, volume, adj_close); -- 截取数据字段从开头到 open 字段位置，包括 open beginning = FOREACH prices GENERATE ..open; -- produces exchange, symbol, date, open -- 截取数据字段从 open 字段和 close 字段之间的字段，包括 open 和 close middle = FOREACH prices GENERATE open..close; -- produces open, high, low, close -- 截取数据字段从 volumn 字段开始到结尾的位置，包括 volumn end = FOREACH prices GENERATE volume..; -- produces volume, adj_close 另外，Pig 还支持标准的算数操作：\n+ 加 - 减 * 乘 / 除 % 取模 ?: 三元表达式，三元表达式应该使用括号包围起来 Note: 三元表达式的返回值的类型必须相同；如果条件返回空，结果也将是空：\n1 == 1 ? 1 : 'yo' -- 这将会报错， NULL == 2 ? 2 : 4 -- 结果将会返回 NULL 从复杂的数据结构中提取数据：\n-- Map 映射 -- 如果在 Map 找不到相应的映射，则 NULL bball = LOAD 'bASeball' AS (name:chararray, team:chararray, position:bag{t:(p:chararray)}, bat:map[]); -- 在字段 bat 中寻找 batting_average 的值 avg = FOREACH bball GENERATE bat#'batting_average'; -- 元组映射 A = LOAD 'input' AS (t:tuple(x:int, y:int)); B = FOREACH A GENERATE t.x, t.$1; -- Bag 映射 A = LOAD 'input' AS (b:bag{t:(x:int, y:int)}); B = FOREACH A GENERATE b.x; C = FOREACH A GENERATE b.(x, y); -- \u0026lt;=\u0026gt; b.x, b.y 元组和 Bag 是通过点号（.）来调用数据的，Map 则是使用井号（#）。\nFOREACH 中的用户自定义函数（UDF） # UDFs 可以在 FOREACH 中定义。这里我们称之为求值函数（eval funcs）。因为他们是 freach 语句的一部分，所以 UDFs 每次只能处理一条记录和产生一条输出。例如，如 下的 UPPER 函数来处理每条记录：\n-- FOREACH_udf.pig divs = LOAD 'data/names' AS (name); upper = FOREACH divs GENERATE name, UPPER(name); DUMP upper; UDFs 可以接受 * 作为参数，表示传入的的一整条记录。\n具体 UDF 用法和相关函数我们将在后面的专题介绍，这里一笔带过。\nFOREACH 的字段命名 # Pig 可以对每个字段的类型进行推断，但是它不能对这些字段的名称进行推断。\n如果只是对字段的简单映射（Projection），Pig 会保持之前的名称：\ndivs = 'data/names' AS (name); name = FOREACH divs GENERATE name; DESCRIBE name; -- 返回：name:{name: chararray} -- `DESCRIBE` 可以打印数据结构 但是使用任何表达式应用在普通的映射之上的话，Pig 就无法给这个字段命名了，如果 你没有明确制定名称，这个字段将无名称，只能通过位置参数来访问（例如，$0）; 你可以使用 AS 来命名该字段：\ndivs = LOAD 'data/names' AS (name); name = FOREACH divs GENERATE name AS key, UPPER(name); DESCRIBE name; -- 返回：name:{key: chararray, chararray} 筛选（Filter） # FILTER 允许你从数据管道中选择你想要的记录；它包含一个断言，如果断言为真， 则传入管道中，反之被抛弃。\n断言（predicates）可以包含: ==、 !=、 \u0026gt;、 \u0026gt;=、 \u0026lt; 以及 \u0026lt;=。这些对比表达式可以 用在任何基础数据类型中。== 和 != 还可以用在 map 和tuple 数据类型上。如果将 他们用在两个 tuple 上，那这两个元组必须是模式相同或者无模式的。相等操作不可 以用在 bag 类型上。\nPig Latin 也和大部分语言一样，操作符拥有优先等级，算术（arithnetic）操作符 优先于相等运算符。因此，x+y == a+b \u0026lt;=\u0026gt; (x+y) == (a+b).\n-- filter_basic.pig lang_age = LOAD 'data/progLang_age' AS (progLang:chararray, birthd ay:int); ancient = FILTER lang_age BY birthday \u0026lt; 1980; DUMP ancient; -- 筛选出开发起始时间在 1980 之前的程序语言 对于字符串，你可以使用关键字 `matches` 和正则表达式来正则匹配： FILTER data_pineline BY [NOT] field_name MATCHES '正则写在这'； 例子：\n-- filter_chararray.pig lang_age = LOAD 'data/progLang_age' AS (progLang:chararray, birthday:int); InitialP = FILTER lang_age BY progLang MATCHES 'P.*'; DUMP InitialP; -- 筛选出语言名称以 P 打头的程序语言 Note: Pig 使用 Java 的正则表达式的格式。他是全匹配的，不想 perl 那样的部分 匹配。例如，如果你想要查找字段包含 prog 的字符，表达式必须写成 .*prog.*， 而不是 prog；后者只能匹配到 prog 这个字符串。\n你还可以使用布尔操作符（AND、OR 和 NOT）来组合多个断言；另外这三个的优先级 如下： NOT \u0026gt; AND \u0026gt; OR\n它遵循短环布尔操作，就是如果表达式提前别断言为 false 它将不会继续执行下去 ，而是直接返回。\nNote: NULL 遵循 SQL 逻辑，空值代表非真和假，空值不会和任何正则表达式匹配成 功。\n分组（Group） # GROUP 语句将拥有相同健的数据汇总在一块。如果你学习过 SQL，对这个概念再熟悉 不过了，但是 Pig 的分组操作和 SQL 存在本质上的区别，这个是你必须注意的。在 SQL 中，GROUP BY 语句创建分组，直接传递给一个或多个聚合（Aggregate）函数。 在 Pig 中，分组和聚合没有必然的联系；它的分组更纯粹：它的工作就是将数据分组 ，并存储在 bag 中。如果你需要聚合，你可以这么操作：\n-- group_basic.pig rock_stars = LOAD 'data/RockNRollHallOfFrame' USING PigStorage(',') AS (joined_year:int, band_name:chararray); grpd = GROUP rock_stars BY joined_year; cnt = FOREACH grpd GENERATE group, COUNT(rock_stars); DUMP cnt; -- 每个摇滚名人堂年度被评上的乐队个数 分组的数据在 Pig 中是如何呈现的呢？让我们 DESCRIBE 下吧！\n-- 这是上述 grpd 的结构 grpd: {group: int,rock_stars: {(joined_year: int,band_name: chararray)}} -- group 存储的是键，这里是获奖年度， -- 后面是拥有该键的所有记录组成的元组 你可以将分组的数据存储起来，以备后面处理：\nSTORE grpd INTO 'data/by_year'; 用过 SQL 的都知道，有时候不仅仅只对一个键来分组：\n-- group_multi.pig artists = LOAD 'data/RockNRollHallOfFrameSidemen' USING PigStorage(',') AS (year:int, name:chararray, instrument:chararray); 5 multi_grpd = GROUP artists BY (year, instrument); DUMP multi_grpd; -- 按照使用乐器和入选年限对摇滚默默奉献奖进行分组 Note: 组合键是使用元组来呈现。\n你还可以有使用 ALL 将所有记录组合起来：\n-- group_all.pig artists = LOAD 'data/RockNRollHallOfFrameMultipleInductees' USING PigStorage(','); grpd_all = GROUP artists ALL; cnt = FOREACH grpd_all GENERATE COUNT(artists); DUMP cnt; -- 这就是 ALL 的其中一个用于，聚合计算出记录条数 由于根据键来分组数据，你经常会得到不对称的结果。那是因为你无法决定每个键最终 分配到数据的条数给相应 reducer。例如，你有一分网页链接索引，你需要对他们的 域名分组。像 sohu.com 这样类型的门户网站的实际数量肯定要比大部分网站的多好几 个 量级。这样，sohu.com 这个键的 reducer 获得的数据远比其他的要多很多。由于 你的 MapReduce 工作需要你的所有的 reducer 都完成之后才能结束，这样数据分配不 均大大地降低了处理效率。而且有时，一个 reducer 也不可能管理那么多的数据。\nPig 为了解决这样的不均衡提供很多种方式。Hadoop 的联结器（Combiner）就是其中 一种方式，这里不具体阐述。虽然它并不能完全的均衡，总有限制它的边界就可以了。 因为 Mapper 的数量可能成千上万，即使 reducer 得到不均衡的记录，如果每个 reducer 的数据足够小,那它就能快速的处理它，就不会被阻塞。\n不幸的是，并不是所有的计算都可以使用 combiner。线性相关的计算能很好的适应 combiner。\nPig 的操作符和内置 UDF 都尽可能使用 combiner。由于 reduce 的不均衡，它在早期 聚合时大量的减少了数据的传输以及读写，因此大大提高的性能。UDF 可以通过应用 代数相关的接口（Algebraic Interface）来指定什么使用调用 combiner。后续将详细 说明。\n排序（ORDER BY） # ORDER 语句将会产生你输出的数据的总排序。总排序（Total Order）意味着不仅仅在 各自的分区中排序，必须保证在第N个分区的所有记录都比第N-1个分区的顺序大。当 你的数据存储在 HDFS 中，每个分区都是一个文件，这意味将会按顺序输出数据。\nORDER 的语法和 GROUP 类似，\n-- order_basic.pig rock_stars = LOAD 'data/RockNRollHallOfFrameInductees' USING PigStorage(',') AS (joined_year:int, band_name:chararray); -- 按照入选年度正序（不指定，默认为 ASC）排列 by_year = ORDER rock_stars BY joined_year; -- 数据结构 DESCRIBE by_year; -- 结果：by_year: {joined_year: int,band_name: chararray} DUMP by_year; 和 GROUP 一样，它也支持多键排序：\n-- order_multi.pig artists = LOAD 'data/RockNRollHallOfFrameMultipleInductees' USING PigStorage('|') AS (name, first_band:chararray, first_year:int, sec_band:chararray, sec_year:int, third_band:chararray, third_year:int ); -- 根据初次入选和第二次入选倒序排序 by_first_year_second = ORDER artists BY first_year DESC, sec_year DESC; DUMP by_first_year_second; 排序的方式是基于该字段的数据类型：数值类型以数字排序，字符串（chararray）和 字节串（bytearray）是以字符排序。注意，不要对 Map，tuple 和 bag 进行排序， 否则会报错。对于所有数据类型，null 比其他的任何可能值都要小。\n去重（DISTINCT） # DISTINCT 语句很简单，去除记录中的重复项目。\n--distinct.pig -- find a distinct list of ticker symbols for each exchange -- This load will truncate the records, picking up just the first two fields. daily = load 'NYSE_daily' as (exchange:chararray, symbol:chararray); uniq = distinct daily; 由于它把所有的记录都手机其爱，DISTINCT 将操作强制进入 reduce 阶段。它也可以 使用 combiner 去除重复，它在 Map 阶段将它删除。\n联表（JOIN） # 联表是数据处理中最耗资源的工作之一。JOIN 从一个输入中选择数据整合另一个输入 的数据中，通过指定键进行联表。和 GROUP，ORDER 一样，它也可以多键联表。\n--join_basic.pig today = LOAD 'data/stock-2014-02-17.csv' USING PigStorage(',') AS (exchange, symbol, name, open, high, low, close, volumn); lastday = LOAD 'data/stock-2014-02-14.csv' USING PigStorage(',') AS (exchange, symbol, name, open, high, low, close, volumn); jnd = JOIN today BY (exchange, symbol) , lastday BY (exchange, symbol); DESCRIBE jnd; 输出的结构：\njnd: { today::exchange: bytearray, today::symbol: bytearray, today::name: bytearray,today::open: bytearray,today::high: bytearray,today::low: bytearray,today::close: bytearray,today::volumn: bytearray, lastday::exchange: bytearray,lastday::symbol: bytearray, lastday::name: bytearray, lastday::open: bytearray,lastday::high: bytearray,lastday::low: bytearray,lastday::close: bytearray,lastday::volumn: bytearray } 只有当字段名在记录中不再唯一的时候，会使用 :: 前缀。\nPig 同样支持外联（OUTER JOINS）。在外联接中，如果找不到匹配的一方会直接以 null 值来补字段。外联接包括：LEFT，RIGHT 和 FULL。它们的概念和 SQL 相同，这 里就不进行详细阐述：\nleft_join = JOIN input1 BY (fld11) LEFT OUTER, input2 BY (fld21); right_join = JOIN input1 BY (fld11) RIGHT OUTER, input2 BY (fld21); full_join = JOIN input1 BY (fld11) FULL OUTER, input2 BY (fld21); 注意，这里的 OUTER 是个干挠词，可以被忽略，它并不等同于 FULL OUTER，而且单独 使用将会被报错。\nPig 只有在了解联表双方的 Schema 的情况下才支持外联接，否则它将不知道如何补 null 字段。\nPig 和 SQL 一样支持多表联接：\nA = LOAD 'input1' AS (x, y); B = LOAD 'input2' AS (u, v); C = LOAD 'input3' AS (e, f); alpha = JOIN A BY x, B BY u, C BY e; Pig 自联接（Self Join）的实现是通过加载两次数据：\n--selfjoin.pig -- For each stock, find all dividends that increased between two dates divs1 = LOAD 'NYSE_dividends' AS (exchange:chararray, symbol:chararray, date:chararray, dividends); divs2 = LOAD 'NYSE_dividends' AS (exchange:chararray, symbol:chararray, date:chararray, dividends); jnd = JOIN divs1 BY symbol, divs2 BY symbol; increased = FILTER jnd BY divs1::date \u0026lt; divs2::date AND divs1::dividends \u0026lt; divs2::dividends; 而下面的实现是错的，而且会执行失败：\n--selfjoin.pig -- For each stock, find all dividends that increased between two dates divs1 = LOAD 'NYSE_dividends' AS (exchange:chararray, symbol:chararray, date:chararray, dividends); jnd = JOIN divs1 BY symbol, divs1 BY symbol; increased = FILTER jnd BY divs1::date \u0026lt; divs2::date AND divs1::dividends \u0026lt; divs2::dividends; 它看起来可以成功，因为 Pig 可以把数据分成两块，分别发送给 JOIN。但是，问题是 联表之后会出现字段冲突，因此必须 load 两次。而且，对于 Pig 来说，虽然代码上 载入了两次相同的数据，实际上它只载入的一次。\n分页（LIMIT） # 有时，你可能只希望看到某个数量的数据：\n--limit.pig divs = LOAD 'NYSE_dividends'; first10 = LIMIT divs 10; 样本（SAMPLE） # SAMPLE 提供了一种从你的数据中获取一份样本。它读取所有的数据，然后返回某一个 百分比的数据。这个值只能是 0 到 1 之间。\n--sample.pig divs = LOAD 'NYSE_dividends'; some = SAMPLE divs 0.1; -- 获取10%的数据 目前的样本算法非常简单，可以把它重写成：\nsome = FILTER divs BY RANDOM()\u0026lt;=0.1; 并行（PARALLEL） # Pig 的核心之一就是提供并行数据处理的语言。Pig 的核心价值观之一就是温驯的动物 ；因此，Pig 更愿意让你告诉它如何并行处理。为此，它提供了 PARALLEL 语句。\nPARALLEL 语句可以和 Pig Latin 中任何关系操作一同使用。然而，它控制 reduce 端的并行，因此它只能识别能强制 reduce 的操作；这些操作包括 GROUP, ORDRE, DISTINCT, LIMIT, COGROUP, 以及 CROSS。 PARALLEL 在本地模式的情况下被忽略， 因为本地模式的数据处理都是序列的。\n--parallel.pig daily = LOAD 'NYSE_daily' AS (exchange, symbol, date, open, high, low, close, volume, adj_close); bysymbl = GROUP daily BY symbol PARALLEL 10; 这个例子，PARALLEL 将被 PIG 把 MR 工作增加 10 个进程。它只能作用当前语句， 而不会影响其他语句。但是，它也提供了一个简单的方式，让它在整个代码中奏效：\n--defaultparallel.pig SET DEFAULT_PARALLEL 10; -- 在这个代码的所有操作都将有 10 个 reduce daily = LOAD 'NYSE_daily' AS (exchange, symbol, date, open, high, low, close, volume, adj_close); bysymbl = GROUP daily BY symbol; average = FOREACH bysymbl GENERATE group, AVG(daily.close) AS avg; sorted = ORDER average BY avg DESC; 如果你不指定 PARALLEL 层级，Pig 0.8 之前的的默认是由你的集群的设置。\n自定义函数（UDF） # Pig 的强大得益于它允许用用户通过 UDF 来定制他们的操作。0.7 之前，所有的 UDF 只允许使用 JAVA 来编写。当前的版本是 0.12，它还支持 JYTHON，JAVASCRIPT， RUBY 以及 GROOVY 来编写 UDF，详情请参见 User Defined Functions。\n还需要特别介绍的就是 Piggybank，用户贡献的 UDF 集，它和 Pig 一同打包和发行 。但是它不包含在 pig.jar 中；因此，如果你要使用，需要登记它。\n当然你也可以编写，或者使用别人开发的 UDF。\n登记 UDF # 当你的 UDF 不包含在你的 Pig 中的时候，你需要告诉 Pig 从哪里寻找这些函数。\n载入 JAVA 编写的 UDF：\nREGISTER 'your_path_to_piggybank/piggybank.jar'; 载入 JYTHON 编写的 UDF：\nREGISTER 'xx.py' USING jython as xx; 定义（DEFINE）和 UDF # DEFINE 可以用来给你的 JAVA 包名建立别名来方便调用；它还可以提供流命令的定义 ，但是不是本章要涵盖的内容。下面是例子：\nREGISTER 'your_path_to_piggybank/piggybank.jar'; DEFINE reverse org.apache.pig.piggybank.evaluation.string.Reverse(); divs = LOAD 'NYSE_dividends' AS (exchange:chararray, symbol:chararray, date:chararray, dividends:float); backwards = FOREACH divs GENERATE reverse(symbol); 求值或者筛选函数可能需要带多个参数。如果你用在 UDF，DEFINE 就是提供这些参数 的地方。例如：\n--define_constructor_args.pig REGISTER 'acme.jar'; DEFINE convert com.acme.financial.CurrencyConverter('dollar', 'euro'); divs = LOAD 'NYSE_dividends' AS (exchange:chararray, symbol:chararray, date:chararray, dividends:float); backwards = FOREACH divs GENERATE convert(dividends); Enjoy Pig! # ","date":"2014-02-18","permalink":"/n3xtchen/2014/02/18/hadoop-data-pig-programming/","section":"时间线","summary":"通过三个章节的铺垫，现在开始深入了解 Pig 专属语言的 Pig Latin； 再次强调下， Pig Latin 是一门数据流语言（dataflow language）。","title":"Data Pig - Pig Latin 基础"},{"content":"Apache HCatalog 是基于 Hadoop 之上的数据表和存储管理服务，让用户可以使用不同 的工具来（例如，Pig，MR 和 Hive）读写网格的数据。HCatalog 是以 HDFS 的数据关 系视图的形式来呈现给用户的，确保用户不需要关心数据的存储形式和存储方式。 HCatalog 可以以RCFile，文本文件或者制表符式的序列文件。它也提供 Rest 接口让 外部系统访问这些表的元数据。\nHCatalog 的功能 # 利用表抽象，解放用户，使它们不用必须了解数据的存储位置和形式 可以提示数据可用性 提供可视化数据清洗和压缩工具 HCatalog 的工作原理 # HCatalog 支持任何 Hive SerDe（serializer-descerializer，序列和反序列）能处理 的形式的文件读写。默认，HCatalog 支持 RCFile，CSV，JSON 和序列文件格式。如果 你想要自定义格式，你需要提供输出格式，输出格式以及序列和反序列的方法。\nHCatalog 是尽力在 Hive 源数据智商的，可以使用 Hive DDL 的组件。HCatalog 提 供读写接口给 Pig ，MR 使用，使用 Hive 的命令行接口来执行数据定义和元数据探索 命令。它也提供了 Rest 接口来允许外部工具访问 Hive DDL 操作，例如，CREATE TABLE 和 DESCRIBE TABLE。\nHCatalog 展现了数据的关系视图。数据存储在表中，这些表存在数据库中。表可以以 一个或多个键进行分区。对于一个给予的键，将使用一个分区来存储包含一个共同键值 的所有行数据。\nRCFile 是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分，再垂直 划分”的设计理念。当查询过程中，针对它并不关心的列时，它会在IO上跳过这些列。\n","date":"2014-02-17","permalink":"/n3xtchen/2014/02/17/hadoop-ecosystem---hcatalog/","section":"时间线","summary":"Apache HCatalog 是基于 Hadoop 之上的数据表和存储管理服务，让用户可以使用不同 的工具来（例如，Pig，MR 和 Hive）读写网格的数据。HCatalog 是以 HDFS 的数据关 系视图的形式来呈现给用户的，确保用户不需要关心数据的存储形式和存储方式。 HCatalog 可以以RCFile，文本文件或者制表符式的序列文件。它也提供 Rest 接口让 外部系统访问这些表的元数据。","title":"Hadoop 生态系统 - HCatalog"},{"content":"","date":"2014-02-17","permalink":"/n3xtchen/tags/hcat/","section":"标签","summary":"","title":"hcat"},{"content":"","date":"2014-02-17","permalink":"/n3xtchen/tags/hive/","section":"标签","summary":"","title":"hive"},{"content":"","date":"2014-02-16","permalink":"/n3xtchen/tags/oozie/","section":"标签","summary":"","title":"oozie"},{"content":" 来源： Oracle - Build Simple Workflows in Oozie\n简介 # 寻常情况下，数据并不能按我们希望的方式被打包。我们从大数据源中抽取洞见（ Insight）之前，我们需要对其进行转换（Transformation），match-merge 操作， 数据清洗和整合（data munging）任务。数据清洗和整合对于大部分人人说是很枯燥， 但是又必须做；这种情况，创建自动化处理来解放我们工作。我们把这些工作转换成 重复运行的单元，并创建工作流。在这篇文章中，我们使用 Oozie 来为并行机器学习 任务创建工作流。\nHive 动作：先为 Pig 预热 # 开始我们的工作流 # Oozie 提供两种方式的工作：工作流和协调工作。工作流很简单：他们定义动作集来 以有向无环（DAG）的方式运行它。协调器工作可以携带工作流中所有相同的动作，并 且他们可以周期性的或者数据到达指定路径的时候被自动触发。为了简化它，我们创建 一个工作流工作；协调工作需要另一个 XML 文件来计划。定义工作流 XML：\n\u0026lt;!-- simpleWorkFlow,xml --\u0026gt; \u0026lt;workflow-app nam=\u0026quot;myApp\u0026quot; xmlns=\u0026quot;uri:oozie:workflow:0.1\u0026quot;\u0026gt; \u0026lt;start to=\u0026quot;startAct\u0026quot; /\u0026gt; \u0026lt;end name=\u0026quot;end\u0026quot; /\u0026gt; \u0026lt;/workflow-app\u0026gt; 现在我们需要添加动作，其中也包括我们指定的 Hive 参数。记住，定义动作的时候， 我们要求指定 和 标签来定位成功或失败后执行的动作。\n\u0026lt;action name=\u0026quot;ParseNCDCData\u0026quot;\u0026gt; \u0026lt;hive xmlns=\u0026quot;uri:oozie:hive-action:0.2\u0026quot;\u0026gt; \u0026lt;job-tracker\u0026gt;localhost:8021\u0026lt;/job-tracker\u0026gt; \u0026lt;name-node\u0026gt;localhost:8020\u0026lt;/name-node\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;oozie.hive.defaults\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/user/n3xtchen/17173_ooze/hive-default.xml\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;script\u0026gt;parse.hql\u0026lt;/script\u0026gt; \u0026lt;/hive\u0026gt; \u0026lt;ok to=\u0026quot;WeatherMan\u0026quot;/\u0026gt; \u0026lt;error to=\u0026quot;end\u0026quot;/\u0026gt; \u0026lt;/action\u0026gt; 这里有一些东西需要注意：\n必须指定工作追踪器（jobTracker）和名称节点（nameNode） 必须包含 hive-default.xml 文件 必须包含脚本文件（.hql） hive-default.xml 和 脚本文件必须存储在 HDFS 文件系统中 最后一点特别重要。Oozie 不会对工作流的存储位置作为假设。你可能要把工作提交到 不同的集群，或者在不同的集群设置不同的 hive-defaults.xml 文件。\n最快速的确认这些文件是否存放在正确地方的方法就是，在本地创建工作目录，为它 创建 workflow.xml 文件，复制到你需要的地方。这是，我们的本地目录应该包含：\nworkflow.xml hive-defaults.xml parse.sql 添加 Pig 到 Oozie 中 # 添加 Pig 动作到 XML 中会稍微简单电。所有我们所要做的就是像下面这样：\n\u0026lt;action name=\u0026quot;CalData\u0026quot;\u0026gt; \u0026lt;pig\u0026gt; \u0026lt;job-tracker\u0026gt;localhost:8021\u0026lt;/job-tracker\u0026gt; \u0026lt;name-node\u0026gt;localhost:8020\u0026lt;/name-node\u0026gt; \u0026lt;script\u0026gt;calData.pig\u0026lt;/script\u0026gt; \u0026lt;/pig\u0026gt; \u0026lt;ok to=\u0026quot;end\u0026quot;/\u0026gt; \u0026lt;error to=\u0026quot;end\u0026quot;/\u0026gt; \u0026lt;/action\u0026gt; 一旦我们添加完成，要将我们的 pig 脚本（calData.pig）上传到我们的工作目录中。\n虽然 Oozie 不会做出太多推断，但是它会对 Pig 的 classpath 做出假设。任何在 working_directory/lib 中的库都会自动被加载，而不需要 Reqister 语句来加载。 任何需要使用 Reqister 登记的库都不能存放在 working_directory/lib 中。如果 存放在不同的 HDFS 目录的化，你可以附加上 标签来指定。\n是的，这个非常让人费解。\n你可以在 Pig Cookbook 中获取相关的完整文档。\n让你的 Workflow 工作 # 你已经定义了你的工作流和收集所有我们需要的的组件。但是在执行它之前，我们还需 要定义工作的一些属性，并把它上传到 Oozie 服务器。我们首先需要先从工作属性开 始，它的本质就是我们提交给 Ooize 服务的请求。在工作目录下，我们将创建叫做 job.properties 的文件：\nnameNode=hdfs://localhost:8020 jobTracker=localhost:8021 queueName=default workRoot=work_zone mapreduce.jobtracker.kerberos.principal=foo dfs.namenode.kerberos.principal=foo oozie.libpath=$(nameNode)/user/share/lib oozie.wf.application.path=${nameNode}/user/$(user.name)/$(workRoot) outputDir=work=work-ooze 这个配置中的一些我们已经很熟悉了（例如，JobTracker address），我就介绍下剩下 的部分：\nworkRoot：这是关键的脚本环境变量。我们使用它来简化 Oozie 的工作命令。 oozie.libpath：这个至关重要。它是 HDFS 中的文件夹，它包含了 Oozie 的共享库 ：执行 Hive，Pig 和其他动作的必要 Jar 包的集合。有一个好方法来确认它们是否被 正确安装并拷贝到 HDFS 上：在应用目录下运行 workflow.xml，并输出到输出目录中。 我们完成提交工作准备了。最后我们还需要做一些事情：\n验证我们的 workflow.xml：\noozie validate workflow.xml\n拷贝我们的工作目录到 HDFS 上：\nhadoop fs -put working_dir /user/n3xtchen/work_dir\n提交我们的工作给 Oozie 服务器。我们需要确认 Oozie 的服务器网址死正确的， 以及已经在 job.properties 中指定好了：\noozie job \u0026ndash;oozie http://url.to.oozie.server:port_number/ -config /path/to/working_dir/job.properties -submit\n我们一定提交了工作，但是我们并没看到在 JobTracker 中任何活动？我们所得到的输 出：\n14-20120525161321-oozie-n3xtchen 这是因为提交给 Oozie 的工作创建了一个工作实体（entry），并把它替换成 PREP 状 态。我们需要为我们的工作流获取乘坐 Oozie 动车的车票。我们负责兑现我们的票以 及执行我们的工作。\noozie --oozie http://url.to.oozie.server:port_number/ -start 14-20120525161321-oozie-n3xtchen 当然，如果我们需要从设置起就开始执行我们的工作，我们只需要改变上面的命令参数 -submit 为 -run 即可。他就会马上开始预备，马上执行工作流。\n运行我们的工作流 题外话 # 从这里，你所获得的是：创建工作流相对比较艰巨的步骤。第一次可能会有点单调乏味 ，然而对于那些花大量时间在数据清洗和整合（data munging）的开发者来说是非常有 价值的。首先，当新数据到达并请求相同的处理时，我们已经有定义好的工作流，并准 备执行。其次，由于我们不断的创建有用的动作，使得创建新的工作流变得更迅速。\n","date":"2014-02-16","permalink":"/n3xtchen/2014/02/16/using-hadoop---oozie/","section":"时间线","summary":"来源： Oracle - Build Simple Workflows in Oozie","title":"Using Hadoop - 创建简单的 Oozie 工作流"},{"content":"","date":"2014-02-14","permalink":"/n3xtchen/tags/data/","section":"标签","summary":"","title":"data"},{"content":"Apache Ambari 提供了 Hadoop 集群的配置，管理和监控，100%的开源框架；它包括操作 工具的集合，以及强大的 API 接口来隐藏 Hadoop 的复杂度，简化集群操作。\nAmbari 的功能 # 配置 Hadoop 集群 无论你的集群规模多大，服务器的部署和维护都可以使用 Ambari 简化。Ambari 提供直观的 Web 界面，使你能更加简单的对 Hadoop 所有服务和核心组件 进行配置 和测试\n管理 Hadoop 集群 Ambari 提供简化集群管理的工具。Web 界面允许你启动/停止/测试 Hadoop 服务， 改变配置和管理集群的持续的增长\n监控 Hadoop 集群 深度了解集群的健康状态。Ambari 预先配置监控 Hadoop 服务的警告，以及使用简单 WEB 界面对集群操作数据可视化展示\n整合 Hadoop 到其他应用 Ambari 提供 Rest 接口使得它能和现有工具整合，例如 MS Center，Teradata Viewpiont。 Ambari 还可以利用标准的技术和协议，例如 Nagios 和 Ganglia，来进行深度定制。\nAmbari 的工作原理 # Hadoop 集群的部署和持续的管理可能是一个非常复杂的任务，尤其是拥有成千上万台的 主机。Ambari 提供一种简单的控制，来查看，更新和管理 Hadoop 服务生命周期； 只要特征如下：\n向导式跨服务器 Hadoop 服务的安装 对 Hadoop 服务和组件进行粗力度配置 使用 Ganglia 进行指标收集，Nagios 进行系统提醒 高级的工作诊断和故障检修工具 强大的 Rest 接口来定制和整合企业系统 集群热图 ","date":"2014-02-14","permalink":"/n3xtchen/2014/02/14/hadoop-ecosystem---ambari/","section":"时间线","summary":"Apache Ambari 提供了 Hadoop 集群的配置，管理和监控，100%的开源框架；它包括操作 工具的集合，以及强大的 API 接口来隐藏 Hadoop 的复杂度，简化集群操作。","title":"Hadoop 生态系统 - Ambari"},{"content":"Apache Falcon 是 Hadoop 的数据管理框架和管道处理框架。它能自动采集（ingest）， 管道（pipelines），灾难恢复和数据保留用例。用户可以依赖 Falcon 来取代复杂的 数据和管道处理的硬编码，它可以对这些函数复用最大，以及保证 Hadoop 的跨应用 之间的数据一致性。\nFalcon 的特点 # Falcon 通过引入用户处理的高层次抽象，简化了数据处理管道的开发和管理。它提供 通用的开箱即用的数据管理服务，提取数据处理应用中的复杂代码，简化数据移动的 配置和编排，灾难恢复以及数据保留工作流。如下是 falcon 的关键特性：\n数据复制处理：Falcon 为了灾难恢复和多集群数据的数据挖掘需要在不同集群之间 同步复制 HDFS 文件和 Hive 表。 数据生命周期管理：Falcon 管理数据驱逐（eviction）政策。 数据沿袭（Lineage）和可追溯幸：Falcon E-R 关系使用户可以查看粗粒度数据沿袭 流程协调和调度：Falcon 自动管理后来的数据处理和重试的复杂逻辑 声明式数据处理编程：Falcon 引入高级数据抽象（集群，Feeds 和处理）把业务 逻辑从应用逻辑中抽出，在建立处理管道时最大化复用和保持数据的一致性 均衡存在的 Hadoop 服务：Falcon 使用现有的 Hadoop 服务（例如 Oozie）能 更透明协调和调度数据工作流 数据沿袭（Lineage）：由数据转换服务 (DTS) 与 Meta Data Services 一起使用 的信息，记录每块数据的包执行和数据转换的历史。)\nFalcon 工作原理 # Falcon 作为 Hadoop 集群的一部分，以独立的服务运行。\n![struct]( http://hortonworks.com/wp-content/uploads/2013/09/falcon-architecture-1024x645.png =491x310)\n用户使用命令行或 Rest 接口创建实例特征（entity specifications）并提交给 Falcon。Falcon 通过 Hadoop 工作流调度器将实例特征转化成重复的动作。 所有的函数和工作状态管理需求授权给调度器。默认，Falcon 使用 Oozie 作为 调度器。\n![workflow]( http://hortonworks.com/wp-content/uploads/2013/09/falcon-userflow-1024x533.png =491x256)\n实体（entities） # ![entities]( http://hortonworks.com/wp-content/uploads/2013/09/falcon-entities-1024x487.png =491x234)\n集群（Cluster）：呈现接口给 Hadoop 集群 Feed：定义数据集（例如 HDFS 文件 或者 Hive 表）的 location，复制调度和 保留方案 进程：消费 Feed 和处理 Feeds Falcon In Action: 同步 # 所有的企业对话在某点上都涉及数据同步。从简单的 “我要多份数据副本”向 “我需要 阶段的，中间结果和展示结果在不同的集群之间同步，并且具有失效备援（failover） 的功能（即当某台机子出现故障了，有另一台机子接手原失效系统的动作）\u0026hellip; 并且 每一个数据集都有不同的保留时间。“\n典型的例子就是解决定制的应用的问题，它是耗时的，易错的（error-prone）和面临 长期维护的挑战。使用 Falcon，你可以避免定制代码的问题，是用简单的声明语言来 表达处理管道和同步机制。\n在下面场景中，阶段数据通过一系列处理提供给 BI 使用。用户希望同步数据到第二个 集群（防止集群停工的时候，用来失效备援）。但是第二个集群的容量要比第一集群小 很多，因此只能数据被同步。\nFalcon 来拯救你。你只要使用 Falcon 定义数据集和处理流程，Falcon 将在指定的时间 同步数据到第二集群中。Falcon 将编排处理和调度同步时间。结果：在第一集群出现问题 被迫下线的时候，关键的阶段和呈现数据已经存储在第二集群上了。\n![Alt text]( http://hortonworks.com/wp-content/uploads/2013/09/falcon-example-replication.png =491x321)\n","date":"2014-02-14","permalink":"/n3xtchen/2014/02/14/hadoop-ecosystem---falcon/","section":"时间线","summary":"Apache Falcon 是 Hadoop 的数据管理框架和管道处理框架。它能自动采集（ingest）， 管道（pipelines），灾难恢复和数据保留用例。用户可以依赖 Falcon 来取代复杂的 数据和管道处理的硬编码，它可以对这些函数复用最大，以及保证 Hadoop 的跨应用 之间的数据一致性。","title":"Hadoop 生态系统 - falcon"},{"content":"Apache Oozie 是一个用来调度 Hadoop 工作的 Java 网页应用。Ozzie 可以将多个工作 组装成一个工作逻辑单位。它被整合到 Hadoop 栈中，可以用来创建 MapReduce，Pig， Hive 和 Apache Sqoop 的工作。它还可以用来调度其他系统的工作，例如 Java 程序以 及 Shell 脚本。\nOzzie 的工作可以分为两种：\n**Ozzie 工作流（Workflow）**是 DAG（Directed Acyclical Graphs，有向无环图，有向 图中一个点经过两种路线到达另一个点未必形成环，因此有向无环图未必能转化成树， 但任何有向树均为有向无环图。），用来指定执行动作的队列。工作流工作必须等待 Ozzie 协调器（Coordinator） 是一个周期性的 Oozie 工作流工作，它绑定了时间 和可利用的数据 Oozie Bundle 提供一种可以打包多个协调器和工作流的方法来管理这些工作的生命 周期 Ozzie 的用途 # Oozie 允许 Hadoop 管理员将复杂的数据转化分割成多个组件任务。这需要对复杂工作 进行更有力的控制，也要简化在预定间隔时间内重复的工作。\nOozie 帮助管理员从 Hadoop 投资中得到更多的回报。\nOzzie 的工作原理 # Oozie 工作流是一个有向无环形式动作的集合。控制节点定义工作排气，设定启动和结束工作 流的规则，可以通过决定，分叉以及加入节点来控制工作执行路径。动作节点出发任务的执行。\nOozie 绑定工作流动作，由 Hadoop MapReduce 执行他们。它允许 Oozie 均衡 Hadoop 其他 栈的能力，可以平衡负载，处理失败。\nOozie 可以通过回调和计票的方法来探测任务的完成。当 Oozie 启动一个任务时，它提供一个 唯一的回调 HTTP URL 给任务，因此当完成的时候会通知这个 URL 。如果任务在调用回调 URL 失败的话，Oozie 也轮询完成的任务。\n在规定的时间间隔内运行 Oozie 工作流是很有必要的，但是需要协调不可预期的可用数据或 事件。在这种情况下，Oozie 协调器允许你以数据，事件或事件推断的形式对工作流执行触发器 模型化。工作流只会在这些断言满足的情况下启动。\nOozie 协调器还可以根据子工作流的输出来管理多个工作流。子工作流的输出成为下一个子工作 流的输入。这个链条称作为数据应用管道（data application pipline）。\n","date":"2014-02-14","permalink":"/n3xtchen/2014/02/14/hadoop-ecosystem---oozie/","section":"时间线","summary":"Apache Oozie 是一个用来调度 Hadoop 工作的 Java 网页应用。Ozzie 可以将多个工作 组装成一个工作逻辑单位。它被整合到 Hadoop 栈中，可以用来创建 MapReduce，Pig， Hive 和 Apache Sqoop 的工作。它还可以用来调度其他系统的工作，例如 Java 程序以 及 Shell 脚本。","title":"Hadoop 生态系统 - Oozie"},{"content":"Apache ZooKeeper 为 Hadoop 集群提供操作服务。它提供了一个分布配置服务，一个同步 服务和一个命名注册表。分布式程序是使用 ZooKeeper 存储和调节重要配置信息的更新。\nZooKeeper 的功能 # ZooKeeper 提供一个非常简单的接口和服务。关键功能如下：\n快速：ZooKeeper 在读多谢少的工作场景下异常快。理想的读写比率是 10:1 可靠：ZooKeeper 在一个服务器主机集群间同步，服务器可以相互识别。只要指定数量 的服务器可用，ZooKeeper 服务就可用。不会有单点失败。 简单：ZooKeeper 维护标准层级的名称空间，类似于文件和目录 有序：服务维护所有交易的记录，它可以用于高级抽象，例如同步通信原语（synchronization primitives） 同步通信原语：当一个进程调用一个send原语时，在消息开始发送后，发送进程便 处于阻塞状态，直至消息完全发送完毕，send原语的后继语句才能继续执行。\nZooKeeper 的工作原理 # ZooKeeper 允许通过一个数据寄存器的共享层次名称空间，在分布式进程之间相互协调， 被成为 znodes。每一个 znode 根据路径辨别（路径使用斜杆 “/\u0026quot; 分割）。除了根目录， 每一个 znode 有一个父节点，如果它有子节点则不能删除。\n它非常像普通的文件系统，但是 ZooKeeper 通过冗余服务提供卓越的可靠。一个服务在多台 机子间同步，各自维护一分数据树的内存镜像和交易日志。客户端连接单独的 ZooKeeper 服务器和保持 TCP 连接。\n这样的架构允许 ZooKeeper 提供最低冗余的高穿透力和可用性，但是 ZooKeeper 能管理 的数据大小收到内存的限制。\n","date":"2014-02-14","permalink":"/n3xtchen/2014/02/14/hadoop-ecosystem---zookeeper/","section":"时间线","summary":"Apache ZooKeeper 为 Hadoop 集群提供操作服务。它提供了一个分布配置服务，一个同步 服务和一个命名注册表。分布式程序是使用 ZooKeeper 存储和调节重要配置信息的更新。","title":"Hadoop 生态系统 - ZooKeeper"},{"content":"","date":"2014-02-14","permalink":"/n3xtchen/tags/installation/","section":"标签","summary":"","title":"installation"},{"content":"","date":"2014-02-14","permalink":"/n3xtchen/tags/workflow/","section":"标签","summary":"","title":"workflow"},{"content":"","date":"2014-02-13","permalink":"/n3xtchen/tags/auth/","section":"标签","summary":"","title":"auth"},{"content":"Know Gateway（简称 knox）是为 Hadoop 集群提供单点验证和访问。这个项目的目的是 为访问集群数据数据和执行工作（jobs）的用户，和控制和管理集群的运营商简化了 Hadoop 安全。Knox 作为服务运行，并为整个 Hadoop 集群服务。\nKnox 的优势 # 提供外围（perimeter）安全使得 Hadoop 安全更容易部署 支持身份验证和令牌（token）验证方案 支持单一集群端点访问不同集群的数据和工作（job） 支持企业和云验证管理环境的整合 支持跨集群跨版本安全管理 Know 的工作原理 # Knox 只在提供外围安全，使其容易集成到现有的安全机制中。为 Hadoop 生态提供 安全是一个关键的社区项目。为了更有效以及成为社区的一部分，Knox 需要更紧密地 融合到 Hadoop 中。\n目前，Hadoop 集群是作为独立服务的松散集合的形式呈现给用户的。由于每一个服务都 有各自的访问方法和安全控制，使得对用户访问 Hadoop 造成困难。Hadoop 的配置和 管理是很复杂的；因此很多 Hadoop 管理员被迫使不得不减慢 Hadoop 部署的进程或者 直接放弃安全了。\n这个项目的目的就是为了覆盖当前所有存在的 Hadoop 生态相关的项目。另外，这个项目 的拓展性足以为（不改动源码的前提下）兼容未来的 Hadoop 开发组件。Knox 期望运行 在 DMZ 环境（无安全机制），这样它可以控制多个 Hadoop 服务的访问。这样，Hadoop 就可以在带有访问控制的防火墙（firewall）中安全运行了。安全认证组件将会是模块化 以及可拓展的，容易集成到现有的安全机制中。\n","date":"2014-02-13","permalink":"/n3xtchen/2014/02/13/hadoop-ecosystem---knox-gateway/","section":"时间线","summary":"Know Gateway（简称 knox）是为 Hadoop 集群提供单点验证和访问。这个项目的目的是 为访问集群数据数据和执行工作（jobs）的用户，和控制和管理集群的运营商简化了 Hadoop 安全。Knox 作为服务运行，并为整个 Hadoop 集群服务。","title":"Hadoop 生态系统 - Knox Gateway"},{"content":"","date":"2014-02-13","permalink":"/n3xtchen/tags/secure/","section":"标签","summary":"","title":"secure"},{"content":"","date":"2014-01-26","permalink":"/n3xtchen/tags/bower/","section":"标签","summary":"","title":"bower"},{"content":"","date":"2014-01-26","permalink":"/n3xtchen/tags/grunt/","section":"标签","summary":"","title":"grunt"},{"content":" Yeoman 是什么？ # 这个问题问的很好；它不是东西，下面这位就是 Yeoman;\n$ yo webapp _-----_ | | |--(o)--| .--------------------------. `---------´ | Welcome to Yeoman, | ( _´U`_ ) | ladies and gentlemen! | /___A___\\ '__________________________' | ~ | __'.___.'__ ´ ` |° ´ Y ` Out of the box I include HTML5 Boilerplate, jQuery, and a Gruntfile.js to build your app. [?] What more would you like? (Press \u0026lt;space\u0026gt; to select) ⬢ Sass with Compass ❯⬢ Bootstrap ⬢ Modernizr create Gruntfile.js create package.json ... I'm all done. Running bower install \u0026amp; npm install for you to install the required dependencies. If this fails, try running the command yourself. ... 正如大家所看到的，他戴着一顶高帽，住在你的电脑里，并且等待你告诉他你要创建什 么样的应用。举一个简单的例子，像下面这样创建一个 Web 应用：\n会不会觉得很神奇啊？\n我们所要所的就是告诉他你要什么？感觉想在吃西餐的感觉，有木有？召唤 Yeoman， Yeoman 提供菜单，然后我们选择，最后就等着“上菜”就好了。\n无厘头之后，接下来正式向大家推荐 Yeoman。\nYeoman 三兄弟 # 我们的工作流使用三个工具来改善你的需求，提高开发的满足感。他们分别是 yo （脚手架工具），grunt（构建工具），bower（包管理工具）。\nyo # yo 是跨平台的命令，根据你指定的模版生成一个新的应用，编写 Grunt 配置，推送 相关的任务以及 Bower 依赖的建立。\n$ yo webapp 生成器 # 实际上，webapp 是一个独立的插件（众多生成器的一种）。Yeoman 可以识别其他的 generator-___ 的Node 模块，例如 Backbone，Angular，以及无数你命名的生成器。\nGrunt # Grunt 是 一个基于 JS 的任务运行器。和 yo 一样，它也提供基础的函数接口，允许 社区为它开发插件（或任务）来帮忙完成共同的事情。当你使用 yo webapp 的时候 ，Grunt 和一些附带的任务将会一起被被安装。\n$ grunt [task_name] task_name 就是你在 Gruntfile.js 中定义的任务，如果你使用 web_app 的模版，那 你默认就会添加 sever（在开发环境运行你的站点），build（组合和压缩代码）以及 test（执行 QUnit 测试）这些目录；如果你不指定任务名，那 grunt 就会执行 Gruntfile.js 中的所有任务。\n具体介绍请见 [Javascript Grunt - 快速入门]({% post_url 2014-01-22-javascript-grunt-starter %})\nBower # 没人喜欢去 Github 或者开发网站上下载一个 Javascript 工具的压缩包把。就和 npm 一样，通过简单的命令 bower 就能帮你把想要的 Javascript 库下到你机子上。\n$ bower install jquery Bower 就会把 jquery 库下载到你的项目目录中的 bower_components 中。\n具体介绍请见 [Javascript Bower - 快速入门]({% post_url 2014-01-16-javascript-bower-starter %})\nYeoman 安装 # Yeoman 并不是默认预装在你的系统中 。你需要通过 NPM 安装他；首先要确认你已经 安装了 Node.js，Git 以及可选的 Ruby 和Compass（如果你打算使用的话）。\n$ npm install -g yo 如果你没有安装 Grunt 和 Bower，它会自动为你安装。\n这是我当前使用的 node 版本\n$ node -v v0.10.24 $ yo -v 1.1.2 $ grunt --version grunt-cli v0.1.11 $ bower -v 1.2.8 你可以使用 yo 生成多种应用，但是前提是你需要安装相应插件来完成它。例如，之 前使用 webapp，你需要事先安装它：\n$ npm install -g generator-webapp 这样才可以使用 yo webapp 来生成你的应用。你可以通过 npm 来安装其他生成器 ；例如，安装 Backbone 生成器：\n$ npm install -g generator-backbone 你可以通过调用 yo 帮助命令查看你的安装的安装器：\n$ yo -h Usage: yo GENERATOR [args] [options] ... Please choose a generator below. Backbone ... backbone:app ... Mocha mocha:app Webapp webapp:app mocha 是默认安装的，我除了安装了 webapp 之外还安装了 backbone 生成器。\n一切都准备好了，可以开始享受你的 yeoman！\n一个标准的应用 # 前提，你已经安装了 webapp 的生成器。\n生成应用：\n$ yo webapp 查看构建的目录：\n. ├── Gruntfile.js # GRUNT 认为配置文件 ├── app # 存放你未编译和压缩的源代码 │ ├── 404.html │ ├── bower_components │ ├── favicon.ico │ ├── images │ ├── index.html # 未压缩的主页 │ ├── robots.txt │ ├── scripts # 存放 JS 脚本 │ └── styles # 存放 CSS 文件 ├── bower.json # bower 依赖库配置文件 ├── node_modules │ ├── grunt │ ├── grunt-autoprefixer │ └── ... ├── package.json # Npm 依赖库配置文件 └── test # 测试目录 ├── bower.json ├── bower_components ├── index.html └── spec 现在看看 yeoman 为你预先定义的 Grunt 任务：\nbuild：将 app 中的代码进行编译，合并和压缩，并输出到 dist 目录中； server：在浏览器中运行你的代码； test：测试你的代码，webapp 使用的默认测试框架是 mocha； 实际上，每一个任务都是有多个自任务构成的；例如， build 需要 concat（合并代 码），cssmin（压缩 css 文件），uglify（压缩 js 脚本，copy:dist（优化后的代 码拷贝到dist目录中）等等多个子任务来帮助它完成这个目标。\n结语 # 上述对 Yeoman 进行了简单的介绍，让大家对它有一个初步的认识，后续我将会使用 例子来帮助大家更快上手\nEnjoy It！\n","date":"2014-01-26","permalink":"/n3xtchen/2014/01/26/javascript-yeoman-starter/","section":"时间线","summary":"Yeoman 是什么？ # 这个问题问的很好；它不是东西，下面这位就是 Yeoman;","title":"Javascript Yeoman - 快速入门"},{"content":"","date":"2014-01-26","permalink":"/n3xtchen/tags/yo/","section":"标签","summary":"","title":"yo"},{"content":"如果你正在进行一个 JS 项目，你可能会有一堆的日常工作要做。那有哪些需要做呢？\n合并代码 执行 JSHint 验收你的代码 执行测试 压缩代码 当然你还有很多备选的工具来完成这些，当时如果能够使用同一的工具完成上述的工作， 你应该不会拒绝把？\n这就是 Grunt 存在的价值；它拥有一堆的内建任务来完成这些，你还可以创建自己的插件 来拓展基本功能。\n安装 Grunt # $ npm install -g grunt-cli 这是我当前使用的 node 版本\n$ node -v v0.10.24 $ grunt --version grunt-cli v0.1.11 grunt v0.4.2 准备工作（项目脚手架） # 标准的设置需要在你的项目目录中添加两个文件：package.json（管理 npm 库）和 Gruntfile（文件名为 Gruntfile.js，用来配置和定义 grunt 任务）。\nPackage.json 的格式大致如下：\n{ \u0026quot;name\u0026quot;: \u0026quot;my-project-name\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.1.0\u0026quot;, \u0026quot;devDependencies\u0026quot;: { \u0026quot;grunt\u0026quot;: \u0026quot;~0.4.2\u0026quot;, \u0026quot;grunt-contrib-jshint\u0026quot;: \u0026quot;~0.6.3\u0026quot;, \u0026quot;grunt-contrib-nodeunit\u0026quot;: \u0026quot;~0.2.0\u0026quot;, \u0026quot;grunt-contrib-uglify\u0026quot;: \u0026quot;~0.2.2\u0026quot; } } devDependencies 用来指定要依赖的 npm 库；你可以使用 npm init 创建。\nGruntfile.js 的范例：\nmodule.exports = function(grunt) { // Project configuration. grunt.initConfig({ pkg: grunt.file.readJSON('package.json'), uglify: { options: { banner: '/*! \u0026lt;%= pkg.name %\u0026gt; \u0026lt;%= grunt.template.today(\u0026quot;yyyy-mm-dd\u0026quot;) %\u0026gt; */\\n' }, build: { src: 'src/\u0026lt;%= pkg.name %\u0026gt;.js', dest: 'build/\u0026lt;%= pkg.name %\u0026gt;.min.js' } } }); // Load the plugin that provides the \u0026quot;uglify\u0026quot; task. grunt.loadNpmTasks('grunt-contrib-uglify'); // Default task(s). grunt.registerTask('default', ['uglify']); }; 在号称 JS 最强大项目自动化管理工具的 Grunt 面前，如果你还使用手动创建， 那就弱爆了。\n首先，安装项目脚手架（Scaffolding）工具：\n$ npm install -g grunt-init $ grunt-init --version grunt-init v0.2.1 grunt v0.4.2 然后，安装模版；模版的默认目录是在~/.grunt-init/中（windows 是在 %USERPROFILE%\\.grunt-init\\ ）。例如，安装 grunt-init-jquery 模版：\n$ git clone https://github.com/gruntjs/grunt-init-jquery.git ~/.grunt-init/jquery 最后，使用模版初始化项目：\n$ grunt-init TEMPLATE # 默认在 ~/.grunt-init/ 目录下查找 $ grunt-init /path/to/TEMPLATE # 也可以指定路径 下面是官方的 grunt-init 的模版：\ngrunt-init-commonjs：创建 common 模块，包括 Nodeunit 单元测试； grunt-init-gruntfile：基本的 Gruntfile 模版； grunt-init-gruntplugin：创建 grunt 测试，包括 Nodeunit 单元测试； grunt-init-jquery：创建 jquery 插件，包含 QUnit 单元； grunt-init-node：创建 node 模块，包括 Nodeunit 单元测试； Note，此教程默认上述模版已经安装\n$ grunt-init -h ... Available templates commonjs Create a commonjs module, including Nodeunit unit tests. gruntfile Create a basic Gruntfile. gruntplugin Create a Grunt plugin, including Nodeunit unit tests. jquery Create a jQuery plugin, including QUnit unit tests. node Create a Node.js module, including Nodeunit unit tests. ... 开始你的 Grunt 之旅 # 正如你所知的，grunt 是命令行工具；因此，这个教程假设你使用命令工具进行接下来的学习；\n创建项目目录 # 首先创建一个项目样本目录，grunt-tut，进入该目录：\n$ cd /path/to/grunt-tut 初始化项目 # grunt-init 需要你填写初始化项目需要的一些信息，同时它也提供默认值（即括号 中的值）。现在只是演示，所有一路 enter 就好\n$ grunt-init jquery ... Please answer the following: [?] Project name (grunt-tut) [?] Project title (Grunt Tut) [?] Description (The best jQuery plugin ever.) [?] Version (0.1.0) [?] Project git repository (git://github.com/xxxx/grunt-tut.git) [?] Project homepage (https://github.com/xxxx/grunt-tut) [?] Project issues tracker (https://github.com/xxxx/grunt-tut/issues) [?] Licenses (MIT) [?] Author name (n3xtchen) [?] Author email (xxx@example.com) [?] Author url (none) [?] Required jQuery version (*) [?] Do you need to make any changes to the above before continuing? (y/N) N ... Initialized from template \u0026quot;jquery\u0026quot;. $ npm install 初始化成功后，你将会看到 grunt 创建项目的基本文件目录：\n$ tree -L 2 # 安装后的目录结构 . ├── CONTRIBUTING.md ├── Gruntfile.js ├── LICENSE-MIT ├── README.md ├── grunt-tut.jquery.json ├── libs │ ├── jquery │ ├── jquery-loader.js │ └── qunit ├── node_modules │ ├── grunt │ ├── grunt-contrib-clean │ ├── grunt-contrib-concat │ ├── grunt-contrib-jshint │ ├── grunt-contrib-qunit │ ├── grunt-contrib-uglify │ └── grunt-contrib-watch ├── package.json ├── src │ └── grunt-tut.js └── test ├── grunt-tut.html └── grunt-tut_test.js 正如你所看到的，它给了我们一个好的开始：不仅仅有插件文件（src/jquer.demo.js) ，同时还写了 QUnit 测试（test/jquery.demo_test.js），还有已经写好的任务的 Gruntfile.js。\n现在我们可以执行 grunt 后将会的到下面的结果:\n$ grunt Running \u0026quot;jshint:gruntfile\u0026quot; (jshint) task \u0026gt;\u0026gt; 1 file lint free. Running \u0026quot;jshint:src\u0026quot; (jshint) task \u0026gt;\u0026gt; 1 file lint free. Running \u0026quot;jshint:test\u0026quot; (jshint) task \u0026gt;\u0026gt; 1 file lint free. Running \u0026quot;qunit:files\u0026quot; (qunit) task Testing test/grunt-tut.html ....OK \u0026gt;\u0026gt; 5 assertions passed (37ms) Running \u0026quot;clean:files\u0026quot; (clean) task Running \u0026quot;concat:dist\u0026quot; (concat) task File \u0026quot;dist/grunt-tut.js\u0026quot; created. Running \u0026quot;uglify:dist\u0026quot; (uglify) task File \u0026quot;dist/grunt-tut.min.js\u0026quot; created. Done, without errors. 这里，我需要特别说明的就是 Qunit 测试。常规的 QUnit 应该是能在浏览器中运行。 然而 grunt qunit 测试为了能在命令行中运行，采用 PhantomJS (终端的浏览器)。 如果你的测试失败了，后续的任务将会被终止。\n下面，grunt jquery 模版的默认任务列表：\ngrunt lint：代码质量检查 grunt qunit：执行 QUnit 测试 grunt concat：把你的所有代码都合并一个新的文件中，并保存在 dist 文件夹中 grunt min：压缩合并后的代码 具体任务实现代码请看你 path/to/grunt_tut/Gruntfile.js。\nEnjoy Grunt！后续的文章将会深入剖析了 GruntJS。 # ","date":"2014-01-22","permalink":"/n3xtchen/2014/01/22/javascript-grunt-starter/","section":"时间线","summary":"如果你正在进行一个 JS 项目，你可能会有一堆的日常工作要做。那有哪些需要做呢？","title":"Javascript Grunt - 快速入门"},{"content":"让我们开始建立一个属于自己的超级简单的 bower 自定义库。它将会是一个无意义的 包，但是它将设计创建的每一个步骤。\n一. 创建你的 bower.json # 首先创建一个项目目录，命名为 next。\n然后在目录下创建 bower.json，类似如下：\n{ \u0026quot;name\u0026quot;: \u0026quot;demo\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;authors\u0026quot;: [ \u0026quot;n3xtchen \u0026lt;echenwen@gmail.com\u0026gt;\u0026quot; ], \u0026quot;main\u0026quot;: { \u0026quot;demo\u0026quot;: \u0026quot;./demo.js\u0026quot; }, \u0026quot;dependencies\u0026quot; : { \u0026quot;jquery\u0026quot;: \u0026quot;1.10.*\u0026quot; } \u0026quot;license\u0026quot;: \u0026quot;MIT\u0026quot; } 这是我使用 bower init 命令创建和修改的，除了 main 之外，其他属性应该都 很熟悉。（不熟悉请见 [上一个教程]({% post_url 2014-01-16-javascript-bower-starter %})）\nmain 属性应该是你主程序脚本的路径，如果有多个程序脚本，应该使用数组。实际 上，该属性的实际用法官方也未确认；但是有一个建议，它应该是一个对象，相关代码 的特征名为键名，值是相关代码路径的字符串；如果包含多个文件（每种文件类型只能 有一个，例如还可以有一个 .html，.css 文件），使用数组。\n... \u0026quot;main\u0026quot;: { “modal”: [“lib/modal.js, “css/modal.css”], “carrousel”: [“lib/carrousel.js”, “css/carrousel.css”] }, ... 二. 创建你 git 版本库 # 初始化 git，上传上一步骤创建的 bower.json\n$ git init $ git add bower.json $ git commit -m '添加 bower.json' $ bower install 现在，我们编写 demo.js：\nfunction sayHi() { alert('Hello, World!'); } 现在我们把它推送到 github 上：\n$ git add demo.js $ git commit -m '添加 demo.js' 接着，我们需要添加 git 的语义化版本标签：\n$ git tag -a 1.0.0 -m 'demo v1.0.0' 最后，我们把它上传到 github 上。\n三. 登记你的 bower 库 # $ bower register packageName git://your/git/url.git 当然，我们不会真的把这个库登记到 bower 上。但是，bower 也可以安装本地的包。 我所要做的的就是模拟一个 Github，创建这个源的 bare 克隆：\n$ git clone --bare path/to/demo path/to/demo.git 现在我们就可以通过这个源的地址来安装自定义的库了：\n$ bower install path/to/sayHi.git 安装的过程中，它的相关依赖也会被安装。\n小结 # 发布你的库需要下述条件：\n在主目录必须有有效的 mainifest JSON文件（就是 bower.json）; 应该使用 Git 的语义化版本标签； 你的包必须上传到 Github 上； 然后使用如下命令：\nbower regiter \u0026lt;my_package-name\u0026gt; \u0026lt;git-endpoint\u0026gt; 享受你的自定义 bower 库！ # ","date":"2014-01-17","permalink":"/n3xtchen/2014/01/17/javascript-bower-register/","section":"时间线","summary":"让我们开始建立一个属于自己的超级简单的 bower 自定义库。它将会是一个无意义的 包，但是它将设计创建的每一个步骤。","title":"Javascript Bower - 发布自己的 bower 库"},{"content":"","date":"2014-01-17","permalink":"/n3xtchen/tags/package_manager/","section":"标签","summary":"","title":"package_manager"},{"content":"Bower， browser package manager（浏览器包管理器）, 不仅仅象 JAM 那样的 JS 包管理器；也不仅仅象 RequireJS 那样的模块载入器；最大的区别就是它不仅仅是 JS 包管理器，还可以管理 HTML，CSS 以及图片。\n不过再怎么强大，bower 也只是一个包管理器，他不能组合代码或者压缩代码，也不能像 AMD 这样的模块系统。\n安装 Bower # $ npm install -g bower 这是我当前使用的 node 版本\n$ node -v v0.10.24 $ bower -v 1.2.8 使用演示 # 查询 # $ bower search backbone # 查询包名与 backbone 相匹配的代码库 Search results: backbone git://github.com/jashkenas/backbone.git backbone-amd git://github.com/amdjs/backbone ... 省略 安装，更新及卸载 # $ bower install backbone # 安装 $ tree -L 2 . └── bower_components └── backbone $ bower install jquery#1.7.0 # 可以制定版本 $ bower install git://github.com/pivotal/jasmine.git # 也可以制定 GH 的地址 $ bower update # 更新库 $ bower uninstall jquery # 卸载库 其他命令 # $ bower list # 查看安装的库 $ bower lookup backbone # 查看库代码的地址 backbone git://github.com/jashkenas/backbone.git $ bower info backbone bower cached git://github.com/jashkenas/backbone.git#1.1.0 bower validate 1.1.0 against git://github.com/jashkenas/backbone.git#* { name: 'backbone', homepage: 'https://github.com/jashkenas/backbone', version: '1.1.0' } Available versions: - 1.1.0 - 1.0.0 - 0.9.10 ... 省略 安装之后，bower 会默认把文件备份一份到根目录的 .bower 文件中（~/.bower/）,这样可以加速 安装的库\n$ bower cache-clean # 清除缓存在 ~/.bower 中的包 $ bower install \u0026lt;package-name\u0026gt; --offline # 从缓存中寻找包并安装 使用 bower 构建你的项目（或库） # 你应该在应用根目录创建 bower.json 来管理你项目的依赖；就想 npm 的 package.json 和 gem 的 Gemfile 那样，是非常有用的。\nNote: 在 bower 0.9.0 之前，包元数据文件称之为 component.json，而不是叫 bower.json。这样做的用意是为了避免和其他工具使用的配置文件名冲突。现在， 你仍然和是用 component.json，但是在将来版本会被彻底禁用。 ： 使用如下命令来创建你的 bower.json：\n$ bower init 定义的选项如下：\nname（必须）：项目或自定义库的名称\nversion：版本号\nmain［字符或者数组］：包的主文件，,下一个教程会详细解释\nigore［数组］：在你安装包的时候，你想要 bower 忽略安装的文件路径列表\ndependencies［哈希对象］：产品所依赖的包名\ndevDependencies［哈希对象］：开发时使用的依赖，可能是测试工具包\nprivate［布尔］：如果你不希望你代码被公开，就设成 true\n{ \u0026quot;name\u0026quot;: \u0026quot;my-project\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;path/to/main.js\u0026quot;, \u0026quot;ignore\u0026quot;: [ \u0026quot;.jshintrc\u0026quot;, \u0026quot;**/*.txt\u0026quot; ], \u0026quot;dependencies\u0026quot;: { \u0026quot;\u0026lt;name\u0026gt;\u0026quot;: \u0026quot;\u0026lt;version\u0026gt;\u0026quot;, # 指定版本 \u0026quot;\u0026lt;name\u0026gt;\u0026quot;: \u0026quot;\u0026lt;folder\u0026gt;\u0026quot;, # 指定本地路径 \u0026quot;\u0026lt;name\u0026gt;\u0026quot;: \u0026quot;\u0026lt;package\u0026gt;\u0026quot; # 指定 git 路径 }, \u0026quot;devDependencies\u0026quot;: { \u0026quot;\u0026lt;test-framework-name\u0026gt;\u0026quot;: \u0026quot;\u0026lt;version\u0026gt;\u0026quot; # } } 定义好你的 bower.json，执行安装：\n$ bower install 你定义的依赖包将被安装在 bower_components。\n结语 # 如果你看过其他的包管理工具，你可能会强加一些东西在 bower 身上，尤其是缺乏了很多 包管理工具的特性。我也有同样的疑惑。当我深入了解了 bower 之后， 借用 Andrew Burgess 的话：\nBower 是比 Jam，Volo 和 Ender 更底层的组件。这些管理器可以借用它来 作为依赖包管理。\n因此，如果你没使用过 bower，最好了解下它的命令，因为很多工具都是借助它来 构建起来。实际上，js 编程工具新贵 Yeoman 就是使用 bower 作为包管理器。如果你不 了解 Yeoman，接下来将会有专门的教程展示给大家。 的教程将会介绍到它\n相关链接 # Andrew Burgess - Meet Bower bower 官网 ","date":"2014-01-16","permalink":"/n3xtchen/2014/01/16/javascript-bower-starter/","section":"时间线","summary":"Bower， browser package manager（浏览器包管理器）, 不仅仅象 JAM 那样的 JS 包管理器；也不仅仅象 RequireJS 那样的模块载入器；最大的区别就是它不仅仅是 JS 包管理器，还可以管理 HTML，CSS 以及图片。","title":"Javascript Bower - 快速入门"},{"content":" 如果你已经使用了 PHP_CodeSniffer 来检查你的代码规范问题，通过手动去修复他们可能 会是个噩梦，尤其是在大项目中;怎么办\n当我在调试代码之后，突然发现我已经把代码搞得乱七八糟了；怎么办？\n我平时都适用 IDE 自带的格式化工具，但是它不是按照 PSR 的规范；怎么办？\nPHP-CS-Fixer 来拯救你；全名：PHP Coding Standards Fixer，它就是为了解决根据 PSR 规 范修复你代码大部分不符合规范的的修复工具。\n如果你还不知道什么 PSR 和 PHP_CodeSniffer, 请看 PHP CodeSniffer - 使用 PSR 规范你的PHP代码\n安装 PHP-CS-Fixer # 下载 php-cs-fixer.phar 文件；确保你的文件路径包含在 PATH 中，就可以直接使用；\n如果是 Linux 用户还需要给该文件可执行的权限：\n$ sudo chmod a+x /path/to/php-cs-fixer 也可以使用 Composer 来安装：\n$ composer global require 'fabpot/PHP-CS-Fixer=*' 确保 ~/.composer/vendor/bin/ 这个路径在你的全局 PATH 中。\n对于 Mac 用户，还可以使用 homebrew（Mac Port 的替代者） 来安装\n$ brew tap josegonzalez/homebrew-php $ brew install php-cs-fixer 如何更新你的 PHP-CS-Fixer 让他支持最新的标准规范： # 对于非 homebrew 安装的用户忙，可以：\n$ sudo php-cs-fixer self-update # window 用户不需要使用 sudo Homebrew 的用户：\n$ brew upgrade php-cs-fixer 开始使用吧！ # $ php-cs-fixer fix /path/to/dir/or/file --level=all level 用来选择格式化依据的标准，有 psr0，prs1，psr2，all；默认是选择 psr-2 标准\n详尽的使用文档请见： docs\nIDE 整合 # IDE 还是大部分选择的主要开发工具，一定程度上是提升了不少开发体验；\nPHP-CS-Fixer 不例外也对大部分 IDE 的支持\nNetbeans # 打开 NetBeans； 进入 Tool =\u0026gt; Plugins =\u0026gt; Download ，然后点击 Plugin； 进入 Available Plugins 搜索 php cs fixer ，选中 PHP CS Fixer 确认安装； 进入 Tool 标签页 =\u0026gt; Options =\u0026gt; PHP , 名称为 PHP CS Fixer； 将 php-cs-fixer 执行文件的绝对路径填入 PHP CS Fixer；勾选 \u0026ndash;level，选择 all； 文件右键点击就可以看到 PHP CS Fixer 的菜单了； Happy，Fixing!\n","date":"2014-01-09","permalink":"/n3xtchen/2014/01/09/php-cs_fixer/","section":"时间线","summary":"如果你已经使用了 PHP_CodeSniffer 来检查你的代码规范问题，通过手动去修复他们可能 会是个噩梦，尤其是在大项目中;怎么办","title":"PHP PHP-CS_Fixer - 使用 PSR 格式化你的代码"},{"content":"","date":"2014-01-09","permalink":"/n3xtchen/tags/psr/","section":"标签","summary":"","title":"PSR"},{"content":"","date":"2014-01-09","permalink":"/n3xtchen/tags/tools/","section":"标签","summary":"","title":"Tools"},{"content":" 风格一致性使你的代码更加专业；相同项目的风格不一致（更坏的，是在同一个文件多个 编码风格）不仅看起来很邋遢，更纵容了将来的不严谨风格的产生；\n当编码风格符合统一标准，错误的代码也很容易察觉；\n对于后续的维护人员来说，也更容易调试，修复错误已经拓展功能；\n维护不一致代码的时候，有时开发者将会重新格式化代码来适应他们。这样会造成代码库 的大范围变动；如果将来造成问题，使用差异化对比工具将无法通过对比代码来排除 Bug。\n如果你出现上述问题，那么 PHP_CodeSniffer 就是你的绝佳选择之一，尤其对于 PHP 开发者来说。\n首先介绍什么是 PSR（重点哦） # 下面中文版的规范：\nPSR-0 PSR-1 PSR-2 PSR-3 PSR-4 由于 PHP 权威的项目支持，个人认为及其有可能发展成为 PHP 的业界的规范；\n加入该标准的项目：\nComposer Doctrine Drupal Laravel PEAR Propel Symfony2 Yii framework Zend Framework 2 。。。 详见： FIG\n安装你的 PHP_CodeSniffer（注意，PHP 5.1.2 骨灰级的开发者请绕行） # 使用 PERA 是最简单的安装方式，首先请确认你已经安装了 PEAR：\n$ pear install PHP_CodeSniffer 使用 pear config-get php_dir 查找 PEAR 的目录，然后创建 \u0026ldquo;/PHP/CodeSniffer/Standards\u0026rdquo; 目录（用来存放标准规则的）；\n也可以使用新潮的安装工具 Composer（不知道，你就 OUT了！） 来安装）：\n$ composer global require 'squizlabs/php_codesniffer=*' 确保 ~/.composer/vendor/bin/ 这个路径在你的全局 PATH 中。\n如果执行下述命令，说明你成功了；\n$ phpcs --version PHP_CodeSniffer version 1.5.1 (stable) by Squiz (http://www.squiz.net) 定制代码规范的存放目录：~/.composer/vendor/squizlabs/php_codesniffer/CodeSniffer/Standards\nPHP_CodeSniffer 源 # Pear： http://pear.php.net/package/PHP_CodeSniffer\nGitHub（非官方）： https://github.com/squizlabs/PHP_CodeSniffer\n开始用 PHP_CodeSniffer # $ phpcs file/to/sniff 或者指定你想要支持的标准，可以这么用：\n$ phpcs --standard=build/phpcs/Joomla path/to/file/or/folder 详尽的文档请见： docs\n安装 PSR-0，PSR-1，PSR-2 规则 # 下载地址： Standards\n如果，你使用 Xampp，把 PSR-1 的规则解压到 \\xampp\\php\\PEAR\\PHP\\CodeSniffer\\Standards\\PSR (其他名称也可以)，IDE 会识别到；\nIDE 整合 # 虽然使用终端命令是最高效的方式，但是可能你需要更好的使用体验，即使是 Linux 大牛。\n幸运的是，Eclipse，Netbean 和 PHPStorm 都有相关的插件支持，因此任何代码标准冲突 都可以和普通一样被展示。\nNetbeans # 打开 NetBeans； 进入 Tool =\u0026gt; Plugins =\u0026gt; Download ，然后点击 Plugin； 搜索选中载入的 phpmdnb 文件，确认安装； 进入 Available Plugins 搜索 phpmd ，选中 phpCs-MD Plubgin 确认安装； 进入 Tool 标签页 =\u0026gt; Options =\u0026gt; PHP , 名称为 PHPCodeSniffer； 你需要设置 phpcs.bat 到路径所在： Unix，使用 /usr/bin/phpcs（你可以使用 which 命令查找 phpcs 的路径）; Xampp 中，你可以在 PHP 的根目录内找到该文件； Standard 类型选择 PSR 来； 现在，你点击测试 Settings 来检查配置，单击 OK 来完成安装； 打开任务窗口（Window =\u0026gt; Tasks）来检测代码； 大部分时间使用任务（只有在编辑文件，或者创建自己的过滤器会现实错误）。 文件右键点击就可以看到 Check Style 菜单（它就是传说中的 CodeSniffer）； Eclipse # 安装非常简单，遵循通常的操作：\nHelp =\u0026gt; Install new Software\u0026hellip; 填入网址： http://www.phpsrc.org/eclipse/pti/ 选中 Tools -\u0026gt; PHP CodeSniffer，安装 重启 你现在就能使用通用的标准如 PEAR 和 ZEND 等等来检查代码规范；\n使用自己的规范，所做的就是制定他们位置，然后激活它就可以：\nWindow =\u0026gt; Preferences PHP Executables，点击 Add..；添加 PHP 执行路径，保存 PHP Tools =\u0026gt;Library =\u0026gt; PEAR，点击 New，添加 PEAR 目录的路径，保存； PHP Tools =\u0026gt; PHP CodeSniffer，PHP Executable 选择第二步添加那个配置； PEAR Libary 选择第三步添加的那个库； 现在就可以选择你安装过的标准包； Happy Sniffing!\n","date":"2014-01-08","permalink":"/n3xtchen/2014/01/08/php-code_sniffer/","section":"时间线","summary":"风格一致性使你的代码更加专业；相同项目的风格不一致（更坏的，是在同一个文件多个 编码风格）不仅看起来很邋遢，更纵容了将来的不严谨风格的产生；","title":"PHP CodeSniffer - 使用 PSR 规范你的PHP代码"},{"content":"Pig 数据类型主要分为两种类型：标量（Scalar）类型，它只包含单一的值；复合（Complex）类型，它可以包含多个值，可以是不同类型。\n标量类型（Scalar Type） # Pig 的标量类型和其他语言的类似，都是简单的类型。除了 byteArray 之外，其他的都可以 在 java.lang 类中找到相对应的类型，这样使得 UDF（用户定制函数）更容易构建：\n整型 int # 整数。Ints 在 Java 中对应的类是 Java.lang.Integer；4位有符号整型；表达形式为整数 ，例如 42；\n长整型 long # 长整数。Longs 在 Java 中对应的类是 Java.lang.Long；8位有符号的整型；表达形式为 整数＋L后缀，例如 500000000L；\n浮点型 float # 浮点型数字。Floats 在 Java 中对应的类是 Java.lang.Float；4位字节；表达形式为 浮点数＋f 后缀，例如 3.14f；或者指数表达形式，例如 6.022e23f;\n双精度浮点型 double # 双精度点型数字。Doubles 在 Java 中对应的类是 Java.lang.Double；4位字节；表达形式 为浮点数，例如 3.1415926；或者指数表达形式，例如 6.022e-34;\n注释 # 你可以在 http:gi/java.sun.com/docs/books/jls/ third_edition/html/typesValues.html#4.2.3. 网站上找到浮点型和双精度浮点型的取值范围。注意，因为是浮点数，所以在一些计算 时会导致精度缺失；如果需要保持精度，你应该使用 int 或者 float。\n字符数组 chararray # 字符串或字符数组。Chararray 在 Java 中对应的类是 Java.lang.String。它的表达形式 是由单引号包围的字符串字画量，例如 \u0026lsquo;fred\u0026rsquo;。除了标准的字符和符号字符外，我们需要 使用反斜杠来转移部分特殊字符，比如, tab \\t，换行符 \\n。Unicode 字符使用 \\u 前缀 ＋ 四位16进制的Unicode值；例如，ctrl-A 表达为 \\u0001；\n字节数组 bytearray # 一个大型2进制对象或者字节数组。Bytearrays 在 Java 中对应的类是 DataByteArray； 没有办法指定一个字节数组常量。\n复合（Complex）类型 # Pig 有三种复合数据类型：maps，tuples 和 bags。所有的这些类型都可以包含在其它的 符合类型中。因此，可能存在 map 类型的值中包含一个 bag，而这个 bag 中还包含了一个 元组。\nMap # Pig 的 Map 数据是指字符串到数据元素的映射（Mapping），它的元素可以是任何 Pig 类型 ，包括复合类型。字符串被称为键（Key），被用来作为索引来帮助查找元素。\n因为 Pig 不知道值的类型，所以他将会假设值是一个字节码（Bytearray）。然而，实际的 值可能会有所不同。如果你知道它的实际类型，你就可以转换（Cast）它。如果你不对他做 类型转换，Pig 将会根据你的使用方法来作出最佳的假设。如果值不是 bytearray， Pig将 在运行时对他进行处理。\n默认，Pig 没有要求 Map 中的各个值必须是同种类型。如果 map 中有姓名和年龄的话，也 是合法的。从 Pig 0.9 开始，Map 数据类型中的各个值必须是同种类型的；这是很有用，就 可以减轻系统的负担，不用遍历每一个元素。\nMap 常量使用方括号［bracket］来界定该类型的，哈希键值，键值之间使用井号隔开，键值 对之间使用逗号（comma）隔开；例如，[\u0026rsquo;name\u0026rsquo;#\u0026lsquo;bob\u0026rsquo;,\u0026lsquo;age\u0026rsquo;#55] 将会创建一个有两个键的 map；第一个值是一个字符串，第二个值是一个整型。\n元组 Tuple # 元组是一个可调整长度，有序的数据集合。元组包含多个字段（fields）；每个字段包含一 个数据元素；每一个字段可以是任何类型，不需要保持同种类型。元组类比 SQL 中的行。 还有元组是有序的，所以可以通过位置来取相应的值。元组可以，但不是必须，关联一个 模式（schema），用来描述各个字段的类型，还可以为字段提供一个名称，来方便取值。 这样使得允许用户检查元组的数据，通过字段名索引相应字段。\n元组常量使用圆括号（paranthese）来指定，每个字段之间使用逗号（comma）隔开；例如， (\u0026lsquo;meo\u0026rsquo;, 27)。\nBag # Bag 是一种无序元组的组合。因为他无序，所以他不能通过位子索引元素。和元组一样，它\n可以但不是必须有有一个模式（schema）来关联它。\nBag 常量使用花括号（braces），其中的元组（tuple）使用逗号（comma）隔开。例如，\n{(\u0026lsquo;meo\u0026rsquo;, 27), (\u0026lsquo;cm\u0026rsquo;, 29), (\u0026lsquo;jl\u0026rsquo;, 26)}。\nPig 用户通常会注意到，Pig 没有提供列表（list）和 set 类型。它可能是因为可以使用\nbag 来模拟一个 set，通过使用一个元素的元组来分装。虽然做法有点傻，但是它生效。\nBag 是 Pig 类型中，不需要受 内存限制的。你后面将会看到，因为他用来存储分组后的\n集合，所以 bag 可能很大。如果必要，Pig 可以把 bag 存储在磁盘中，只有 bag 中的\n部分存储在内存中。bag 到大小限制是可用的磁盘空间。\nPig 数据的内存要求 # 在之前的章节，我都会提到每个类型存储的大小。这就是告诉你这个值占用多少内存。而\n不是告诉你存储这个值的对象的到校。Pig 使用 Java 对象内部呈现这些值；所以还需要\n多余的空间来存储它。这个多出来的空间取决于你使用的 JVM，但是通常占用 8 个字节\n。最糟糕的是字符串（chararray），因为 Java 字符串中的每个字符占用两个字节。\n因此，如果你要计算 Pig 处理数据需要的存储量，不要计算硬盘中的字节数，而是算你\n需要的内存。硬盘和内存的乘积关系依赖你的数据，是否被压缩，还有你的磁盘格式等\n等。作为参考，存储在内存的一倍数据量，就需要四倍的磁盘空间（未压缩的）。\nNulls # Pig 的数据类型可以是空值（null）。任何类型的数据都可以为空。Pig 空值的概念和 SQL\n中的相同，但是完全不同与 C，Java，Python 等等语言的空值。Pig 中的空值意味着位置\n。可能由于数据丢失，处理时发生错误等等原因。在大部分过程（procedural）语言中，\n当未被设定或者未指向相应的地址或对象时被称作空。这样的差异导致 Pig 处理空值的方\n式差异很大。\n不象 SQL，Pig 没有限制（constraints）的概念。根据 null 的定义，这意味着任何数据\n元素可以永远是空。这个需要在你编写 Pig 脚本时，需要牢记的。\n模式（Schema） # Pig 对数据的模式保持很宽松的态度。这是 Pig eat anyting 哲学导致的结果。如果你的\n数据模式可用，Pig 将会在错误检查或者优化的时候，使用它。但是，如果无模式，Pig 仍然可以处理数据，它可以基于脚本来判断他的类型。首先，我们会寻找与 Pig 交互\n的模式；然后，我们会检查 Pig 处理模式未提供部分的处理方式。\n最简单的方式就是直接明确告诉 Pig 处理数据的模式：\ndividends = load 'NYSE_dividends' as (exchange:chararray, sybol:chararray, date:chararray, dividend: float); 这样，Pig 期待你的数据有四个字段。如果多了，多余的部分切割掉；少了使用 null 补齐 。\n你也可以不指定模式，这些类型都会被默认当作 bytearray：\ndividends = load 'NYSE_dividends' as (exhange, sybol, date, dividends); 模式的运行时声明是很好的习惯。对于用户来说，操作数据时不用先把它载入到元数据系统 是很方便的。这意味着，如果你只要前几个字段，你只要声明这几个字段就好了。\n但是，在产品系统，每天每小时运行相同的数据，你就会有一些大问题。第一， 无论你数据什么事变动，你都必须改变你的 Pig 脚本。第二，即使在 5 列的数据工作 良好，但是如果有 100 列的时候是很痛苦的。为了定位这些问题，Pig 提供了其他的 方式来载入模式；\n如果你使用的载入函数已经知道数据的模式，函数将会和 Pig 沟通。（载入函数定义 Pig 读取数据的方式；）载入函数可能已经知道数据的模式，如果这些数据存储在元数据源 ，例如 HCatalog；或者数据本身就存储着模式，例如 JSON。在这种情况，你不用在载入 数据的时候声明模式。不过你还是需要使用名称命名字段，因为在做错误检查的时候， PIg 将会从载入函数获取模式：\nmdata = load 'mydata' using HCatLoader(); cleansed = filter mdata by name is not null; ... 如果出现你指定的模式和 loader 返回不一致，Pig 将会以loader 返回的模式为准。例如， 如果你指定一个字段为 long 类型，但是 loader 告诉你它是 int 类型，Pig 将会对它转化 成 int 类型。然而，如果它不能将你给的模式转化成适合 loader 的时候，它将会报错。\n还有一种情况，你和载入函数都没有告诉该数据的结构。你可以使用位置编号来索引字段， 它是从 0 开始。语法之前已经说过了：$n，这里 n 是非负整数。Pig 默认把它当作 bytearray 类型，从你的实际操作，推断它的类型。\n--no_schema.pig daily = load 'NYSE_daily'; calcs = foreach daily generate $7 / 1000, $3 * 100.0, SUBSTRING($0, 0, 1), $6 - $3; 在表达式中：\n$7 / 1000 : 很容易推断出第八个位置的数据类型可以是任何能转化哼整数的类型 $3 * 100.0：第3个位置的数据类型应该是浮点型 SUBSTRING($0, 0, 1)：能进行字符操作就很容易推断出她是 chararray 类型 $6 - $3：- 在 Pig 中只能用于数值运算 这样，Pig 就能很容易和安全地推断出他们实际的类型。当然，Pig 并不是完全智能的：\n--no_schema_filter daily = load 'NYSE_daily'; fltrd = filter daily by $6 \u0026gt; $3; \u0026gt; 对于数值，字符，字节类型都有效，因此 Pig 不能做出准确的判断。这时，都默认当作 字节类型处理。\n还有一种使 Pig 失去判断力的情况：\n--unintended_walks.pig player = load 'baseball' as (name:chararray, team:chararray, pos:bag{t:(p:chararray)}, bat:map[]); unintended = foreach player generate bat#'base_on_balls' - bat#'ibbs'; 因为 map 中的值可以是任何类型，Pig 不知道 bat#\u0026lsquo;base_on_balls\u0026rsquo; 和 bat#\u0026lsquo;ibbs\u0026rsquo; 到底 是什么类型。基于该字段的用法，Pig 将假设该字段是 doubles 类型。但是，实际上它是 int 类型。\n另外还有一种情况，当有模式的数据和无模式的数据进行联表，产生的结果的模式将会全丢失：\n--no_schema_join.pig divs = load 'NYSE_dividends' as (exchange, stock_symbol, date, dividends); daily = load 'NYSE_daily'; jnd = join divs by stock_symbol, daily by $1; 这个例子中，Pig 不知道 daily 的模式，所以它就理解 divs 和 daily 联表后的数据模式了。\nEnjoy It!!! # ","date":"2014-01-06","permalink":"/n3xtchen/2014/01/06/hadoop---data-pig---data-type/","section":"时间线","summary":"Pig 数据类型主要分为两种类型：标量（Scalar）类型，它只包含单一的值；复合（Complex）类型，它可以包含多个值，可以是不同类型。","title":"Data Pig - 数据类型和数据结构"},{"content":"","date":"2014-01-06","permalink":"/n3xtchen/tags/schema/","section":"标签","summary":"","title":"schema"},{"content":"","date":"2014-01-05","permalink":"/n3xtchen/tags/fig/","section":"标签","summary":"","title":"FIG"},{"content":" 译自 http://www.php-fig.org/psr/psr-0/\n接下来描述的是强制性的要求，为了保障自动载入器（Autoloader）的通用性（ Interoperability），必须坚持该原则。\n强制性 # 完整的命名空间（Namespace）和类（Class）必须遵循如下结构:\n\\\u0026lt;VendorName\u0026gt;\\(\u0026lt;NameSpace)\\)*\u0026lt;Class Name\u0026gt;\n每一个命名空间必须有一个顶级命名空间（Vendor Name）；\n每一个命名空间都可以有多个子命名空间；\n每一个命名空间的分隔符都必须转化成 DIRECTORY_SEPARATOR，当它被文件系统载入时；\n每一个在类名中的 _ 符号都会被转化成 DIRECTORY_SEPARATOR。而 _ 符号在 命名空间中没有任何意义；\n完整的的命名空间和类当从文件系统中被加载的时候，都会以 .php 为后缀；\nVendor 名，命名空间和类名中的英文字符可以是任意大小写的组合；\n范例 # \\Doctrine\\Common\\IsolatedClassLoader =\u0026gt; /path/to/project/lib/vendor/Doctrine/Common/IsolatedClassLoader.php \\Symfony\\Core\\Request =\u0026gt; /path/to/project/lib/vendor/Symfony/Core/Request.php \\Zend\\Acl =\u0026gt; /path/to/project/lib/vendor/Zend/Acl.php \\Zend\\Mail\\Message =\u0026gt; /path/to/project/lib/vendor/Zend/Mail/Message.php 命名空间和类名中的下划线 # mespace\\package\\Class_Name =\u0026gt; /path/to/project/lib/vendor/namespace/package/Class/Name.php \\namespace\\package_name\\Class_Name =\u0026gt; /path/to/project/lib/vendor/namespace/package_name/Class/Name.php 这里我们设定的标准应该是无痛自动加载器通用性的最低共同标准。你可以使用下面 SplClassLoader 的实现范例来测试这些是否符合标准。\n范例实现 # 下面的例子用来简单的演示上面符合标准的代码如何被自动加载。\n\u0026lt;?php function autoload($className) { $className = ltrim($className, '\\\\'); $fileName = ''; $namespace = ''; if ($lastNsPos = strrpos($className, '\\\\')) { $namespace = substr($className, 0, $lastNsPos); $className = substr($className, $lastNsPos + 1); $fileName = str_replace('\\\\', DIRECTORY_SEPARATOR, $namespace) . DIRECTORY_SEPARATOR; } $fileName .= str_replace('_', DIRECTORY_SEPARATOR, $className) . '.php'; require $fileName; } SplClassLoader 的实现 # 下面的 gist 是一个 SplClassLoader 实现的范例代码；如果你编写符合上述的标准，你就可 以利用它来自动加载你的类。这是一个目前用来加载 PHP 5.3 类的推荐方式：\n( http://gist.github.com/221634) ","date":"2014-01-05","permalink":"/n3xtchen/2014/01/05/php-fig---psr-0/","section":"时间线","summary":"译自 http://www.","title":"PHP FIG - PSR-0 自动载入标准（Autoloading Standard）"},{"content":" 译自 http://www.php-fig.org/psr/psr-1/\n这个章节讨论的是必须确保共享 PHP 代码之间的最高共通性， 以及技术互操作的高水平的标准编码要素。\n1. 概况 # 源文件中必须而且只能使用 \u0026lt;?php 和 \u0026lt;?= 标签； 源文件必须而且只能使用 UTF-8 without BOM 字符编码来编写 PHP 代码； 源文件中的代码应当只能有下述操作中的一种，但不应同时并存： 声明符号（classes, functions, constants, etc.）， 引起副作用（ side-effects） （例如，产生输出， 或者改变 .ini 设置，etc）; 命名空间（NameSpaces）和类（Classes）必须遵循 PSR-0 ； 类（Class）名必须使用 首字母大写的驼峰（StudlyCaps） 命名法则； 类常量（Class constants）必须使用 全大写以下划线隔开 的命名法则； 方法（Method）名称必须使用 驼峰（camelCase） 的命名法则； 2. 文件 # 2.1. PHP 标签（Tag） # PHP 代码必须使用 \u0026lt;?php ?\u0026gt; 长标签或者 \u0026lt;?= ?\u0026gt; 短输出标签； 但他们不应包含在对方的标签中。\n2.2. 字符类型（Character Encoding） # PHP 代码必须只能使用 UTF-8 without BOM 字符类型。\n2.3. 副作用（Side Effects） # 源文件中的代码应当只能声明符号（classes, functions, constants, etc.）， 但不产生任何副作用；或只能进行引起副作用的逻辑操作； 但不应同时并存：\n”Side effects“ 的语义指的是逻辑的执行，但不直接关联声明的类，方法和常量等等， 仅是包含文件而已。\n“Side effects” 可以包含如下：产生输出，明确 require 和 include 的使用，连接外 部的服务，改变 ini 配置，输出错误或异常，改变全局或静态变量，读写文件等等；\n下面是一个同时声明和副作用的例子（是我们应该避免的）：\n\u0026lt;?php // side effect: 改变 ini 设置 ini_set('errror_reporting', E_ALL); // side effect: 载入文件 include \u0026quot;file.php\u0026quot;; // side effect: 产生输出 echo \u0026quot;\u0026lt;html\u0026gt;\\n\u0026quot;; // 声明 function foo() { // function body } 接下来的例子是只包含声明但没有副作用的（是我们提倡的，要遵守的）：\n\u0026lt;?php // 声明 function foo() { // function body } // 条件声明不属于 side effect if (! funciton_exists('bar')) { function bar() { // funciton body } } 3. 命名空间（NameSpace）和类名（Class） # 命名空间和类必须遵守 PSR-0 。\n这意味着一个文件只能包含一个类，而且要在一个至少一级的命名空间下： 顶级的 vendor 名。\n类名必须使用 首字母大写的驼峰命名形式（StudlyCaps） 。\n代码必须支持 PHP 5.3 或者更高的版本，因为 5.3 之后才能支持正式的命名空间。\n例如：\n\u0026lt;?php // PHP 5.3 and later: namespace Vendor\\Model; class Foo { } PHP 5.2.x 之前应当使用 伪命名空间原则（Vender 名加下划线在类名之前） ：\n\u0026lt;?php // PHP 5.2.x and earlier: class Vendor_Model_Foo { } 4. 类常量（Class Constants）,属性（Properties）和方法（Methods） # 这里的类指的是类，接口和 Traits 的统称。\n4.1. 常量（Constants） # 类常量（Class constants）必须使用 全大写以下划线隔开 的命名法则；例如：\n\u0026lt;?php namespace Vendor\\Model; class Foo { const Version ＝ '1.0'; const DATE_APPROVED = '2013-06-01'; } 4.2. 属性（Properties） # 这个指南有意对属性的各种命名建议不做任何推荐，如 首字母大写的驼峰（$StudlyCaps） ， 驼峰（$camelCase） 和 下划线命名（$under_score） 。\n无论你使用哪一个原则，你都应当在合理的代码范围内保持一致性。这个范围可以是 vender 级别的，包级别的，类级别，还可以是方法级别的。\n4.3. 方法（Methods） # 方法名必须使用 驼峰的形式（camelCase()） 。\n","date":"2014-01-05","permalink":"/n3xtchen/2014/01/05/php-fig---psr-1/","section":"时间线","summary":"译自 http://www.","title":"PHP FIG - PSR-1 基本代码规范（Basic Coding Standard）"},{"content":" 译自 http://www.php-fig.org/psr/psr-2/\n这个指南为了补充说明 PSR-1，基本编码标准\n这个指南的意图是为了减少认知摩擦，当你浏览来自不同作者的代码时。我们通过列举我们 如何规范 PHP 代码的共同规则和预期来阐述这个指南。\n这里的代码规范源自各个成员项目的共性。当不同作者通过多个项目的合作，将可能得出gi 所有这些项目使用的行为指南。因此，这个指南的价值不是在于规则本身，而是规则的共性。\n1. 概况 # 代码必须符合 PSR-1； 代码必须使用 4 个空格缩进，而不是 tabs 缩进； 不应该对行数进行硬性限制；软限制应该是 120 行数；每行的长度不大于 80 个字符； 命名空间（namespace）声明和使用的语句之后应该留一行空白行； 类（class）的左花括号（openning braces）应该在类名的下一行；右花括号（closing gi braces）应该在类 body 的下一行； 所有属性和方法的声明都应该在访问控制之后（visibility）的；abstract 和 final 的声明必须在应该在访问控制之前；static 应该在访问控制之后的； 控制结构关键词之后应该留一个空格；方法和函数的调用则不需要； 左花括号必须和控制结构同行；右花括号（closing braces）应该在控制 body 的下一行； 控制结构的左括号后不需要留一个空格，右括号之后也不用留一个空格； 1.1. 范例 # \u0026lt;?php namespace VendorgiPackage; use FooInterface; use BarClass as Bar; use OtherVendorgiOtherPackage\\BazClass; class Foo extends Bar implements FooInterface { public function sampleFunction($a, $b = null) { if ($a === $b) { bar(); } elseif ($a \u0026gt; $b) { $foo-\u0026gt;bar($arg1); } else { BazClass::bar($arg2, $arg3); } } final public static function bar() { // method body } } 2. 通用 # 2.1. 基本编码标准 # 必须遵循 PSR-1;\n2.2. 文件（Files） # 所有的 PHP 文件都应该使用 Unix 风格的换行符（LF，linefeed）\n所有的 PHP 文件都应该以一个空白行结束；\n所有被包含的 PHP 文件的 ?\u0026gt; 闭标签应该被省略；\n2.3. 行（Lines） # 行长度不应该被硬性限制；\n行长度的软限制应该是 120 个字符；自动代码风格检查器应该发出警告但是不应该报错；\n行宽不应该大于 80 个字符；如果超过，应该拆分成多行；\n非空白行在结尾不应该出现空格；\n为了改善代码可读性，和区分不同代码块，应该添加空白行；\n每一行不应该超过一个语句；\n2.4. 缩进（Indenting） # 代码必须使用四个空白的缩进，不应该使用 tab；\n2.5. 关键词（Keywords）和 True/False/Null # PHP 关键词必须小写；\nPHP 常量 true，false 和 null 也必须小写；\n3. 命名空间（namesapce）和使用（use）声明 # 如果存在，namespae 的声明语句之后应该有一行空白行；\n如果存在，所有的 use 声明语句应该在 namespace 声明语句之后；\n每个声明应该匹配一个 use 关键字；\nuse 声明之后应该有一行空白行；\n例如：\n\u0026lt;?php namespace VendorgiPackage; use FooClass; use BarClass as Bar; use OtherVendorgiOtherPackage\\BazClass; // ... additional PHP code ... 4. 类，属性（Properties）和方法（Methods） # 这里的类是广义的类，它包含所有的类（classes），接口（interface）和traits；\n4.1. Extends 和 Implements # extends 和 implements 关键字应该和类的声明同一行；\n左花括号应该独立成一行；右花括号应该在类 body 的下一行；\n\u0026lt;?php namespace VendorgiPackage; use FooClass; use BarClass as Bar; use OtherVendorgiOtherPackage\\BazClass; class ClassName extends ParentClass implements giArrayAccess, \\Countable { // constants, properties, methods } implements 的类可以分布在多行，随后的行应该缩进一次。如果这么做，第一项应该 在下一行，而且每一个接口独占一行；\n\u0026lt;?php namespace VendorgiPackage; use FooClass; use BarClass as Bar; use OtherVendorgiOtherPackage\\BazClass; class ClassName extends ParentClass implements giArrayAccess, giCountable, giSerializable { // constants, properties, methods } 4.2. 属性（Properties） # 必须对所有属性设置访问控制（如，public，protect，private）；\nvar 关键词不应该用在声明属性上；\n一个声明一个属性，不应该超出；\n属性名不应该使用下划线作为前缀来指明 protected 和 私有类型。\n一个属性应该声明类似如下；\n\u0026lt;?php namespace VendorgiPackage; class ClassName { public $foo = null; } 4.3. 方法（Methods） # 访问控制必须在所有方法中声明；\n方法名不应该使用下划线作为前缀来指明 protected 和 私有类型。\n方法名和左括号之间不应该有空格。左花括号应该自成一行；右花括号应该在函数体的gi 下一行；左括号之后和右括号之前不应该有空格；\n一个方法声明应该类似如下；注意看括号，逗号，空格以及花括号；\n\u0026lt;?php namespace VendorgiPackage; class ClassName { public function fooBarBaz($arg1, \u0026amp;$arg2, $arg3 = []) { // method body } } 4.4. 方法参数（Method Arguments） # 参数表中，逗号之前不应该有空格；逗号之后应该空格；\n参数表中的默认参数应该位于参数表后部；\n\u0026lt;?php namespace VendorgiPackage; class ClassName { public function foo($arg1, \u0026amp;$arg2, $arg3 = []) { // method body } } 参数以分布在多行，随后的行应该缩进一次。如果这么做，第一项应该 在下一行，而且每一个参数独占一行；\n当列表尾部（不管是参数还是变量）被分成多行，右括号和左花括号必须夹带一个空格放 在一起自成一行。\n\u0026lt;?php namespace VendorgiPackage; class ClassName { public function aVeryLongMethodName( ClassTypeHint $arg1, \u0026amp;$arg2, array $arg3 = [] ) { // method body } } 4.5. abstract，final 和 static # 如果存在，abstract 和 final 的声明必须在应该在访问控制之前； 如果存在，static 应该在访问控制之后的；\n\u0026lt;?php namespace VendorgiPackage; abstract class ClassName { protected static $foo; abstract protected function zim(); final public static function bar() { // method body } } 4.6. 方法和函数调用 # 执行方法和函数的时候，函数或方法名和左括号之间不应该空格；左括号之后和右括号之前 不应该有空格；参数表中，逗号之前不应该有空格；逗号之后应该空格；\n\u0026lt;?php bar(); $foo-\u0026gt;bar($arg1); Foo::bar($arg2, $arg3); 参数以分布在多行，随后的行应该缩进一次。如果这么做，第一项应该 在下一行， 而且每一个参数独占一行；\n\u0026lt;?php $foo-\u0026gt;bar( $longArgument, $longerArgument, $muchLongerArgument ); 5. 控制结构（Control Structures） # 控制结构的通用编码风格规范：\n控制结构的关键字之后应该有一个空格 左括号之后不应该有空格 右括号之前不应该有空格 右花括号和左花括号之间应该有一个空格 结构体应该缩印一次 右花括号应该在结构体的下一行 每一个结构体应该被花括号包围；这样子，结构看起来很标准，并且当加入新行的时候，gi 可以减少引入错误的可能性。\n5.1 if，elseif，else # if 语句看起来类似如下；注意看括号，空格，逗号和花括号；else 和 elseif 和前一个 代码块的右花括号在同行；\n\u0026lt;?php if ($expr1) { // if body } elseif ($expr2) { // elseif body } else { // else body; } elseif 应该被使用来替代 else if 替换，这样每一个控制关键字看起来都是一个单词；\n5.2. switch, case # switch 语句看起来类似如下；注意看括号，空格和花括号；case 语句应该缩进一次， break 关键字（其他终止关键字）应该和 case 代码块同种缩进等级；如果 case 非空但是 不用终止，那应该给出注释（例如 // no break）\n\u0026lt;?php switch ($expr) { case 0: echo 'First case, with a break'; break; case 1: echo 'Second case, which falls through'; // no break case 2: case 3: case 4: echo 'Third case, return instead of break'; return; default: echo 'Default case'; break; } 5.3. while，do while # while 语句看起来类似如下；注意看括号，空格和花括号；\n\u0026lt;?php while ($expr) { // structure body } 同理，do while 也是一样的；\n\u0026lt;?php do { // structure body; } while ($expr); 5.4. for # for 语句看起来类似如下；注意看括号，空格和花括号；\n\u0026lt;?php for ($i = 0; $i \u0026lt; 10; $i++) { // for body } 5.5. froeach # foreach 语句看起来类似如下；注意看括号，空格和花括号；\n\u0026lt;?php foreach ($iterable as $key =\u0026gt; $value) { // foreach body } 5.6. try，catch # try catch 代码块看起来类似如下；注意看括号，空格和花括号；\n\u0026lt;?php try { // try body } catch (FirstExceptionType $e) { // catch body } catch (OtherExceptionType $e) { // catch body } 6. 闭包（Closures） # 闭包声明中的 function 关键词之后应该有一个空格，use 关键字的前后也应该有空白；\n左花括号应该和 function 关键词同行，右花括号应该紧接在 body 的下一行；\n左括号之后和右花括号之前不应该有空格；\n在参数表和变量表当中，每个逗号之前不应该有空格；逗号之后应该要有空格；\n闭包参数中的默认参数必须要位于参数表的后部；\n下面是一个闭包的声明；注意看括号，逗号，空格和花括号：\n\u0026lt;?php $closureWithArgs = function ($arg1, $arg2) { // body }; $closureWithArgsAndVars = function ($arg1, $arg2) use ($var1, $var2) { // body }; 参数表和变量表可以分布在不同行，随后的行应该缩进一次。这么做的时候，第一个参数 /变量应该在下一行，每一行应该只能一个变量或变量；\n当列表尾部（不管是参数还是变量）被分成多行，右括号和左花括号必须夹带一个空格放gi 在一起自成一行。\n下面是一个参数和变量列表被分割成多行的示例。\n\u0026lt;?php $longArgs_noVars = function ( $longArgument, $longerArgument, $muchLongerArgument ) { // body }; $noArgs_longVars = function () use ( $longVar1, $longerVar2, $muchLongerVar3 ) { // body }; $longArgs_longVars = function ( $longArgument, $longerArgument, $muchLongerArgument ) use ( $longVar1, $longerVar2, $muchLongerVar3 ) { // body }; $longArgs_shortVars = function ( $longArgument, $longerArgument, $muchLongerArgument ) use ($var1) { // body }; $shortArgs_longVars = function ($arg) use ( $longVar1, $longerVar2, $muchLongerVar3 ) { // body }; 注意，当闭包直接在一个函数或方法调用时作为参数传入。\n\u0026lt;?php $foo-\u0026gt;bar( $arg1, function ($arg2) use ($var1) { // body }, $arg3 ); 7. 结论 # 这里指南还有很多代码风格规范没有涵盖。内容如下：\n全局变量和全局常量的声明 函数的声明 操作符和赋值 行间对齐 注释和文档块 类名前缀和后缀 最佳实践 ","date":"2014-01-05","permalink":"/n3xtchen/2014/01/05/php-fig---psr-2/","section":"时间线","summary":"译自 http://www.","title":"PHP FIG - PSR-2 编码风格指南（Coding Style Guide）"},{"content":" 译自 http://www.php-fig.org/psr/psr-3/\n这个文档用来描述日志操作库的通用接口；\n主要目的是为了允许代码库接受 Psr\\Log\\LoggerInterface 对象，以简单通用的方式 记录日志；框架和 CMSs 可通过该接口定制他们需要的日志系统，但是必须兼容这个文档； 这个是为了确保程序使用的地方库可以写到集中的应用日志系统中。\n文档中的 implementor 被解释成他人通过 LoggerInterface 接口的实现；日志的用户 就是 user;\n1. 说明（Sepecification） # 1.1 基础（Basic） # LoggerInterface 抛出8个方法用来记录 8 种 RFC 5424 等级的版本（debug，info， notice，warning，error，critical，alert，emergency）；\n第九个方法 log 接受日志登记作为第一个参数；用一个日志等级常量来调用这个方法的 结果必须和调用具体等级方法的一致。如果具体的实现不知道传入的不按规范的等级来 调用这个方法必须抛出一个 Psr\\Log\\InvalidArgumentException；用户不应自定义一个 当前不支持的未知等级。\n1.2 信息（Message） # 每个方法应该接受一个字符串作为信息，或者是带有 __toString() 的对象； 实现（Implementors）可以对传入对象进行特殊处理。如果不是那种情况；实现 （Implementors） 必须将它 转化成字符串；\n信息可以包含占位符；这样可以根据上下文数据对其进行替换；\n占位符名称应该和对应的数据的键相匹配；\n占位符必须使用花括号（{}）来界定（即包围）；界定符和名称之间不可以有空格；\n占位符名应该只包含大小写字母，下划线（_）以及点号（.）；其他字符的使用由将来的 规格变更需要时添加；\n实现（Implementors）可以使用占位符实现各种转义；用户不了解上下文的情况不应该 体现转移占位符的值；\n下面是占位符转义的实现（Implementors）例子：\n/** * Interpolates context values into the message placeholders. */ function interpolate($message, array $context = array()) { // build a replacement array with braces around the context keys $replace = array(); foreach ($context as $key =\u0026gt; $val) { $replace['{' . $key . '}'] = $val; } // interpolate replacement values into the message and return return strtr($message, $replace); } // a message with brace-delimited placeholder names $message = \u0026quot;User {username} created\u0026quot;; // a context array of placeholder names =\u0026gt; replacement values $context = array('username' =\u0026gt; 'bolivar'); // echoes \u0026quot;User bolivar created\u0026quot; echo interpolate($message, $context); 1.3 上下文（Context） # 方法接受一个关联上文数据的数组参数；这意味着这意味着可能包含大量与数组无关 的数据；数组可以包含任何东西；实现（Implementors）必须对关联上下文数据竟可能 仁慈；传入的值不得抛出任何异常，错误，警告或者提示；\n如果 Exception 作为上下文传入；它必须存储在 exception 的键中；日志异常是 一个通用的模式，它允许实现（Implementors）提取其中的信息；实现（Implementors） 可以识别 excpetion 的键来处理异常对象；正因为他可以包含任何东西；\n1.4 帮助类和接口 # 托张 Psr\\Log\\AbstractLogger 类让你很容易实现 LoggerInterface 的通用 log 方法；其他八个方法通过它来转发信息；\n类似的，使用 Psr\\Log\\LoggerTrait 只需要你实现通用的log方法。记住traits不能 实现接口前，你依然需要 implement LoggerInterface；\nPsr\\Log\\NullLogger 是和接口一起提供的。它可以为使用接口的用户提供一个后备的 “黑洞”。如果上下文数据非常重要，这不失为一个记录日志更好的办法。\nPsr\\Log\\LoggerAwareInterface 只有一个setLogger(LoggerInterface $logger)方法 可以用来随意设置一个日志记录器。\nPsr\\Log\\LoggerAwareTraittrait 可以更简单的实现等价于接口。通过它可以访问 到$this-\u0026gt;logger。\nPsr\\Log\\LogLevel 类拥有八个等级的常量。\n2. 包（Package） # PSR/log 包中包含之前描述的接口和类，还有相关的异常类以及验证你实现的测试包；\n3. Psr\\Log\\LoggerInterface # \u0026lt;?php namespace Psr\\Log; /** * Describes a logger instance * * The message MUST be a string or object implementing __toString(). * * The message MAY contain placeholders in the form: {foo} where foo * will be replaced by the context data in key \u0026quot;foo\u0026quot;. * * The context array can contain arbitrary data, the only assumption that * can be made by implementors is that if an Exception instance is given * to produce a stack trace, it MUST be in a key named \u0026quot;exception\u0026quot;. * * See https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-3-logger-interface.md * for the full interface specification. */ interface LoggerInterface { /** * System is unusable. * * @param string $message * @param array $context * @return null */ public function emergency($message, array $context = array()); /** * Action must be taken immediately. * * Example: Entire website down, database unavailable, etc. This should * trigger the SMS alerts and wake you up. * * @param string $message * @param array $context * @return null */ public function alert($message, array $context = array()); /** * Critical conditions. * * Example: Application component unavailable, unexpected exception. * * @param string $message * @param array $context * @return null */ public function critical($message, array $context = array()); /** * Runtime errors that do not require immediate action but should typically * be logged and monitored. * * @param string $message * @param array $context * @return null */ public function error($message, array $context = array()); /** * Exceptional occurrences that are not errors. * * Example: Use of deprecated APIs, poor use of an API, undesirable things * that are not necessarily wrong. * * @param string $message * @param array $context * @return null */ public function warning($message, array $context = array()); /** * Normal but significant events. * * @param string $message * @param array $context * @return null */ public function notice($message, array $context = array()); /** * Interesting events. * * Example: User logs in, SQL logs. * * @param string $message * @param array $context * @return null */ public function info($message, array $context = array()); /** * Detailed debug information. * * @param string $message * @param array $context * @return null */ public function debug($message, array $context = array()); /** * Logs with an arbitrary level. * * @param mixed $level * @param string $message * @param array $context * @return null */ public function log($level, $message, array $context = array()); } 4. Psr\\Log\\LoggerAwareInterface # \u0026lt;?php namespace Psr\\Log; /** * Describes a logger-aware instance */ interface LoggerAwareInterface { /** * Sets a logger instance on the object * * @param LoggerInterface $logger * @return null */ public function setLogger(LoggerInterface $logger); } 5. Psr\\Log\\LogLevel # \u0026lt;?php namespace Psr\\Log; /** * Describes log levels */ class LogLevel { const EMERGENCY = 'emergency'; const ALERT = 'alert'; const CRITICAL = 'critical'; const ERROR = 'error'; const WARNING = 'warning'; const NOTICE = 'notice'; const INFO = 'info'; const DEBUG = 'debug'; } ","date":"2014-01-05","permalink":"/n3xtchen/2014/01/05/php-fig---psr-3/","section":"时间线","summary":"译自 http://www.","title":"PHP Fig - PSR-3 日志接口（Logger Interface）"},{"content":" 译自 http://www.php-fig.org/psr/psr-4/\n1. 概况 # 这个 PSR 描述的是通过文件路径自动载入类的指南；它作为对 PSR-0 的补充；根据这个 指导如何规范存放文件来自动载入；\n2. 说明（Specification） # 1. 类是一个泛称；它包含类，结构，traits 以及其他类似的结构； # 2. 完整的类名应该类似如下范例： # \\\u0026lt;NamespaceName\u0026gt;(\\\u0026lt;SubNamespaceNames\u0026gt;)*\\\u0026lt;ClassName\u0026gt; 每一个命名空间必须有一个顶级命名空间（Vendor Name）； 每一个命名空间都可以有多个子命名空间； 每一个命名空间的分隔符都必须转化成 DIRECTORY_SEPARATOR，当它被文件系统载入时； 每一个在类名中的 _ 符号都会被转化成 DIRECTORY_SEPARATOR。而 _ 符号在 命名空间中没有任何意义； 完整的的命名空间和类当从文件系统中被加载的时候，都会以 .php 为后缀； Vendor 名，命名空间和类名中的英文字符可以是任意大小写的组合； 3. 当从完整的类名载入文件时： # 一个连续的一个或多个主命名空间和子命名空间名称，不包括主命名空间分隔符， 在完全限定类名（一个“命名空间前缀”）必须对应于至少一个“基本目录”。 在“命名空间前缀”后的连续子命名空间名称对应的子目录中的“基本目录”，其中的命名 空间分隔符表示目录分隔符；子目录名称必须匹配的子命名空间名称； 最后的类名应该和 php 文件名匹配；文件名的大小写必须匹配； 4. 自动载入器的实现不能抛出任何异常，不能抛出任何等级的错误；也不能返回值； # 3. 范例 # 如下表格展示的是完整的类名与其中相关文件路径的关系：\n完整的类名 命名空间前缀 基础目录 实际的类文件路径 \\Acme\\Log\\Writer\\File_Writer Acme\\Log\\Writer ./acme-log-writer/lib/ ./acme-log-writer/lib/File_Writer.php \\Aura\\Web\\Response\\Status Aura\\Web /path/to/aura-web/src/ /path/to/aura-web/src/Response/Status.php \\Symfony\\Core\\Request Symfony\\Core ./vendor/Symfony/Core/ ./vendor/Symfony/Core/Request.php \\Zend\\Acl Zend /usr/includes/Zend/ /usr/includes/Zend/Acl.php 例子中的自动载入器非常适应这个指南，请查阅 examples file ；但是他不能作为指南的一部分；可能随时被改变；\n","date":"2014-01-05","permalink":"/n3xtchen/2014/01/05/php-fig---psr-4/","section":"时间线","summary":"译自 http://www.","title":"PHP FIG - PSR-4 自动载入（Autoloader）"},{"content":" 什么是 Pig? # Pig 提供了一个引擎在 Hadoop 集群上并行执行数据流。他包含一个语言，Pig Latin， 用来表达这些数据流。Pig Latin 可以执行一些传统的数据的操作（join, sort, filter 等等），同时也能让用户开发自自己的函数用来读，处理和写数据。\nHello, MapReduce # 这边使用简单的单词计数程序演示 MapReduce。例子中，map 读取文本的每一行；然后拆成 一个词；shuffle 使用这些单词作为 key，并哈希化后传递给 reducers；reduce 将会汇总 起来计算它们出现的频度。这里有有一首打油诗 《Mary Had a Little Lamb》:\nMary had a little lamb\nits fleece was white as a snow\nand everywhere that Mary went\nthe lamb was sure to go\n假设每一行发给不同的 map 任务。实际上，每一个 map 任务都会分配比这多的多的数据； 只是为了便于举例。\n一旦 map 阶段完成，shuffle 阶段将会把相同的单词手机到相同的 reducer。这个例子，我 们假设有两个 reducers：所有单词分成两个：A-L 和 M-Z。每一个单词都将输出汇总个数。\nPig 使用 MapReduce 执行所有的数据处理。它把用户编写的 Pig Latin 脚本编译成一系列 一个或多个 MapReduce 构成的数据流，然后执行它。\n-- 载入名为 mary.txt 的文件 -- 把每一行命名为 line my_input = load 'mary.txt' as (line) -- TOKENIZE 将一行拆分成单词 -- flatten 将会获取 TOKENIZE 返回的集合，并为每个单词产生一条记录，并把他命名 -- 为word words = foreach my_input generate flatten(TOKENIZE(line)) as word; -- 现在按每一个单词分组 grpd = group words by word -- 计数 cntd = foreach grpd generate group, COUNT(words); -- 打印结果 dump cntd 当使用 Pig 时，我们不需要关心 map，shuffle 和 reduce。他将会分解操作到合适的任务中。\nPig Latin，一个并行的数据流语言 # Pig Latin 是一个数据流语言。这意味着它允许用户描述数据应该怎么被读取，处理，然后 存储。这些数据流可以是线性的，就象上一个例子；它也可能很复杂，包含多个输入源连接 ，根据不同的操作输出给不同的输出源。为了精确（数学），Pig Latin 脚本可以被描述为 有向非循环图（DAG），边缘是数据流，节点是操作。\n这意味着 Pig Latin 看起来和大部分语言不一样。在 Pig 中，没有 if, 没有 for。因为 传统的过程式或对象式编程都是控制流式的，而数据流是程序的副作用。 相反，Pig Latin 集中数据流。\n对比查询和数据流语言 # 大部分人都认为 Pig Latin 是 SQL 的过程式版本。虽然他们很相似，但是还是有很多不同 的。SQL 是允许用户描述他们想要的答案，而不是描述如何作答；而 Pig Latin 是需要用户 描述如何处理输入的信息。\n另一个不同是SQL是面向回答问题的。当用户想要同时做几个数据操作时，他们必须分开写查 询语句，将结果存在临时表中，或用子查询。然而，很多 SQL 用户发现子查询很迷惑忍，而 且难以合适地编写。\nPig 是将一长串的数据操作在脑中设计，因此他们不需要在倒置的写数据管道或者担心存储 数据到临时的表中。\n假设一个案例，用户想要为一张表基于一个键分组，然后和第二张表连接查询。因为连接查 询发生在分组查询之后的，它必须被表达为子查询，或者两个查询（使用临时表存储中间结 果）。\nSQL 实现:\nCREATE TEMP TABLE t1 AS SELECT customer, sum(purchase) AS total_purchases FROM transations GROUP BY customer; SELECT customer, total_purchases, zipcode FROM t1, customer_profile WHERE t1.customer = customer_profile.customer; Pig 实现:\n-- 载入 transactions 文件，以用户分组，汇总他的付款 txns = load 'transactions' as (customer, purchase); grouped = group txns by customer; total = foreach grouped generate group, SUM(txns.purchase) as tp; -- 载入 customer_profile 文件 profile = load 'customer_profile' as (customer, zipcode); -- 连接分组汇总后的交易记录和用户档案数据 answear = join total by group, profile by customer; -- 把结果写到屏幕上 dump answear; 然而，他们各自设计的场景是不同的。SQL 是为关系数据库设计的，他存储的数据需要范式 化（normalized），模式（schema）以及适当的限制（constraints）是强制的。Pig 是为 Hadoop 设计的，有时它的模式是未知或者反常的。他可能不能被适当的现实，很少被范式化 。\n一个关于人类语言和文化的例子可能会帮助你理解这些。我的妻子和我一起去法国一段时间 了。我平时很少说法语。但因为英语是贸易通用语言（也可能是因为美国和英国喜欢到法 国度假的缘故吧，^_^），所以在法国使用英语并没有障碍。相反，我的妻子经常说发育。她 经常有朋友来玩。她可以和这些人沟通。他可以理解法语不仅仅限于旅游用语。她在法国的 经历肯定要比我深刻多了，因为他会说本地语言。\nSQL 是数据处理世界中的英语。他有很多优雅的特性，而且很多工具都了解他，因此他的入 门门槛非常低。而我们要使 Pig Latin 成为并行数据处理系统（例如，Hadoop）中的当地语 言。他可能学习曲线有点抖，但是它让我们更能利用 Hadoop 的本身优点。\nPig 与 MapReduce 的区别 # Pig 提供给用户在 MapReduce 之上的优势。Pig Latin 提供了所有标准的数据操作，例如 Join，filter，group by，order by，union 等等。MapReduce 提供的只是 group by 的操作 （他是实现是 shuffle ＋ reduce），他提供的 Order by 操作是简介利用 group 来实现的 。 Filter 和 展示勉强使用 Map 来实现。而且其他操作，尤其是 join，是不被提供的，必 须用户自己实现。\nPig 提供了一些复杂完整数据操作的实现方法。例如，由于数据集中的每个键的记录很少被分 发，被发到 reducers 的数据经常被扭曲。也就是说，一个reducer获取的数据可能是其他 reducer 的 10倍甚至更多。Pig 的 join 和 order by 操作就是为了解决这个问题的，来 均衡各个 reducers。但是，Pig 中的实现和 MapReduce 的实现是同样耗时的。\n在 MapReduce 中，Map 和 Reduce 中的数据处理对于系统来说是很不透明的。这意味着 MapReduce 没有机会优化和检查用户的代码。相反，Pig可以分析 Pig Latin，理解用户描述 的数据流。这样就可以尽早地做错误检查（例如用户尝试把字符串添加到整型中）和代码优化 （例如 两个分组操作是否能合并）。\nMapReduce 没有类型系统。这是有意而为之的，这样给了用户足够的灵活来使用他们自己的 数据类型和序列化工作。但是负面的影响就是很大程度上限制了系统检查用户代码错误的能力 。\n所有的这些观点都证明了 Pig Latin 在编写和维护上都比使用 JAVA 编写的 MapReduce 耗 费更低的成本。在一个科学非常不严谨的实验中，我同时用 Pig Latin 和 MapReduce 编写 了同样的操作。代码如下\n-- 载入 users 文件，包含两个字段：name，age Users = load 'users' as (name, age); -- 筛选用户年龄为 18岁到25岁 之间 Fltrd = filter Users by age \u0026gt;= 18 and age \u0026lt;= 25; -- 载入 pages 文件，也包含两个字段：user，url Page = load 'pages' as (user, url); -- 连接 Fltrd(name) 和 Page(user) 数据 Jnd = join Fltrd by name, Page by user; -- 以 url 来分组 Grpd = group Jnd by url; -- 汇总各个 url 的用户数 Smmd = foreach Grpd generate group, COUNT(Jnd) as clicks; -- 以 clicks 降序排列 Srtd = order Smmd by clicks desc; -- 取前五条 Top5 = limit Srtd 5; -- 并将结果存储到 top5sites 文件中 store Top5 into 'top5sites'; 在 Pig Latin 中，使用了 9 行代码，大概花了 15分钟的时间来编写和调试。同样的在 MapReduce 中实现大概需要 170 行的代码，并发了 4 个小时让他正常运行。Pig Latin 将 更加的抑郁维护，对于未来开发者来说更容易理解和修改代码。\n当然，如果在算法开发上来说，Pig Latin 并不擅长。假设被给予足够的开发时间，一个好 的工程师永远能写出在通用系统运行性能良好的代码。对于非通用算法或者性能极端敏感的 开发， MapReduce 仍然是最好的选择。那同样的情况下，使用 JAVA 和 其他脚本语言（ 例如，Python）又有什么区别呢？Java 更加强大，但是由于相对比较底层，他需要更长的开 发周期。因此，开发需要针对不同的工作选择不同的工具。\nPig 的适用场景 # 以我个人经验而言，Pig 的使用场景大致有三个：传统的 ETL(Etract transform load) 数据管道（Data Pipeline），原始数据(Row Data)研究和迭代处理。\nETL 数据管道 # 最大的使用场景要属数据管道了。\nETL 用例1：数据清理和整合 # 在入库前，分析访问日志，净化数据和对数据的预处理。 例如，数据载入集群中， 使用 Pig 清理来自各个点的脏数据。它也可以用于将 Web 日志与用户数据库数据整合，以至于可以将用户 cookie 和用户信息相互关联。\nETL 用例2：离线计算创建行为预测模型 # 使用 Pig 检索用户的网页交互，将用户分割成不同的群体。然后，根据不同的属性， 产生相应的数据模型，来预测如何促进用户的访问。使用这种方法， 来决策使用哪一类广告能更能促进用户，哪一类故事更能促进用户回访。\n原始数据（Raw Data）研究 # 传统来看，即席查询（ad-hoc queries，比如 SQL）能更简单快速地分析数据。 然而 ，对于 Raw Data 的研究，一些用户更喜欢使用 Pig Latin；因为 Raw Data 的结构 常常不可预知，可能不完成，甚至反常态，而 Pig 能够很好的处理嵌套的数据结构， ，这是 SQL 所不擅长的。因此，使用 Pig 来处理未清理和载入数据仓库的数据再合适 不够了。\n另外，处理大数据的研究员经常使用 Perl 或者 Python 这类脚本语言来完成他们的处 理。有这些技术背景的用户更喜欢 Pig 的数据流风格而不是 SQL 的声明式查询风格。\n数据迭代处理 # 建立迭代处理模型的用户也正在转向使用 Pig。考虑到新网站需要追踪所有的新故事的访问情况。 在这个图谱中，每一个新故事（story）都是一个节点（node），连接线（edges）用来指明两个 故事之间（story）的关系。例如，所有关于即将到来的选举的故事被联系在一块。每隔5分钟， 都会有新的故事进入，数据处理引擎必须将这些都整合进图谱来。一些故事是新的，一些是更新 新故事的，一些是替代存在的故事的。一些数据操作需要在整个故事图谱中操作的。例如，一个 处理行为标记模型的处理需要将用户信息与整个故事图谱进行关联。每5分钟重新进行整体连表 是不可能，因为在合理硬件情况下，是不可能在 5分钟内处理完它的。但是，模型创建者只在 每日一次更新他们模型，因为这样意味着失去一整天的机会点。\n为了解决这些问题，他可能首先在常规的基础上做一次所有的图谱关联，例如，每天。然后， 每五分钟做一次对新数据的关联，这些结果会合并到整个图谱关联产生的结果中。这个合并需要 几点注意，因为每五分钟的数据包含了在整个图谱中插入，更新和删除。这种合并可以使用 Pig Latin 可以并且很合理地进行表达。\n把之前说的都归纳到一点就是 Pig（象 MapReduce）是面向数据批处理的。如果需要处理海量的 数据，Pig 是很好的选择。但是，它要求读一个文件的所有记录，并按一定顺序来输出结果。对于 需要写单一或者数据一小部分，或者按随机顺序查找许多不同记录的操作，Pig 就不适用了。\nPig 哲学 # Pigs 吃一切可以吃的东西 # Pig 可以处理的数据不管是不是元数据。它可以操作关联的，内嵌的，或者无结构的数据。并且 它可以通过简单拓展来操作任何形式的数据。\nPigs 无处不在 # Pig 更像并行数据操作世界的一种语言。他并不是绑定给某一特定并行框架。首先，它可以用于 Hadoop，但我们不仅仅希望只用于 Hadoop。\nPigs 是家畜 # Pig 被设计成可以简单地被用户控制和修改。\n它允许与用户代码整合，目前，它支持用户定义字段转换方法，用户定义的聚合操作以及用户定义 的条件。这些方法可以直接使用 Java 或者可以编译成 java 代码的脚本语言来编写。Pig 还支持 用户提供的数据载入和存储的方法。他支持通过它的 stream 命令在外部调用，以及通过 MapReduce 命令来操作它。他还允许用户在一些特定环境提供自定义分区或者设置 rduce 层次。\nPig 可以通过重新安排一些操作来优化性能。也很容易关掉他们。\n飞猪在天 # Pig 可以快速的处理数据。我们总是要提高它的性能，不会加入一些使它变重的功能。\nPig 名称的由来 # “它为什么叫做 Pig？” 这个问题被问道的很频繁。人们也想知道 Pig 是不是一个宏语言。它不是。 整个故事是这样的；这个项目的研究人员最初只是简单地称它为 “Language”。最终，还是要给它取 个名字。一个研究人员突发奇想想出了 Pig 这个名称，起初大家都不接受。但是它出奇地好记而且容易 拼写。然而一部分人认为听起来难以出口甚至联想到愚蠢，他们为我们提供一个有趣的命名，例如， Pig Latin 是语言， Grunt 是shell脚本，和 Piggybank 是 类 CPan 的分享资源库。\n","date":"2013-12-31","permalink":"/n3xtchen/2013/12/31/hadoop---data-pig/","section":"时间线","summary":"什么是 Pig?","title":"Data Pig - Pig 简介"},{"content":"","date":"2013-11-25","permalink":"/n3xtchen/tags/django/","section":"标签","summary":"","title":"django"},{"content":" 一.创建项目 # $ django-admin.py startproject proj_name $ tree -L 2 proj_name ├── proj_name │ ├── __init__.py │ ├── __init__.pyc │ ├── settings.py │ ├── settings.pyc │ ├── urls.py │ └── wsgi.py └── manage.py 启动服务器 $ cd proj_name $ python manage.py runserver\n二.数据库配置 # $ vim proj_name/setting.py DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': 'database.db', 'USER': '', 'PASSWORD': '', 'HOST': '', 'PORT': '', } } 三.创建模块 # $ python manager.py startapp app_name $ tree -L 2 proj_name ├── proj_name │ ├── __init__.py │ ├── __init__.pyc │ ├── settings.py │ ├── settings.pyc │ ├── urls.py │ └── wsgi.py ├── app_name │ ├── __init__.py │ ├── models.py │ ├── tests.py │ └── views.py └── manage.py 四.激活模块 # $ vim proj_name/settings.py INSTALLED_APPS = ( 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.sites', 'django.contrib.messages', 'django.contrib.staticfiles', # Uncomment the next line to enable the admin: # 'django.contrib.admin', # Uncomment the next line to enable admin documentation: # 'django.contrib.admindocs', 'app_name', // 添加这一行 ) $ vim proj_name/urls.py url(r'^app_name/', include(app_name.urls)) $ vim app_name/urls.py # app_name/urls.py from django.conf.urls import patterns, include, url urlpatterns = patterns('', ) 五.创建自己的模型(Models) # $ vim app_name.models.py from django.db import models # Create your models here. class Blog(models.Model): title = models.CharField(max_length=50) content = models.TextField() ","date":"2013-11-25","permalink":"/n3xtchen/2013/11/25/python-django---simple/","section":"时间线","summary":"一.","title":"Python Django - simple guide"},{"content":" Ruby # Ruby 是一种拥有复杂的极具表达力语法的动态语言，还附带丰富强大接口的核心类库。 Ruby 启发于 Lisp，Smalltalk 和 Perl，并且使用便于 C 和 JAVA 程序学习的语法。 Ruby 是纯面向对象的语言，但也适用于过程和函数式编程。它涵盖强大的元编程，也 可以用于创建适用特殊领域的语言(又称 DSL)。\nMatz # Yukihiro Matsumoto，以网名 Matz 闻名于英语 Ruby 论坛，是 Ruby 语言的创始人，\nRuby is desingned to make programmers happy - Matz\u0026rsquo;s guiding philosophy\n启动 Ruby 解释器 # $ irb 1.9.3-p448 :001 \u0026gt; # 这是我机子ruby的版本 Ruby 是面向对象的(Object-Oriented) # 在 Ruby 的世界里，任何值都是对象，即使是简单的数值，布尔值 甚至是空值(nil 是 一个特殊的值，它表示一个缺省值 ；它是 Ruby 版的 Null)。\n在 Ruby 中，注释(comment) 使用 #；在注视仲， =\u0026gt; 箭头表示所注视的代码的返回 回值说明。\n1.class # =\u0026gt; Fixnum: 整型 0.0.class # =\u0026gt; Float: 浮点型 true.class # =\u0026gt; TrueClass: 真 false.class # =\u0026gt; FalseClass: 假 nil.class # =\u0026gt; NilClass: 空 在很多语言中，函数和方法调用需要圆括号，但是在上述代码仲没有包含任何的括号。 在 Ruby 中，括号时常是可选的，通常可以忽略，尤其是调用时不需要传参数的方法。 事实上，括号是可忽略的，使得他们看起来象字段索引或者对象变量。这个是作者有意 的而为之，但是 Ruby 在对象的封装上非常严格；它不允许外部访问对象内部的状态。 如果真需要访问，必须指定访问函数。\n块(Blocks)和迭代(Iterators) # 事实上，在数字上调用方法并不是 Ruby 深奥的一面。\n3.times { print \u0026quot;Ruby\u0026quot; } # 打印RubyRubyRuby =\u0026gt; 3 1.upto(9) { |x| print x } # 打印123456789 =\u0026gt; 1 times, upto 他们是特殊的方法，这样的调用，我们称之为跌代器(Iterators)。\n花括号中的代码, 就称作块(Block)。\n当然，整型不是唯一拥有跌代器的值。数组(类似枚举对象)也定义了迭代器(each)。\nlist = [3, 2, 1, 0] # =\u0026gt; [3, 2, 1, 0] list.each do |item| print item+1 end # 打印 4321 =\u0026gt; [3, 2, 1, 0] 还有许多定义在 each 之上的跌代器:\na = [1, 2, 3, 4] # =\u0026gt; [1, 2, 3, 4] a.map { |x| x*x } # =\u0026gt; [1, 4, 9, 16] a.select { |x| x%2==0 } # 选择器 =\u0026gt; [2, 4] a.inject do |sum, x| sum + x end # 计算 =\u0026gt; 10 哈希(Hashes)，类似与数组，它也是 Ruby 的基础数据类型。顾名思义，他们是基于哈 希表的数据结构，将特定的键(Key)对象映射到值(Value)对象。\nh = { :one =\u0026gt; 1, :two =\u0026gt; 2 } # 创建一个哈希数组 =\u0026gt; {:one=\u0026gt;1, :two=\u0026gt;2} h[:one] ＃ 取值,和数组使用方法相似 =\u0026gt; 1 h[:three] = 3 # 追加指,也类似数组 =\u0026gt; 3 h.each do |key, value| print \u0026quot;#{key}, #{value};\u0026quot; end # 打印 one, 1;two, 2;three, 3; =\u0026gt; {:one=\u0026gt;1, :two=\u0026gt;2, :three=\u0026gt;3} 双引号字符串中包含 Ruby 表达式，它由 #{ 和 } 包围；表达式中的值将被转化成字 符型(通过调用它们的 to_s 方法，所有的对象都有这个方法。)。\n这种表示值替换成字符的过程，我们通常称之为字符插入(string interpolation).\nEffective Ruby # 回忆下学习到的基础数据类型：\n整形\n浮点型\n真类\n假类\n字符串\n数组\n哈希\n目前学习到的迭代器:\n整形\ntimes\nupto\n数组/哈希\neach\nmap\nselect\ninject\nRuby 中的表达式和操作符 # Ruby 的语法是面对表达式的(expression-oriented)。控制结构例如 if 在其他语言中 叫做声明语句(Statements)，但在 Ruby 实际上就是表达式。\nminimun = if x \u0026lt; y then x else y end 虽说所有的语句在 Ruby 中都是表达式，但是它们并不是都返回有意义的值。例如， while 循环和方法定义是表达式，它一般返回 nil 值。\n和其他语言一样，Ruby 表达式由值和操作符(Operator)组成。大部分操作符和 C, Java 系 的语言类似。\n1 + 1 # =\u0026gt; 2 1 * 2 # =\u0026gt; 2 1 + 2 == 3 # 判断 =\u0026gt; true 2 ** 8 # N次方 =\u0026gt; 256 \u0026quot;cool\u0026quot; + \u0026quot; Ruby\u0026quot; # 字符拼接 =\u0026gt; \u0026quot;cool Ruby\u0026quot; \u0026quot;Ruby\u0026quot; * 3 # 重复字符，注意和数字不同 =\u0026gt; \u0026quot;RubyRubyRuby\u0026quot; \u0026quot;%d %s\u0026quot; % [2, 'rubies'] # 格式化字符 =\u0026gt; \u0026quot;2 rubies\u0026quot; 很多操作符都是作为方法实现的，类可以根据你的想法定义(重定义)这些方法。* 就是个 很好的例子，在字符和数值的行为完全不同。\u0026lt;\u0026lt; 也是一个很好的例子；整型使用它进行 移位操作，这个和 C 的语法一致。与此同时，象 C++ 一样，string，数组和流(stream) 可以使用这个操作符进行追加操作。\n还有一个可以覆盖的强大的操作符就是 []。 在数组和哈希类中用这个来访问元素。但是 你能在你的类中重定义它。\n方法(methods) # 使用 def 定义方法；方法的返回值是该方法定义块的最后一个表达式。\ndef sayHiTo(name) print \u0026quot;Hi, \u0026quot;+name true end # =\u0026gt; nil sayHiTo('N3xtchen') # 打印 Hi, N3xtchen =\u0026gt; true 上述方法定义在类和模块的外部，他的影响范围是全局的。(从技术角度来说，方法应该作用在对象 中比较科学。)\n如何在外部定义类和模块的方法的，(当然，这些类和模块是开放的，允许在运行时外部被修改)?我们 可以通过方法冠以类名，使用 . 来分隔。\ndef Math.square(x) x*x end Math 是 Ruby 集成的核心库，它允许外部给他添加新方法。\n方法参数可以制定默认值，它智能接受指定个数的参数。\n赋值(Assignment) # x =1 # =\u0026gt; 1 x += 1 # 可以和操作符组合使用 =\u0026gt; 2 x -= 1 # =\u0026gt; 1 x, y = 1, 2 # =\u0026gt; [1, 2] x, y = y, x # 变量交换值 =\u0026gt; [2, 1] x, y, z = [1, 2, 3] # =\u0026gt; [1, 2, 3] x = Math.square(2) # 调用函数赋值 =\u0026gt; 4 Math.x = 1 # 给对象属性赋值 =\u0026gt; 1 前置和后置标点(Punctuation Suffixes and Prefixs) # Ruby 方法可以以问好和感叹号结尾。\n例如，数组和哈希都有定义了一个名为 empty? 的方法，用来用来测试该数据是否元素。\n感叹号结束的方法用来表明使用该方法时需要特别小心。很多 RUBY 类库定义一组同名 的方法，除了其中一个方法已感叹号结束，另一个没有。通常，没有感叹号的方法返回 调用所在对象的可变拷贝，而带感叹号的是原对象的本身的方法，改变它会改变原对象 。例如，数组对象定义的 sort 和 sort!\n除了这些，你将注意到变量的前导符号:\n全局变量 $ 对象属性 @ 类变量 @@ 刚开始使用这些东西可能会有一些不习惯，如果习惯了，你将会非常乐意这些前缀告诉 你变量的作用域。这些前缀旨在消失 Ruby 非常灵活语法的歧义。从另一个方面看，前 缀是我们可以区分调用不带括号的是变量还是方法。\n正则(Regexp) 和 范围(Range) # 先前，我们提到了数组和哈希作为 Ruby 的基础数据类型；还演示了字符和数字的方法。还有两个数据 类型值得关注。\n正则表达式(Regular Expression) 用来描述文本模型，并拥有匹配的函数。正则表达式由 / 包围。 注意，匹配的是字符型。\n范围(Range) 呈现的是值是否落在某个区间, 匹配的是数值。\n他们都是使用 === 测试匹配，返回布尔值。\n# 正则 /[Rr]uby/ === \u0026quot;Ruby\u0026quot; # 匹配成功 =\u0026gt; true /[Rr]uby/ === \u0026quot;ruby\u0026quot; # 匹配成功 =\u0026gt; true /[Rr]uby/ === \u0026quot;wrong\u0026quot; # 匹配失败 =\u0026gt; false /\\d{5}/ === \u0026quot;11111\u0026quot; # 匹配成功 =\u0026gt; True # 范围 range = 1..3 # 大等于1，小等于3 =\u0026gt; 1..3 range === 1 # =\u0026gt; true range === 3 # =\u0026gt; true range === 0 # =\u0026gt; false range === 4 # =\u0026gt; false range = 1...3 # 大等于1，小于3 =\u0026gt; 1...3 range === 1 # =\u0026gt; true range === 3 # =\u0026gt; false range === 0 # =\u0026gt; false range === 4 # =\u0026gt; false 这里，我们可能会用到 Case 条件语句(和 C 的 Swtich 语句类似)。 现在我们可以尝试写一个小程序:\n# Short description for generation.rb # # @package case # @author n3xtchen \u0026lt;echenwen@gmail.com\u0026gt; # @version 0.1 # @created in 2013-11-08 14:46 def enterYouBirthYear? while true print \u0026quot;Enter Your Birth Year[yyyy]: \u0026quot; year = gets case year when /\\d{4}/ return year.to_i end end end def whichGeneration(year) generation = case year when 1946..1963 \u0026quot;Baby Boomer\u0026quot; when 1964..1976 \u0026quot;Generation X\u0026quot; when 1978..2000 \u0026quot;Generation Y\u0026quot; else nill end end year = enterYouBirthYear? print \u0026quot;I am born in #{year}!\u0026quot; generation = whichGeneration(year) print \u0026quot;I am \u0026quot;+generation+\u0026quot;!\u0026quot; 运行它！(使用 Ruby + 程序文件名来执行 ruby 程序。)\n$ ruby generation.rb Enter Your Birth Year[yyyy]: 1987 I am born in 1987!I am Generation Y! 类(Classes)和模块(Modules) # 一个类是相关操作一个对象状态方法的集合。一个对象状态在对象中以对象属性的形式 存在：这些变量以 @ 开头。\n# # Short description for Gi.rb # # @package Gi # @author n3xtchen \u0026lt;echenwen@gmail.com\u0026gt; # @version 0.1 # @created in 2013-11-08 23:25 class Gi # 构造函数 def initialize(name, hobbies) @name, @hobbies = name, hobbies end def like(new_interest) @hobbies += \u0026quot;,\u0026quot; + new_interest end def hobbies @hobbies end end gi = Gi.new(\u0026quot;n3xtchen\u0026quot;, \u0026quot;Python, PHP\u0026quot;) gi.like(\u0026quot;Ruby\u0026quot;) print gi.hobbies+\u0026quot;\\n\u0026quot; 运行它！\n$ ruby Gi.rb Python, PHP,Ruby 如果我们只兴趣方法，那我们不需要定义完整的类。 模块(Modules)是相关操作的集合 。\nMoudle FuncCollection def fun1 # do something end end Ruby 额外特性 # 每个语言都有自己的特性，让程序员喜欢上它，然后就离不开它了。这里我们描述两个 惊人的特性。\nRuby 的字符串是可变的，这可能对 Java 程序非常吃惊。\n[]= 操作允许你访问字符串的每一个自负，插入，删除和替换子串。 \u0026lt;\u0026lt; 追加字符。 字符串字画量在程序中不是唯一的对象。例如轮询时，每次迭代都会产生一个新对象。 freeze 方法阻止对象将来被变动。 条件和循环语句中，除了 nil 被当作 false 对待，其他任何类型的值都是 true。\n使用 gem 管理 Ruby 包 # gem list # List installed gems gem enviroment # Display RubyGems configuration information # Update a named gem gem update rails gem update # Update all installed gems gem update --system # Update RubyGems itself gem uninstall rails # Remove an installed gem It\u0026rsquo;s Over # ","date":"2013-11-07","permalink":"/n3xtchen/2013/11/07/ruby-learning/","section":"时间线","summary":"Ruby # Ruby 是一种拥有复杂的极具表达力语法的动态语言，还附带丰富强大接口的核心类库。 Ruby 启发于 Lisp，Smalltalk 和 Perl，并且使用便于 C 和 JAVA 程序学习的语法。 Ruby 是纯面向对象的语言，但也适用于过程和函数式编程。它涵盖强大的元编程，也 可以用于创建适用特殊领域的语言(又称 DSL)。","title":"Coding in Ruby - Starter"},{"content":"","date":"2013-11-07","permalink":"/n3xtchen/tags/learning/","section":"标签","summary":"","title":"learning"},{"content":"","date":"2013-10-27","permalink":"/n3xtchen/tags/nertree/","section":"标签","summary":"","title":"NERTree"},{"content":" 常见命令 # o 打开文件(夹) O 递归打开当前节点的所有子节点 i 水平分割屏打开文件 s 垂直分割屏打开文件 x 关闭文件 P 跳到根节点 p 跳到当前父节点 K 跳到当前节点开始位置 J 跳到当前节点结束位置 C 将选中的目录作为根节点 u 将当前根节点的父目录作为根节点 r 刷新当前目录 R 算新当前根节点 I 切换隐藏文件的显示 F 切换文件的现实 q 关闭 A 最大化/最小化目录窗口 ? 打开帮助 ","date":"2013-10-27","permalink":"/n3xtchen/2013/10/27/vim-nertree/","section":"时间线","summary":"常见命令 # o 打开文件(夹) O 递归打开当前节点的所有子节点 i 水平分割屏打开文件 s 垂直分割屏打开文件 x 关闭文件 P 跳到根节点 p 跳到当前父节点 K 跳到当前节点开始位置 J 跳到当前节点结束位置 C 将选中的目录作为根节点 u 将当前根节点的父目录作为根节点 r 刷新当前目录 R 算新当前根节点 I 切换隐藏文件的显示 F 切换文件的现实 q 关闭 A 最大化/最小化目录窗口 ?","title":"Vim Tip - 树形目录(NERTree)"},{"content":"","date":"2013-10-26","permalink":"/n3xtchen/tags/folding/","section":"标签","summary":"","title":"folding"},{"content":" 基本命令 # zo 打开当前光标下的折叠 zO 打开当前光标下的所有折叠 zc 关闭当前光标下的折叠 zC 关闭当前光标下的所有折叠 za zo/zc zA zO/zC zm 关闭一级折叠层(foldlevel) zM 关闭所有的折叠层 zr 打开一级折叠层 zR 打开所有折叠层 zj 移到当前光标的下一个折叠 zk 移到当前光标的上一个折叠 [z 移到当前打开折叠的起始位置 ]z 移到当前打开折叠的结束位置 \u0026quot; 折叠方式为 manual 或 marker时，下面命令可用 zf{motion}/{Visual}zf 创建一个折叠 {n}ZF 创建当前光标下的n行的折叠 :{range}fo[ld] 创建范围行内的 zd 删除当前坐标折叠 zD 删除当前坐标中的所有折叠 zE 删除全部的折叠 折叠规则 # \u0026quot; 手动折叠,使用 zf/zF/zd/zD/zE 自定义折叠规则 :set foldmethod=manual \u0026quot; 语法折叠，根据相应编程语言的语法自动折叠 :set foldmethod=syntax \u0026quot; 缩进折叠,根据缩进层次(shiftwidth)自动锁进 :set foldmethod=indent \u0026quot; 特定标记折叠，所有文本将按照特定标记（默认为\\{\\{\\{和\\}\\}\\}）自动折叠。 :set foldmethod=marker ","date":"2013-10-26","permalink":"/n3xtchen/2013/10/26/vim---fold/","section":"时间线","summary":" 基本命令 # zo 打开当前光标下的折叠 zO 打开当前光标下的所有折叠 zc 关闭当前光标下的折叠 zC 关闭当前光标下的所有折叠 za zo/zc zA zO/zC zm 关闭一级折叠层(foldlevel) zM 关闭所有的折叠层 zr 打开一级折叠层 zR 打开所有折叠层 zj 移到当前光标的下一个折叠 zk 移到当前光标的上一个折叠 [z 移到当前打开折叠的起始位置 ]z 移到当前打开折叠的结束位置 \u0026quot; 折叠方式为 manual 或 marker时，下面命令可用 zf{motion}/{Visual}zf 创建一个折叠 {n}ZF 创建当前光标下的n行的折叠 :{range}fo[ld] 创建范围行内的 zd 删除当前坐标折叠 zD 删除当前坐标中的所有折叠 zE 删除全部的折叠 折叠规则 # \u0026quot; 手动折叠,使用 zf/zF/zd/zD/zE 自定义折叠规则 :set foldmethod=manual \u0026quot; 语法折叠，根据相应编程语言的语法自动折叠 :set foldmethod=syntax \u0026quot; 缩进折叠,根据缩进层次(shiftwidth)自动锁进 :set foldmethod=indent \u0026quot; 特定标记折叠，所有文本将按照特定标记（默认为\\{\\{\\{和\\}\\}\\}）自动折叠。 :set foldmethod=marker ","title":"Vim Tip - 折叠(Folding)"},{"content":" 系统 # Ubuntu 12.04 X64 编辑器 # Vim 7.3 Vundle Ctag CScope 编程语言及IDE # PHP 5.3.10\npecl Composer php-apc php-curl php-memcahe php-mongo php-redis php-amqp oci8 mysql PDO PDO-MYSQL PDO-OCI PHPUnit php-fpm Python 2.7\nEZ PIP IPython Java 7\nScala\nsbt Ruby 2.0.0\nRVM gem 2.0.3 JavaScript\nPhantonJS NodeJS NVM NPM 服务器 # Apache\nSSI UserDir Nginx\nSSI OpenResty 数据库 # MySQL\nORACLE\nInstantClient 11.1 sqlplus 11.1 Memcache\nRedis\nMongo\n工具类 # Secure Shell Mobile Shell TMux GParted Curl Subversion Git rabbitmq-server VirtualBox ","date":"2013-10-14","permalink":"/n3xtchen/2013/10/14/linux---n3xt-soft-package/","section":"时间线","summary":"系统 # Ubuntu 12.","title":"Linux - 软件列表（自用）"},{"content":"","date":"2013-10-14","permalink":"/n3xtchen/tags/software/","section":"标签","summary":"","title":"software"},{"content":"","date":"2013-10-12","permalink":"/n3xtchen/tags/disk-partion/","section":"标签","summary":"","title":"disk-partion"},{"content":"","date":"2013-10-12","permalink":"/n3xtchen/tags/gnome/","section":"标签","summary":"","title":"gnome"},{"content":" 摘自 http://www.ibm.com/developerworks/cn/linux/l-tip-prompt/tip05/\n/home 是最经常被移动的分区。某些时候，/home 中的全部空间都用完了，而且需要增加 一个硬盘驱动器。另一些时候，/home 被设置为根分区的一部分，为了提高性能或便于备 份，可能需要将它移动到别的地方。\n警告\n下面的技术说明如何移动一个或多个分区。尽管这项技术的设计使您能够“撤销”失败的 分区移动，但它并不防止用户的错误。换言之，只要进行格式化分区或复制大量文件的操 作，就存在因输入错误而导致大量数据被破坏的可能性。因此，强烈建议您 在行动之前采 取适当的措施来备份所有的重要文件。\n1.创建新分区 # 安装完硬盘， 你需要在该硬盘上创建分区；\n你可能需要使用的工具：\n命令行的有：cfdisk 或者 fdisk 图形分区工具：Parted 在创建了适当的主分区或扩展分区以后，应重新启动系统以便正确地重新读取分区表。这是 唯一需要重启系统的时候。\n2.在新分区上创建文件系统 # 假设我们第一步创建了一个分区（路径为：/dev/sdb1）\n$ sudo mkfs.ext4 /dev/sdb1 # 不一定一定要 ext4 3.挂载新分区 # $ sudo mkdir /mnt/new_home $ sudo monnt /dev/sdb1 /mnt/new_home 4.进入单用户 # 在更新新分区的时候，我们必须保证 /home 的所有文件都是关闭状态，没有被打开；进入 单用户模式，就为了消除这一点：\n$ sudo -i # 进入超管用户 password: ****** # init 1 复制原先的 /home 目录文件到新的分区中 # # cp -ax /home/* /mnt/newpart cp -ax 命令循环地将 /home 中的内容复制到 /mnt/newpart 中，并保留全部文件属性， 也不会交叉任何挂载点\n使用新分区 # 原先 /home 目录是一个分区时 # # cd / # umount /home # umount /mnt/new_home # mount /dev/sdb1 /home 重要步骤\n使用超级用户编辑 /etc/fstab\n替换该行：\n/dev/xx /home some_fs defaults 1 2 为\n/dev/sda1 /home ext4 defaults 1 2 原先 /home 目录不是一个分区时 # 假设 /home 目录和根目录在同一分区，也就是说这时，/home 只是一个目录的时候\n# cd / # mv /home /home.old # mkdir /home # mount /dev/sdb1 /home 重要步骤\n使用超级用户编辑 /etc/fstab\n添加该行：\n/dev/sda1 /home ext4 defaults 1 2 7. 扫尾工作 # 我们特意将原来的 /home 目录/分区保留下来，以防复制文件时出现问题。在证实系 统稳定运行以后，您就可以将原来的 /home 分区用于其他目的，或者删除原来的 /home 目录。\n","date":"2013-10-12","permalink":"/n3xtchen/2013/10/12/linux-change-home-partion/","section":"时间线","summary":"摘自 http://www.","title":"Linux - 调整 /home 分区"},{"content":" 摘自 http://blog.csdn.net/x504635/article/details/8210422\n安装 Gnome # sudo apt-get install gnome-session-fallback # or gnome-panel Gnome 设置为默认桌面： # sudo /usr/lib/lightdm/lightdm-set-defaults -s gnome-classic # 如果你喜欢 gnome3 用下面的命令 sudo /usr/lib/lightdm/lightdm-set-defaults -s gnome-shell 卸载 Unity # sudo apt-get -y –auto-remove purge unity sudo apt-get -y –auto-remove purge unity-common sudo apt-get -y –auto-remove purge unity-lens* sudo apt-get -y –auto-remove purge unity-services sudo apt-get -y –auto-remove purge unity-asset-pool ","date":"2013-10-12","permalink":"/n3xtchen/2013/10/12/ubuntu-install-gnome-remove-unity/","section":"时间线","summary":"摘自 http://blog.","title":"Ubuntu 12.04 - 安装 Gnome 卸载 Unity"},{"content":"","date":"2013-10-12","permalink":"/n3xtchen/tags/unity/","section":"标签","summary":"","title":"unity"},{"content":"","date":"2013-10-03","permalink":"/n3xtchen/tags/module/","section":"标签","summary":"","title":"module"},{"content":"","date":"2013-10-03","permalink":"/n3xtchen/tags/mongo/","section":"标签","summary":"","title":"mongo"},{"content":" 连接服务器 # \u0026gt;\u0026gt;\u0026gt; from pymongo import MongoClient \u0026gt;\u0026gt;\u0026gt; client = MongoClient('127.0.0.1', 27017) // 方法 A \u0026gt;\u0026gt;\u0026gt; client = MongoClient('mongodb://localhost:27017/') // 方法 B 进入数据库 # \u0026gt;\u0026gt;\u0026gt; db = client.test_database // 方法 A \u0026gt;\u0026gt;\u0026gt; db = client['test_database'] // 方法 B 进入集合（Collection） # 集合（Collection）是存储在 MongoDB 数据库中的文档组合，可以把它当作关系数据 库中的表。\n\u0026gt;\u0026gt;\u0026gt; collection = db.test_collection // 也可以采用数据调用，和数据访问两种方式一致 还有一点要注意，集合和数据库在 MongoDB 中创建是懒惰的 - 实际上，上述的操作都没有在 MongoDB 中产生影响。集合和数据库只有在第一个文档被插入时才会被创 建。\n文档（Document） # 数据在 MongoDB 中是使用 JSON 风格的文档来呈现的。在 PyMongo 中，我们使用字典 （Dictionary）来呈现文档的。作为例子，我们使用如下字典来呈现一条博客：\n\u0026gt;\u0026gt;\u0026gt; import datetime \u0026gt;\u0026gt;\u0026gt; post = {\u0026quot;author\u0026quot;: \u0026quot;n3xtchen\u0026quot;, ... \u0026quot;text\u0026quot; : \u0026quot;My first blog post!\u0026quot; ... \u0026quot;data\u0026quot; : datatime.datetime.utcnow()} 注意，文档可以包含 Python 原生的类型（例如 datetime.datetime 实例）； 他们会自动与相应的 BSON类型相互转化。\n插入一个文档 # \u0026gt;\u0026gt;\u0026gt; posts = db.posts \u0026gt;\u0026gt;\u0026gt; post_id = posts.insert(post) \u0026gt;\u0026gt;\u0026gt; post_id ObjectId('...') 当创建文档时，\u0026quot;_id\u0026quot; 会自动被加入，如果文档中没有包含的话。\u0026quot;_id\u0026quot; 的值必须 是在一个集合中必须是唯一的。insert() 会返回插入的 \u0026quot;_id\u0026quot;的。\n插入第一个文档后，posts 集合实际上已经在服务器上被创建了。我们可以通过下面 方法来罗列我们数据库的所有集合来验证这一点：\n\u0026gt;\u0026gt;\u0026gt; db.collection_name() [u'system.indexes', u'posts'] 注意： system.indexes 集合是一个特殊的内部集合，它是自动被创建的。\n使用 find_one() 来获取一条文档 # 在 MOngoDB 中最基本的查询方式就是 find_one()。这种方式返回一个符合查询的文 档（或者如果没有匹配的话返回 None）。当你只想要一条匹配的文档或者你只对第 一条感兴趣的时候，这个命令很有用。\n\u0026gt;\u0026gt;\u0026gt; post.fine_one() {u'data': datetime.datetime(...), u'text': u'My first blog post!' ...} 匹配的结果就是我们之前插入的那一条\nfind_one() 也支持指定元素的查询\n\u0026gt;\u0026gt;\u0026gt; post.fine_one({\u0026quot;author\u0026quot;: \u0026quot;n3xtchen\u0026quot;}) {u'data': datetime.datetime(...), u'text': u'My first blog post!' ...} 通过 ObjectId 查询 # 我们也可以 _id 来查询 post。\n\u0026gt;\u0026gt;\u0026gt; post_id ObjectId(...) \u0026gt;\u0026gt;\u0026gt; posts.find_one({\u0026quot;_id\u0026quot;: post_id}) {u'data': datetime.datetime(...), u'text': u'My first blog post!' ...} 注意，ObjectId 和字符串不同\n\u0026gt;\u0026gt;\u0026gt; post_id_as_str str(post_id) \u0026gt;\u0026gt;\u0026gt; posts.find_one({\u0026quot;_id\u0026quot;: post_id_as_str}) # No result 在网页应用中，我会从请求的URL中获取一个 ObjectId 来查询匹配的文档是非常寻常 的事情，所以，如何从传递过来的字符串转化成 ObjectID 是非常必要的：\nfrom bson.objectid import ObjectId def get(post_id): document = cliemt.db.collection.find_one({'_id': ObjectId(post_id)}) Unicode 字符串 # 你可能注意到我们之前存储的是常规的 Python 字符串和从服务器返回的看起来不一样 （e.g. u\u0026rsquo;Mike\u0026rsquo; 代替 \u0026lsquo;Mike\u0026rsquo;）。\nMongoDB 存储数据使用的 BSON 类型。BSON 字符串是通过 UTF-8 编码的，因此 PyMongo 必须保证存储的任何字符串只包含有效的 UTF-8 数据。常规的字符串 （\u0026lt;type \u0026lsquo;str\u0026rsquo;\u0026gt;）会被验证然后被存储而不被改变。Unicode 字符串 （\u0026lt;type \u0026lsquo;unicode\u0026rsquo;\u0026gt;）会首先被 UTF-8 编码。而会出现上述（u\u0026rsquo;Mike\u0026rsquo; 代替 \u0026lsquo;Mike\u0026rsquo;）是因为 PyMongo 会把每一个 BSON 字符串转化成 Python 的 unicode字符串 ，而不是常规字符串。\n大批量插入（Bulk Inserts） # 为了使查询更有意思，让我插入多一些的文档。\n\u0026gt;\u0026gt;\u0026gt; new_posts = [{\u0026quot;author\u0026quot;: \u0026quot;Mike\u0026quot;, ... \u0026quot;text\u0026quot;: \u0026quot;Another post!\u0026quot;, ... \u0026quot;tags\u0026quot;: [\u0026quot;bulk\u0026quot;, \u0026quot;insert\u0026quot;], ... \u0026quot;date\u0026quot;: datetime.datetime(2009, 11, 12, 11, 14)}, ... {\u0026quot;author\u0026quot;: \u0026quot;Eliot\u0026quot;, ... \u0026quot;title\u0026quot;: \u0026quot;MongoDB is fun\u0026quot;, ... \u0026quot;text\u0026quot;: \u0026quot;and pretty easy too!\u0026quot;, ... \u0026quot;date\u0026quot;: datetime.datetime(2009, 11, 10, 10, 45)}] \u0026gt;\u0026gt;\u0026gt; posts.insert(new_posts) [ObjectId('...'), ObjectId('...')] 这个例子有些有趣的东西我们需要注意：\n返回的是两个 ObjectId new_posts 的形状和之前的不一样 - 有一个没有 tags 字段，和我们加入了一个 新的字段 title。是因为 MongoDB 是 schema-free（无模式的）。 查询多个文档 # 为了获取多个文档作为结果的查询，我们使用 find() 方法。`find() 返回一个指针 （cursor）实例。\n\u0026gt;\u0026gt;\u0026gt; for post in posts.find(): ... post {u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']} {u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']} {u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'} 计数 # 如果我们只想知道有多少个文档匹配，我们可以使用 count() 操作。\n\u0026gt;\u0026gt;\u0026gt; posts.count() 3 \u0026gt;\u0026gt;\u0026gt; posts.find({\u0026quot;author\u0026quot;: \u0026quot;Mike\u0026quot;}).count() 2 索引 # 为了使查询更快，我们可以添加组合索引在 \u0026quot;date\u0026quot; 和 \u0026quot;author\u0026quot; 上。我们使用 explain() 方法来获取关于查询性能的信息\n\u0026gt;\u0026gt;\u0026gt; posts.find({\u0026quot;date\u0026quot;: {\u0026quot;$lt\u0026quot;: d}}).sort(\u0026quot;author\u0026quot;).explain()[\u0026quot;cursor\u0026quot;] u'BasicCursor' \u0026gt;\u0026gt;\u0026gt; posts.find({\u0026quot;date\u0026quot;: {\u0026quot;$lt\u0026quot;: d}}).sort(\u0026quot;author\u0026quot;).explain()[\u0026quot;nscanned\u0026quot;] 3 我们可以看出查询使用的使 BasicCursor 和检索了所有的三个文档。现在我们加上组 合索引，查看相同的信息：\nimport pymongo import ASCENDING, DESCENDING \u0026gt;\u0026gt;\u0026gt; posts.create_index([(\u0026quot;date\u0026quot;, DESCENDING), (\u0026quot;author\u0026quot;, ASCENDING)]) u'date_-1_author_1' \u0026gt;\u0026gt;\u0026gt; posts.find({\u0026quot;date\u0026quot;: {\u0026quot;$lt\u0026quot;: d}}).sort(\u0026quot;author\u0026quot;).explain()[\u0026quot;cursor\u0026quot;] u'BtreeCursor date_-1_author_1' \u0026gt;\u0026gt;\u0026gt; posts.find({\u0026quot;date\u0026quot;: {\u0026quot;$lt\u0026quot;: d}}).sort(\u0026quot;author\u0026quot;).explain()[\u0026quot;nscanned\u0026quot;] 2 现在我们使用的使 BTreeCursor 索引和只检索了两个匹配的文档。\n","date":"2013-10-03","permalink":"/n3xtchen/2013/10/03/python-module-pymongo/","section":"时间线","summary":"连接服务器 # \u0026gt;\u0026gt;\u0026gt; from pymongo import MongoClient \u0026gt;\u0026gt;\u0026gt; client = MongoClient('127.","title":"Python Mongo 模块 - Pymongo"},{"content":"","date":"2013-10-03","permalink":"/n3xtchen/tags/%E6%A8%A1%E5%9D%97/","section":"标签","summary":"","title":"模块"},{"content":" 空白处理 # 去除字符中的所有空格 # _string = ' 88 11 10 ' print \u0026quot;\u0026quot;.join(_string.split()) # 结果是 881110 文件路径 # 脚本文件所在的目录 # import os print os.path.dirname(__file__) ","date":"2013-09-28","permalink":"/n3xtchen/2013/09/28/python-trick-re/","section":"时间线","summary":"空白处理 # 去除字符中的所有空格 # _string = ' 88 11 10 ' print \u0026quot;\u0026quot;.","title":"Python Trick"},{"content":"","date":"2013-09-28","permalink":"/n3xtchen/tags/re/","section":"标签","summary":"","title":"re"},{"content":"","date":"2013-09-28","permalink":"/n3xtchen/tags/trick/","section":"标签","summary":"","title":"trick"},{"content":"","date":"2013-09-27","permalink":"/n3xtchen/tags/beginner/","section":"标签","summary":"","title":"beginner"},{"content":"","date":"2013-09-27","permalink":"/n3xtchen/tags/lesson/","section":"标签","summary":"","title":"lesson"},{"content":" 我叫 Slim # Slim 是一个 PHP 微框架，它帮助我们快速的编写简单而且强大的网络应用和借口。\n特性 # 强大的路由（Router） 标准和可定制的 HTTP 请求方式 路由参数可使用通配符和条件 重定向，停止以及传递 路由中间件 自定义模版（Template） 消息 使用 AES-256 加密的安全 cookies HTTP 缓存 自定义日志 错误处理和调试 中间件和钩子架构 简单的配置 系统要求 # PHP 5.3.0 以上 安装 # Mac OS(with homebrew) # $ brew tap josegonzalez/php $ brew install PHP53 # 如果你未安装 homebrew-php 的话 $ brew install composer $ cat composer.json # 创建和这个内容相同 { \u0026quot;require\u0026quot;: { \u0026quot;slim/slim\u0026quot;: \u0026quot;2.*\u0026quot; } } $ composer install 手动安装 # $ wget https://github.com/codeguy/Slim/zipball/master $ unzip master app_name Hello, Slim # 应用代码如下：\n# app.php \u0026lt;?php require __DIR__.'/vendor/autoload.php'; // 使用 Composer 安装的方法 $app = new \\Slim\\Slim(); $app-\u0026gt;get('/hello/:name', function ($name) { echo \u0026quot;Hello, $name\u0026quot;; }); $app-\u0026gt;run(); ?\u0026gt; 这里假设您使用的是 Apache，并且应用的目录重写模块打开，重写规则如下：（稍后我们将为你详细介绍 Apache/Nginx 重写规则）\n# .htaccess RewriteEngine On RewriteBase / # 代码所在 webroot 内的相对路径, 这里是在 webroot 目录内 RewriteCond %{REQUEST_FILENAME} !-d RewriteCond %{QUEST_FILENAME} !-f RewriteRule ^ index.php [QSA,L] 我们访问 http:127.0.0.1/hello/slim，浏览器中将打印 Hello, Slim\n","date":"2013-09-27","permalink":"/n3xtchen/2013/09/27/php-slim-shady/","section":"时间线","summary":"我叫 Slim # Slim 是一个 PHP 微框架，它帮助我们快速的编写简单而且强大的网络应用和借口。","title":"PHP 苗条(Slim)的框架 - 第一部分"},{"content":"","date":"2013-09-27","permalink":"/n3xtchen/tags/slim/","section":"标签","summary":"","title":"Slim"},{"content":" 生成配置文件 # import ConfigParser config = ConfigParser.RawConfigParser() config.add_section('Section1') config.set('Section1', 'an_int', '15') config.set('Section1', 'a_bool', 'true') config.set('Section1', 'a_float', '3.1415') config.set('Section1', 'baz', 'fun') config.set('Section1', 'bar', 'Python') config.set('Section1', 'foo', '%(bar)s is %(baz)s!') # Writing our configuration file to 'example.cfg' with open('example.cfg', 'wb') as configfile: config.write(configfile) 读取配置 # import ConfigParser config = ConfigParser.RawConfigParser() config.read('example.cfg') # getfloat() raises an exception if the value is not a float # getint() and getboolean() also do this for their respective types a_float = config.getfloat('Section1', 'a_float') an_int = config.getint('Section1', 'an_int') print a_float + an_int # Notice that the next output does not interpolate '%(bar)s' or '%(baz)s'. # This is because we are using a RawConfigParser(). if config.getboolean('Section1', 'a_bool'): print config.get('Section1', 'foo') ","date":"2013-09-24","permalink":"/n3xtchen/2013/09/24/python-module-configparser/","section":"时间线","summary":"生成配置文件 # import ConfigParser config = ConfigParser.","title":"Python 常用模块 - 通用配置configParser"},{"content":"","date":"2013-09-23","permalink":"/n3xtchen/tags/flask/","section":"标签","summary":"","title":"Flask"},{"content":"","date":"2013-09-23","permalink":"/n3xtchen/tags/microframe/","section":"标签","summary":"","title":"microframe"},{"content":"","date":"2013-09-23","permalink":"/n3xtchen/tags/senatra/","section":"标签","summary":"","title":"Senatra"},{"content":" 什么是微框架(MicroFrameWork) # 某种编程语言语言以最小代价实现某一领域应用的特定领域语言 (DSL)。\n什么是 Web微框架 # 某种编程语言，针对 Web 应用的微框架\n认识几个常见语言的 Web 微框架 # Python - Flask # Flask 是一個輕量級的 Web應用框架 , 使用 Python 編寫。基於 Werkzeug WSGI 工具箱和 Jinja2 模板引擎。 Flask 使用 BSD 授權。 Flask也被稱為 “microframework” ，因為它使用簡單的核心，用 extension 增加其他功能。Flask沒有預設使用的資料庫、表單驗證工具。然而，Flask保留了擴增的彈性，可以用 Flask-extension 加入這些功能：ORM、表單驗證工具、檔案上傳、各種開放式身份驗證技術。\nRuby - Senatra # Sinatra是一个基于Ruby语言，以最小精力为代价快速创建web应用为目的的DSL（ 领域专属语言）\nPHP - Slim # Slim是一种PHP微框架，它帮助你快速编写简单但是强大的Web应用或者接口。\n","date":"2013-09-23","permalink":"/n3xtchen/2013/09/23/microframework/","section":"时间线","summary":"什么是微框架(MicroFrameWork) # 某种编程语言语言以最小代价实现某一领域应用的特定领域语言 (DSL)。","title":"WEB 微框架"},{"content":"","date":"2013-09-20","permalink":"/n3xtchen/2013/09/20/hello-g-tech/","section":"时间线","summary":"","title":"Hello, G Tech"},{"content":"","date":"0001-01-01","permalink":"/n3xtchen/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"0001-01-01","permalink":"/n3xtchen/series/","section":"Series","summary":"","title":"Series"}]